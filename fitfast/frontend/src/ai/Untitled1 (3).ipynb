{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6B_-9QV4Tfm",
    "outputId": "9ebfe55a-3ca8-4f8d-ff31-1618b0cda899"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Fashion_Recommendation_System.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "# ðŸ›ï¸ **Complete Fashion Recommendation & Outfit Building System**\n",
    "\n",
    "**System Features:**\n",
    "1. Personalized recommendations based on purchase history\n",
    "2. Size/fit recommendations based on body measurements\n",
    "3. Outfit building and compatibility scoring\n",
    "4. Cold-start recommendations for new users\n",
    "5. Style-based filtering\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "# @title âš™ï¸ **Step 0: Install & Import Required Libraries**\n",
    "\n",
    "!pip install pandas numpy scikit-learn tensorflow openpyxl sqlalchemy pymysql python-dotenv -q\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "import builtins\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine.url import URL\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# TensorFlow for embeddings\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "ARTIFACTS_DIR = Path.cwd() / \"artifacts\"\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def artifact_path(filename: str) -> Path:\n",
    "    return ARTIFACTS_DIR / filename\n",
    "\n",
    "def find_project_file(filename: str, start: Path | None = None) -> Path | None:\n",
    "    current = (start or Path.cwd()).resolve()\n",
    "    for parent in [current, *current.parents]:\n",
    "        candidate = parent / filename\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "def _resolve_artifact_path(path):\n",
    "    if isinstance(path, (str, Path)):\n",
    "        path_str = str(path)\n",
    "        if path_str.startswith(\"/content/\"):\n",
    "            return artifact_path(path_str.replace(\"/content/\", \"\", 1))\n",
    "    return path\n",
    "\n",
    "if not hasattr(builtins, \"_artifact_original_open\"):\n",
    "    builtins._artifact_original_open = builtins.open\n",
    "    def _artifact_open(file, *args, **kwargs):\n",
    "        resolved = _resolve_artifact_path(file)\n",
    "        return builtins._artifact_original_open(resolved, *args, **kwargs)\n",
    "    builtins.open = _artifact_open\n",
    "\n",
    "if not hasattr(builtins, \"_artifact_original_print\"):\n",
    "    builtins._artifact_original_print = builtins.print\n",
    "    def _artifact_print(*args, **kwargs):\n",
    "        prefix = f\"{ARTIFACTS_DIR.resolve()}/\"\n",
    "        updated_args = []\n",
    "        for arg in args:\n",
    "            if isinstance(arg, str):\n",
    "                updated_args.append(arg.replace(\"/content/\", prefix))\n",
    "            else:\n",
    "                updated_args.append(arg)\n",
    "        builtins._artifact_original_print(*updated_args, **kwargs)\n",
    "    builtins.print = _artifact_print\n",
    "\n",
    "if not hasattr(pd, \"_artifact_original_read_pickle\"):\n",
    "    pd._artifact_original_read_pickle = pd.read_pickle\n",
    "    def _artifact_read_pickle(path, *args, **kwargs):\n",
    "        resolved = _resolve_artifact_path(path)\n",
    "        return pd._artifact_original_read_pickle(resolved, *args, **kwargs)\n",
    "    pd.read_pickle = _artifact_read_pickle\n",
    "\n",
    "if not hasattr(pd.DataFrame, \"_artifact_original_to_pickle\"):\n",
    "    pd.DataFrame._artifact_original_to_pickle = pd.DataFrame.to_pickle\n",
    "    def _artifact_to_pickle(self, path, *args, **kwargs):\n",
    "        resolved = _resolve_artifact_path(path)\n",
    "        return pd.DataFrame._artifact_original_to_pickle(self, resolved, *args, **kwargs)\n",
    "    pd.DataFrame.to_pickle = _artifact_to_pickle\n",
    "\n",
    "if not hasattr(np, \"_artifact_original_save\"):\n",
    "    np._artifact_original_save = np.save\n",
    "    def _artifact_np_save(file, arr, *args, **kwargs):\n",
    "        resolved = _resolve_artifact_path(file)\n",
    "        return np._artifact_original_save(resolved, arr, *args, **kwargs)\n",
    "    np.save = _artifact_np_save\n",
    "\n",
    "ORIGINAL_DATA_PICKLE = artifact_path(\"original_items.pkl\")\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eepShULD_Q5m",
    "outputId": "07e1c71b-ce6d-43d0-952f-78124bd97e17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”‘ Loaded environment variables from C:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\.env\n",
      "âš ï¸ Dataset not found at C:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\\data\\items.csv â€” attempting to load from seeded database...\n",
      "ðŸ—„ï¸ Connecting to MySQL at 127.0.0.1:3306/fitfast\n",
      "   âœ… Retrieved 250 items from MySQL\n",
      "ðŸ“Š Data loaded: 250 rows, 8 columns\n",
      "\n",
      "First 3 rows:\n",
      "   ID               Name  Price  Category             Store  Total Stock  \\\n",
      "0   1   Classic Crew Tee  19.99  T-Shirts  Fashion Store 10          103   \n",
      "1   2         V-Neck Tee  21.99  T-Shirts   Fashion Store 9          109   \n",
      "2   3  Graphic Print Tee  24.99  T-Shirts   Fashion Store 8          108   \n",
      "\n",
      "                              Color Variants Details  \\\n",
      "0  {\"Black\":{\"name\":\"Black\",\"stock\":81},\"White\":{...   \n",
      "1  {\"Gray\":{\"name\":\"Gray\",\"stock\":32},\"Navy\":{\"na...   \n",
      "2  {\"Black\":{\"name\":\"Black\",\"stock\":90},\"White\":{...   \n",
      "\n",
      "                                         Sizing Data  \n",
      "0  {\"garment_type\":\"t_shirt\",\"measurements_cm\":{\"...  \n",
      "1  {\"garment_type\":\"v_neck_tee\",\"measurements_cm\"...  \n",
      "2  {\"garment_type\":\"t_shirt\",\"measurements_cm\":{\"...  \n",
      "\n",
      "ðŸ“‹ Columns (8 total):\n",
      " 1. ID\n",
      " 2. Name\n",
      " 3. Price\n",
      " 4. Category\n",
      " 5. Store\n",
      " 6. Total Stock\n",
      " 7. Color Variants Details\n",
      " 8. Sizing Data\n",
      "\n",
      "ðŸ’¾ Original data saved to 'C:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\\artifacts\\original_items.pkl'\n"
     ]
    }
   ],
   "source": [
    "# @title ðŸ“ **Step 1: Load Your Data File or Database Seed**\n",
    "\n",
    "import os\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "CSV_DATA_PATH = Path(\"data/items.csv\")\n",
    "ENV_PATH = find_project_file(\".env\")\n",
    "SQLITE_PATH = find_project_file(\"database.sqlite\")\n",
    "\n",
    "if ENV_PATH:\n",
    "    load_dotenv(ENV_PATH)\n",
    "    print(f\"ðŸ”‘ Loaded environment variables from {ENV_PATH}\")\n",
    "\n",
    "def _coerce_json(value):\n",
    "    if value is None or (isinstance(value, float) and pd.isna(value)):\n",
    "        return \"\"\n",
    "    if isinstance(value, (dict, list)):\n",
    "        return json.dumps(value)\n",
    "    if isinstance(value, str):\n",
    "        return value\n",
    "    return json.dumps(value)\n",
    "\n",
    "def _load_from_csv(path: Path) -> pd.DataFrame:\n",
    "    print(f\"ðŸ“‚ Loading data from {path.resolve()}\")\n",
    "    if path.suffix.lower() == \".csv\":\n",
    "        df_loaded = pd.read_csv(path)\n",
    "        print(\"ðŸ“„ Loaded as CSV file\")\n",
    "    elif path.suffix.lower() in {\".xlsx\", \".xls\"}:\n",
    "        df_loaded = pd.read_excel(path)\n",
    "        print(\"ðŸ“Š Loaded as Excel file\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type for {path}\")\n",
    "    return df_loaded\n",
    "\n",
    "def _build_items_query() -> str:\n",
    "    return \"\"\"\\\n",
    "SELECT\n",
    "    items.id AS `ID`,\n",
    "    items.name AS `Name`,\n",
    "    items.price AS `Price`,\n",
    "    categories.name AS `Category`,\n",
    "    stores.name AS `Store`,\n",
    "    items.stock_quantity AS `Total Stock`,\n",
    "    items.color_variants AS `Color Variants Details`,\n",
    "    items.sizing_data AS `Sizing Data`\n",
    "FROM items\n",
    "INNER JOIN categories ON items.category_id = categories.id\n",
    "INNER JOIN stores ON items.store_id = stores.id\n",
    "\"\"\"\n",
    "\n",
    "def _load_from_sqlite(sqlite_file: Path) -> pd.DataFrame | None:\n",
    "    if not sqlite_file.exists():\n",
    "        return None\n",
    "    print(f\"ðŸ—„ï¸ Loading items from SQLite database at {sqlite_file}\")\n",
    "    conn = sqlite3.connect(sqlite_file)\n",
    "    try:\n",
    "        df_loaded = pd.read_sql_query(_build_items_query(), conn)\n",
    "        print(f\"   âœ… Retrieved {len(df_loaded)} items from SQLite\")\n",
    "        return df_loaded\n",
    "    except Exception as exc:\n",
    "        print(f\"   âŒ SQLite query failed: {exc}\")\n",
    "        return None\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def _load_from_mysql() -> pd.DataFrame | None:\n",
    "    driver = os.getenv(\"DB_CONNECTION\", \"mysql\").lower()\n",
    "    if driver != \"mysql\":\n",
    "        return None\n",
    "    db_name = os.getenv(\"DB_DATABASE\")\n",
    "    db_user = os.getenv(\"DB_USERNAME\") or os.getenv(\"DB_USER\")\n",
    "    db_pass = os.getenv(\"DB_PASSWORD\") or \"\"\n",
    "    db_host = os.getenv(\"DB_HOST\", \"127.0.0.1\")\n",
    "    db_port = os.getenv(\"DB_PORT\", \"3306\")\n",
    "    if not db_name or not db_user:\n",
    "        print(\"   âš ï¸ MySQL credentials missing in environment variables\")\n",
    "        return None\n",
    "    try:\n",
    "        url = URL.create(\n",
    "            \"mysql+pymysql\",\n",
    "            username=db_user,\n",
    "            password=db_pass or None,\n",
    "            host=db_host,\n",
    "            port=int(db_port),\n",
    "            database=db_name\n",
    "        )\n",
    "        print(f\"ðŸ—„ï¸ Connecting to MySQL at {db_host}:{db_port}/{db_name}\")\n",
    "        engine = create_engine(url)\n",
    "        with engine.connect() as conn:\n",
    "            df_loaded = pd.read_sql_query(_build_items_query(), conn)\n",
    "        print(f\"   âœ… Retrieved {len(df_loaded)} items from MySQL\")\n",
    "        return df_loaded\n",
    "    except Exception as exc:\n",
    "        print(f\"   âŒ MySQL query failed: {exc}\")\n",
    "        return None\n",
    "\n",
    "df = None\n",
    "\n",
    "if CSV_DATA_PATH.exists():\n",
    "    df = _load_from_csv(CSV_DATA_PATH)\n",
    "else:\n",
    "    print(f\"âš ï¸ Dataset not found at {CSV_DATA_PATH.resolve()} â€” attempting to load from seeded database...\")\n",
    "    if SQLITE_PATH:\n",
    "        df = _load_from_sqlite(SQLITE_PATH)\n",
    "    if df is None:\n",
    "        df = _load_from_mysql()\n",
    "\n",
    "if df is None or df.empty:\n",
    "    raise FileNotFoundError(\"Unable to load dataset from CSV or database sources. Provide a CSV/Excel file or ensure the database is reachable.\")\n",
    "\n",
    "df[\"Color Variants Details\"] = df[\"Color Variants Details\"].apply(_coerce_json)\n",
    "df[\"Sizing Data\"] = df[\"Sizing Data\"].apply(_coerce_json)\n",
    "\n",
    "print(f\"ðŸ“Š Data loaded: {len(df)} rows, {len(df.columns)} columns\")\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "print(df.head(3))\n",
    "\n",
    "print(f\"\\nðŸ“‹ Columns ({len(df.columns)} total):\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "df.to_pickle(ORIGINAL_DATA_PICKLE)\n",
    "print(f\"\\nðŸ’¾ Original data saved to '{ORIGINAL_DATA_PICKLE.resolve()}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tDVT8-Z5ArOV",
    "outputId": "61411125-1c00-48b8-b92b-8a5a3852b300"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ COMPLETE STEP 2: Feature Engineering with ALL Fixes Applied\n",
      "============================================================\n",
      "\n",
      "1. ðŸ”„ Loading original data...\n",
      "   âœ… Loaded 250 items\n",
      "\n",
      "2. ðŸ“‹ Correct parsing of all data...\n",
      "   Parsing sizing data...\n",
      "   âœ… Parsed 250 items\n",
      "\n",
      "3. ðŸ·ï¸ Correct categorization with precise rules...\n",
      "\n",
      "4. ðŸ”§ Applying special fixes for specific items...\n",
      "   Fixing 'Performance Training' items to 'athletic'...\n",
      "   âœ… Fixed 2 'Performance Training' items\n",
      "   Fixing 'Training' items to 'athletic'...\n",
      "   âœ… Fixed 8 'Training' items\n",
      "   Fixing 'Athletic' items to 'athletic'...\n",
      "   âœ… Fixed 5 'Athletic' items\n",
      "   âœ… Categorized all items with special fixes\n",
      "\n",
      "5. ðŸ”§ Creating feature engineering...\n",
      "\n",
      "6. ðŸ”  Encoding categorical features...\n",
      "   âœ… Categorical features encoded\n",
      "\n",
      "7. ðŸ“ Scaling numerical features...\n",
      "   âœ… Numerical features scaled\n",
      "\n",
      "8. ðŸ’¾ Saving processed data...\n",
      "   âœ… Processed data saved\n",
      "\n",
      "ðŸŽ¯ Feature engineering complete!\n"
     ]
    }
   ],
   "source": [
    "# @title ðŸŽ¯ **STEP 2: Feature Engineering with ALL Fixes Applied**\n",
    "print(\"ðŸŽ¯ COMPLETE STEP 2: Feature Engineering with ALL Fixes Applied\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from collections import Counter\n",
    "\n",
    "# ========== 1. RELOAD ORIGINAL DATA ========== \n",
    "print(\"\\n1. ðŸ”„ Loading original data...\")\n",
    "original_df = pd.read_pickle(ORIGINAL_DATA_PICKLE)\n",
    "print(f\"   âœ… Loaded {len(original_df)} items\")\n",
    "\n",
    "# ========== 2. CORRECT PARSING OF ALL DATA ========== \n",
    "print(\"\\n2. ðŸ“‹ Correct parsing of all data...\")\n",
    "\n",
    "def parse_sizing_data_final(sizing_str):\n",
    "    \"\"\"Properly parse sizing data\"\"\"\n",
    "    if pd.isna(sizing_str) or not isinstance(sizing_str, str):\n",
    "        return {'garment_type': 'unknown'}\n",
    "\n",
    "    result = {'garment_type': 'unknown'}\n",
    "    try:\n",
    "        # First try JSON parsing\n",
    "        if sizing_str.strip().startswith('{'):\n",
    "            data = json.loads(sizing_str)\n",
    "            result['garment_type'] = data.get('garment_type', 'unknown')\n",
    "            result['fit_characteristics'] = data.get('fit_characteristics', {})\n",
    "            result['measurements_cm'] = data.get('measurements_cm', {})\n",
    "            result['size_system'] = data.get('size_system', 'US')\n",
    "            return result\n",
    "\n",
    "        # Fallback: string parsing\n",
    "        parts = [p.strip() for p in sizing_str.split(';') if p.strip()]\n",
    "        for part in parts:\n",
    "            if 'garment_type:' in part:\n",
    "                result['garment_type'] = part.split('garment_type:')[1].strip()\n",
    "            elif 'fit_type:' in part:\n",
    "                if 'fit_characteristics' not in result:\n",
    "                    result['fit_characteristics'] = {}\n",
    "                result['fit_characteristics']['fit_type'] = part.split('fit_type:')[1].strip()\n",
    "            elif 'ease:' in part:\n",
    "                if 'fit_characteristics' not in result:\n",
    "                    result['fit_characteristics'] = {}\n",
    "                result['fit_characteristics']['ease'] = part.split('ease:')[1].strip()\n",
    "            elif 'stretch:' in part:\n",
    "                if 'fit_characteristics' not in result:\n",
    "                    result['fit_characteristics'] = {}\n",
    "                result['fit_characteristics']['stretch'] = part.split('stretch:')[1].strip()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return result\n",
    "\n",
    "def parse_colors_final(color_str):\n",
    "    \"\"\"Properly parse color variants\"\"\"\n",
    "    colors = {}\n",
    "    if pd.isna(color_str):\n",
    "        return colors\n",
    "\n",
    "    try:\n",
    "        if isinstance(color_str, str):\n",
    "            if color_str.strip().startswith('{'):\n",
    "                # JSON format\n",
    "                color_dict = json.loads(color_str)\n",
    "                for color_name, color_data in color_dict.items():\n",
    "                    if isinstance(color_data, dict):\n",
    "                        colors[color_name] = color_data.get('stock', 1)\n",
    "                    else:\n",
    "                        colors[color_name] = 1\n",
    "            else:\n",
    "                # String format\n",
    "                for item in color_str.split(','):\n",
    "                    item = item.strip()\n",
    "                    if ':' in item:\n",
    "                        color_name = item.split(':')[0].strip()\n",
    "                        colors[color_name] = 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return colors\n",
    "\n",
    "# Parse all data\n",
    "print(\"   Parsing sizing data...\")\n",
    "all_items = []\n",
    "for idx, row in original_df.iterrows():\n",
    "    item = {\n",
    "        'item_id': idx + 1,\n",
    "        'name': row.get('Name', f'Item {idx+1}'),\n",
    "        'price': float(row.get('Price', 0)),\n",
    "        'category': row.get('Category', 'unknown'),\n",
    "        'store': row.get('Store', 'unknown'),\n",
    "        'total_stock': int(row.get('Total Stock', 0)) if pd.notna(row.get('Total Stock')) else 0\n",
    "    }\n",
    "\n",
    "    # Parse colors\n",
    "    colors = parse_colors_final(row.get('Color Variants Details', ''))\n",
    "    item['colors'] = list(colors.keys())\n",
    "    item['color_stocks'] = colors\n",
    "\n",
    "    # Parse sizing data\n",
    "    sizing = parse_sizing_data_final(row.get('Sizing Data', ''))\n",
    "    item['garment_type'] = sizing.get('garment_type', 'unknown')\n",
    "\n",
    "    fit_chars = sizing.get('fit_characteristics', {})\n",
    "    item['fit_type'] = fit_chars.get('fit_type', 'regular')\n",
    "    item['ease'] = fit_chars.get('ease', 'standard')\n",
    "    item['stretch'] = fit_chars.get('stretch', 'medium')\n",
    "\n",
    "    measurements = sizing.get('measurements_cm', {})\n",
    "    item['measurements'] = measurements\n",
    "\n",
    "    all_items.append(item)\n",
    "\n",
    "features_df = pd.DataFrame(all_items)\n",
    "print(f\"   âœ… Parsed {len(features_df)} items\")\n",
    "\n",
    "# ========== 3. CORRECT CATEGORIZATION WITH PRECISE RULES ========== \n",
    "print(\"\\n3. ðŸ·ï¸ Correct categorization with precise rules...\")\n",
    "\n",
    "# Define precise categorization rules\n",
    "garment_type_to_category = {\n",
    "    # Tops\n",
    "    't_shirt': ('top', 'casual'),\n",
    "    'v_neck_tee': ('top', 'casual'),\n",
    "    'fitted_shirt': ('top', 'business_casual'),\n",
    "    'dress_shirt': ('top', 'formal'),\n",
    "    'polo_shirt': ('top', 'business_casual'),\n",
    "    'henley_shirt': ('top', 'casual'),\n",
    "\n",
    "    # Sweaters & Hoodies\n",
    "    'crewneck_sweater': ('top', 'casual'),\n",
    "    'cardigan': ('top', 'casual'),\n",
    "    'turtleneck': ('top', 'casual'),\n",
    "    'pullover_hoodie': ('top', 'casual'),\n",
    "    'zip_hoodie': ('top', 'casual'),\n",
    "\n",
    "    # Bottoms\n",
    "    'slim_pants': ('bottom', 'business_casual'),\n",
    "    'regular_pants': ('bottom', 'business_casual'),\n",
    "    'cargo_pants': ('bottom', 'casual'),\n",
    "    'regular_jeans': ('bottom', 'casual'),\n",
    "    'slim_jeans': ('bottom', 'casual'),\n",
    "    'casual_shorts': ('bottom', 'casual'),\n",
    "    'cargo_shorts': ('bottom', 'casual'),\n",
    "\n",
    "    # Athletic\n",
    "    'training_shorts': ('bottom', 'athletic'),\n",
    "    'yoga_pants': ('bottom', 'athletic'),\n",
    "    'leggings': ('bottom', 'athletic'),\n",
    "\n",
    "    # Dresses\n",
    "    'a_line_dress': ('dress', 'business_casual'),\n",
    "    'bodycon_dress': ('dress', 'business_casual'),\n",
    "    'maxi_dress': ('dress', 'casual'),\n",
    "    'midi_dress': ('dress', 'business_casual'),\n",
    "    'wrap_dress': ('dress', 'business_casual'),\n",
    "\n",
    "    # Skirts\n",
    "    'a_line_skirt': ('bottom', 'business_casual'),\n",
    "    'pencil_skirt': ('bottom', 'business_casual'),\n",
    "    'tennis_skirt': ('bottom', 'athletic'),\n",
    "\n",
    "    # Outerwear\n",
    "    'bomber_jacket': ('outerwear', 'casual'),\n",
    "    'denim_jacket': ('outerwear', 'casual'),\n",
    "    'windbreaker': ('outerwear', 'casual'),\n",
    "    'puffer_jacket': ('outerwear', 'casual'),\n",
    "    'trench_coat': ('outerwear', 'formal'),\n",
    "\n",
    "    # Swimwear\n",
    "    'bikini_top': ('swimwear', 'athletic'),\n",
    "    'swim_trunks': ('swimwear', 'athletic'),\n",
    "    'board_shorts': ('swimwear', 'athletic'),\n",
    "    'one_piece_swimsuit': ('swimwear', 'athletic'),\n",
    "    'rash_guard': ('swimwear', 'athletic'),\n",
    "\n",
    "    # Footwear\n",
    "    'sneakers': ('footwear', 'casual'),\n",
    "    'dress_shoes': ('footwear', 'formal'),\n",
    "\n",
    "    # Underwear\n",
    "    'briefs': ('underwear', 'casual'),\n",
    "    'boxer_briefs': ('underwear', 'casual'),\n",
    "\n",
    "    # Socks\n",
    "    'crew_socks': ('socks', 'casual'),\n",
    "    'ankle_socks': ('socks', 'casual'),\n",
    "}\n",
    "\n",
    "# Apply categorization\n",
    "features_df['garment_category'] = 'other'\n",
    "features_df['garment_formality'] = 'casual'\n",
    "\n",
    "for idx, row in features_df.iterrows():\n",
    "    garment_type = row['garment_type']\n",
    "    if garment_type in garment_type_to_category:\n",
    "        category, formality = garment_type_to_category[garment_type]\n",
    "        features_df.at[idx, 'garment_category'] = category\n",
    "        features_df.at[idx, 'garment_formality'] = formality\n",
    "    else:\n",
    "        # Fallback based on name\n",
    "        name_lower = str(row['name']).lower()\n",
    "        if any(word in name_lower for word in ['dress', 'gown']):\n",
    "            features_df.at[idx, 'garment_category'] = 'dress'\n",
    "            features_df.at[idx, 'garment_formality'] = 'business_casual'\n",
    "        elif any(word in name_lower for word in ['shirt', 'blouse', 'top', 'tee']):\n",
    "            features_df.at[idx, 'garment_category'] = 'top'\n",
    "            features_df.at[idx, 'garment_formality'] = 'business_casual' if 'shirt' in name_lower else 'casual'\n",
    "        elif any(word in name_lower for word in ['pants', 'jeans', 'shorts', 'skirt']):\n",
    "            features_df.at[idx, 'garment_category'] = 'bottom'\n",
    "            features_df.at[idx, 'garment_formality'] = 'business_casual' if 'pants' in name_lower and 'dress' in name_lower else 'casual'\n",
    "        elif any(word in name_lower for word in ['jacket', 'coat', 'blazer']):\n",
    "            features_df.at[idx, 'garment_category'] = 'outerwear'\n",
    "            features_df.at[idx, 'garment_formality'] = 'formal' if 'coat' in name_lower else 'casual'\n",
    "        elif any(word in name_lower for word in ['shoes', 'sneakers', 'boots']):\n",
    "            features_df.at[idx, 'garment_category'] = 'footwear'\n",
    "            features_df.at[idx, 'garment_formality'] = 'formal' if 'dress' in name_lower else 'casual'\n",
    "\n",
    "# ========== 4. SPECIAL FIXES FOR SPECIFIC ITEMS ========== \n",
    "print(\"\\n4. ðŸ”§ Applying special fixes for specific items...\")\n",
    "\n",
    "# Fix 1: Performance Training items should be athletic\n",
    "print(\"   Fixing 'Performance Training' items to 'athletic'...\")\n",
    "mask = features_df['name'].str.contains('Performance Training', case=False, na=False)\n",
    "features_df.loc[mask, 'garment_formality'] = 'athletic'\n",
    "print(f\"   âœ… Fixed {mask.sum()} 'Performance Training' items\")\n",
    "\n",
    "# Fix 2: Training items should be athletic\n",
    "print(\"   Fixing 'Training' items to 'athletic'...\")\n",
    "mask = features_df['name'].str.contains('Training', case=False, na=False) & \\\n",
    "       ~features_df['name'].str.contains('Performance Training', case=False, na=False)\n",
    "features_df.loc[mask, 'garment_formality'] = 'athletic'\n",
    "print(f\"   âœ… Fixed {mask.sum()} 'Training' items\")\n",
    "\n",
    "# Fix 3: Athletic items should be athletic\n",
    "print(\"   Fixing 'Athletic' items to 'athletic'...\")\n",
    "mask = features_df['name'].str.contains('Athletic', case=False, na=False)\n",
    "features_df.loc[mask, 'garment_formality'] = 'athletic'\n",
    "print(f\"   âœ… Fixed {mask.sum()} 'Athletic' items\")\n",
    "\n",
    "print(f\"   âœ… Categorized all items with special fixes\")\n",
    "\n",
    "# ========== 5. CREATE FEATURE ENGINEERING ========== \n",
    "print(\"\\n5. ðŸ”§ Creating feature engineering...\")\n",
    "\n",
    "# Create color features\n",
    "all_colors = []\n",
    "for colors in features_df['colors']:\n",
    "    all_colors.extend(colors)\n",
    "\n",
    "top_colors = [color for color, count in Counter(all_colors).most_common(10)]\n",
    "\n",
    "color_themes = {\n",
    "    'dark_colors': ['Black', 'Navy', 'Charcoal', 'Dark', 'Brown', 'Dark Blue', 'Dark Gray'],\n",
    "    'light_colors': ['White', 'Beige', 'Ivory', 'Cream', 'Light', 'Light Gray'],\n",
    "    'bold_colors': ['Red', 'Blue', 'Green', 'Yellow', 'Pink', 'Orange', 'Purple', 'Royal Blue', 'Burgundy'],\n",
    "    'neutral_colors': ['Gray', 'Beige', 'White', 'Black', 'Navy', 'Brown', 'Charcoal', 'Dark Gray']\n",
    "}\n",
    "\n",
    "for theme_name, colors in color_themes.items():\n",
    "    features_df[f'has_{theme_name}'] = features_df['colors'].apply(\n",
    "        lambda x: 1 if any(color in str(color_item) for color in colors for color_item in x) else 0\n",
    "    )\n",
    "\n",
    "# Create measurement features\n",
    "def extract_measurement_features(measurements):\n",
    "    \"\"\"Extract features from measurements\"\"\"\n",
    "    features = {\n",
    "        'has_measurements': 0,\n",
    "        'has_bust': 0,\n",
    "        'has_waist': 0,\n",
    "        'has_hips': 0,\n",
    "        'has_length': 0,\n",
    "        'bust_cm': np.nan,\n",
    "        'waist_cm': np.nan,\n",
    "        'hips_cm': np.nan,\n",
    "        'length_cm': np.nan\n",
    "    }\n",
    "\n",
    "    if isinstance(measurements, dict) and measurements:\n",
    "        features['has_measurements'] = 1\n",
    "        for key, value in measurements.items():\n",
    "            key_lower = key.lower()\n",
    "            if 'bust' in key_lower or 'chest' in key_lower:\n",
    "                features['has_bust'] = 1\n",
    "                features['bust_cm'] = float(value) if value not in [None, ''] else np.nan\n",
    "            elif 'waist' in key_lower:\n",
    "                features['has_waist'] = 1\n",
    "                features['waist_cm'] = float(value) if value not in [None, ''] else np.nan\n",
    "            elif 'hip' in key_lower:\n",
    "                features['has_hips'] = 1\n",
    "                features['hips_cm'] = float(value) if value not in [None, ''] else np.nan\n",
    "            elif 'length' in key_lower:\n",
    "                features['has_length'] = 1\n",
    "                features['length_cm'] = float(value) if value not in [None, ''] else np.nan\n",
    "\n",
    "    return features\n",
    "\n",
    "measurement_features = features_df['measurements'].apply(extract_measurement_features).apply(pd.Series)\n",
    "features_df = pd.concat([features_df.drop(columns=['measurements']), measurement_features], axis=1)\n",
    "\n",
    "# ========== 6. ENCODE CATEGORICAL FEATURES ========== \n",
    "print(\"\\n6. ðŸ”  Encoding categorical features...\")\n",
    "\n",
    "categorical_columns = ['category', 'store', 'garment_type', 'garment_category', 'garment_formality', 'fit_type', 'ease', 'stretch']\n",
    "encoders = {col: LabelEncoder() for col in categorical_columns}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    features_df[f'{col}_encoded'] = encoders[col].fit_transform(features_df[col].astype(str))\n",
    "\n",
    "print(\"   âœ… Categorical features encoded\")\n",
    "\n",
    "# ========== 7. SCALE NUMERICAL FEATURES ========== \n",
    "print(\"\\n7. ðŸ“ Scaling numerical features...\")\n",
    "\n",
    "numerical_columns = ['price', 'total_stock', 'bust_cm', 'waist_cm', 'hips_cm', 'length_cm']\n",
    "scaler = StandardScaler()\n",
    "scaled_values = scaler.fit_transform(features_df[numerical_columns].fillna(0))\n",
    "scaled_df = pd.DataFrame(scaled_values, columns=[f'scaled_{col}' for col in numerical_columns])\n",
    "features_df = pd.concat([features_df.reset_index(drop=True), scaled_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(\"   âœ… Numerical features scaled\")\n",
    "\n",
    "# ========== 8. SAVE PROCESSED DATA ========== \n",
    "print(\"\\n8. ðŸ’¾ Saving processed data...\")\n",
    "\n",
    "with open(ARTIFACTS_DIR / 'feature_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(encoders, f)\n",
    "\n",
    "with open(ARTIFACTS_DIR / 'scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "features_df.to_pickle(ARTIFACTS_DIR / 'features_df.pkl')\n",
    "print(\"   âœ… Processed data saved\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Feature engineering complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wdanatsVDE9K",
    "outputId": "f23c4a06-39bb-4874-c287-2117e7075dac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ STEP 3: Create Item Embeddings (FINAL CORRECTED VERSION - FIXED FOR REAL)\n",
      "============================================================\n",
      "âœ… All libraries imported\n",
      "\n",
      "1. ðŸ”„ Loading data from Step 2...\n",
      "   âœ… Loaded processed features: (250, 41) from c:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\\artifacts\\features_df.pkl\n",
      "   Columns: ['item_id', 'name', 'price', 'category', 'store', 'total_stock', 'colors', 'color_stocks', 'garment_type', 'fit_type', 'ease', 'stretch', 'garment_category', 'garment_formality', 'has_dark_colors', 'has_light_colors', 'has_bold_colors', 'has_neutral_colors', 'has_measurements', 'has_bust', 'has_waist', 'has_hips', 'has_length', 'bust_cm', 'waist_cm', 'hips_cm', 'length_cm', 'category_encoded', 'store_encoded', 'garment_type_encoded', 'garment_category_encoded', 'garment_formality_encoded', 'fit_type_encoded', 'ease_encoded', 'stretch_encoded', 'scaled_price', 'scaled_total_stock', 'scaled_bust_cm', 'scaled_waist_cm', 'scaled_hips_cm', 'scaled_length_cm']\n",
      "   âœ… Loaded rich features from c:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\\artifacts\\features_df.pkl\n",
      "   garment_category dtype: object\n",
      "   Sample categories: ['top' 'bottom' 'dress' 'outerwear' 'swimwear']\n",
      "\n",
      "2. ðŸ› ï¸ APPLYING CRITICAL FIXES: Category-Aware Feature Engineering\n",
      "   Data types fixed\n",
      "   Adding strong category separation features...\n",
      "   Creating clothing vs non-clothing feature...\n",
      "   Creating one-hot category features...\n",
      "   âœ… Added 9 strong category features\n",
      "   Adding formality separation features...\n",
      "   âœ… Added 4 formality features\n",
      "   Creating interaction features...\n",
      "   Scaling continuous features...\n",
      "   Enhancing color features...\n",
      "   âœ… All critical fixes applied\n",
      "\n",
      "3. ðŸŽ¯ Selecting and weighting features...\n",
      "   âœ… category_features: 9 features (weight: 5.0x)\n",
      "   âœ… category_strength: 2 features (weight: 5.0x)\n",
      "   âœ… formality_features: 4 features (weight: 3.0x)\n",
      "   âœ… formality_strength: 1 features (weight: 5.0x)\n",
      "   âœ… interaction_features: 2 features (weight: 3.0x)\n",
      "   âœ… garment_features: 5 features (weight: 2.0x)\n",
      "   âœ… measurement_features: 1 features (weight: 2.0x)\n",
      "   âœ… color_features: 5 features (weight: 1.0x)\n",
      "   âœ… price_features: 1 features (weight: 1.0x)\n",
      "\n",
      "   Total features: 30\n",
      "\n",
      "   Creating weighted feature matrix...\n",
      "   âœ… Weighted feature matrix: (250, 30)\n",
      "\n",
      "4. ðŸ¤– Creating embeddings with PCA (more stable than t-SNE)...\n",
      "   Applying PCA for dimensionality reduction...\n",
      "   âœ… PCA embeddings created: (250, 29)\n",
      "   Explained variance ratio: 100.00%\n",
      "\n",
      "5. ðŸ·ï¸ Validating embeddings with clustering...\n",
      "   Creating 9 clusters (one per garment category)\n",
      "\n",
      "   Cluster-Category Alignment:\n",
      "--------------------------------------------------\n",
      "   Cluster 0: bottom       purity = 64.3%\n",
      "   Cluster 1: other        purity = 100.0%\n",
      "   Cluster 2: top          purity = 100.0%\n",
      "   Cluster 3: bottom       purity = 65.0%\n",
      "   Cluster 4: outerwear    purity = 100.0%\n",
      "   Cluster 5: footwear     purity = 100.0%\n",
      "   Cluster 6: socks        purity = 100.0%\n",
      "   Cluster 7: underwear    purity = 100.0%\n",
      "   Cluster 8: bottom       purity = 71.9%\n",
      "\n",
      "   Average cluster purity: 89.0%\n",
      "   âœ… EXCELLENT cluster separation!\n",
      "\n",
      "6. ðŸ§ª Testing similarity with Cosine Similarity (better than Euclidean)...\n",
      "\n",
      "   Testing similarity with sample items:\n",
      "--------------------------------------------------\n",
      "\n",
      "   'Classic Crew Tee' (Category: top):\n",
      "   1. âœ… Classic Crew Tee Variant  | top          | sim: 100.0%\n",
      "   2. âœ… Organic Cotton Tee        | top          | sim: 85.4%\n",
      "   3. âœ… Cable Knit Sweater Varian | top          | sim: 83.7%\n",
      "\n",
      "   'Classic Straight Jeans' (Category: bottom):\n",
      "   1. âŒ Flannel Shirt Variant 108 | top          | sim: 100.0%\n",
      "   2. âŒ Flannel Shirt             | top          | sim: 100.0%\n",
      "   3. âŒ Oxford Button-Down Varian | top          | sim: 81.2%\n",
      "\n",
      "   'Everyday Sneakers' (Category: footwear):\n",
      "   1. âŒ Maxi Dress                | dress        | sim: 99.9%\n",
      "   2. âŒ Flowy A-Line Dress        | dress        | sim: 55.6%\n",
      "   3. âŒ Flowy A-Line Dress Varian | dress        | sim: 55.5%\n",
      "\n",
      "7. ðŸ’¾ Creating final embeddings dataframe...\n",
      "   âœ… Embeddings dataframe created: (250, 38)\n",
      "   Embedding dimensions: 29\n",
      "   Negative embeddings: YES\n",
      "   Embedding norms - Mean: 5.20, Std: 1.41\n",
      "\n",
      "8. ðŸ§  Testing semantic similarity...\n",
      "\n",
      "   Semantic Similarity Tests:\n",
      "--------------------------------------------------\n",
      "\n",
      "   Test 1: Tops matching tops\n",
      "   Classic Crew Tee â†’ Classic Crew Tee Variant 13 (sim: 100.0%)\n",
      "   V-Neck Tee â†’ Henley Shirt Variant 19 (sim: 100.0%)\n",
      "\n",
      "   Test 2: Cross-category similarity (should be low)\n",
      "   Classic Crew Tee vs Slim Chino Pants: -0.137 (-13.7%)\n",
      "\n",
      "   Test 3: Same category, different formality\n",
      "   Casual: Classic Crew Tee vs Formal: Executive Dress Shirt: 0.279\n",
      "\n",
      "9. ðŸ’¾ Saving all models and data...\n",
      "============================================================\n",
      "âœ… Saved embeddings dataframe â†’ C:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\\artifacts\\item_embeddings.pkl\n",
      "âœ… Saved embeddings array\n",
      "âœ… Saved PCA model\n",
      "âœ… Saved KMeans model\n",
      "âœ… Saved PCA scaler\n",
      "âœ… Saved robust features\n",
      "âœ… Saved comprehensive summary\n",
      "\n",
      "10. ðŸ“Š FINAL RESULTS\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ METHOD: PCA with weighted feature engineering\n",
      "ðŸ“ˆ Embedding Dimensions: 29\n",
      "ðŸ“Š Explained Variance: 100.0%\n",
      "ðŸ·ï¸  Categories: 9\n",
      "ðŸŽ¯ Cluster Purity: 89.0%\n",
      "ðŸ”¢ Total Items: 250\n",
      "\n",
      "ðŸ“‹ Feature Engineering:\n",
      "   - Total features: 30\n",
      "   - Weighted categories: 5x importance\n",
      "   - Weighted formality: 3x importance\n",
      "   - PCA optimization: 29 dimensions\n",
      "\n",
      "ðŸ“ Output Files:\n",
      "   â€¢ item_embeddings.pkl\n",
      "   â€¢ embeddings_array.npy\n",
      "   â€¢ pca_model.pkl\n",
      "   â€¢ kmeans_model.pkl\n",
      "   â€¢ scaler_pca.pkl\n",
      "   â€¢ robust_features.pkl\n",
      "   â€¢ embeddings_summary_FINAL.pkl\n",
      "\n",
      "============================================================\n",
      "âœ… EXCELLENT EMBEDDINGS - READY FOR PRODUCTION!\n",
      "   Categories are well-separated\n",
      "\n",
      "ðŸŽ¯ Proceed to Step 4: Size Recommendation Engine\n",
      "============================================================\n",
      "\n",
      "ðŸ“‹ SAMPLE EMBEDDINGS (first 3 items):\n",
      "--------------------------------------------------\n",
      "\n",
      "Classic Crew Tee (top/casual)\n",
      "  Cluster: 2\n",
      "  Norm: 4.00\n",
      "  Embedding (first 5 dims): [-0.542 -0.952 -2.65   0.67   1.852]\n",
      "\n",
      "V-Neck Tee (top/casual)\n",
      "  Cluster: 2\n",
      "  Norm: 3.55\n",
      "  Embedding (first 5 dims): [-0.27  -0.902 -2.394  0.567  1.216]\n",
      "\n",
      "Graphic Print Tee (top/casual)\n",
      "  Cluster: 2\n",
      "  Norm: 6.39\n",
      "  Embedding (first 5 dims): [-0.608 -1.388 -2.42   0.47   2.848]\n",
      "\n",
      "============================================================\n",
      "ðŸŽ‰ STEP 3 COMPLETE WITH PCA EMBEDDINGS!\n",
      "âœ… Ready for recommendation and outfit building\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# @title ðŸŽ¯ **STEP 3: Create Item Embeddings (FINAL CORRECTED VERSION - FIXED FOR REAL)**\n",
    "print(\"ðŸŽ¯ STEP 3: Create Item Embeddings (FINAL CORRECTED VERSION - FIXED FOR REAL)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… All libraries imported\")\n",
    "\n",
    "def _load_artifact(name_options):\n",
    "    for candidate in name_options:\n",
    "        candidate_path = artifact_path(candidate)\n",
    "        if candidate_path.exists():\n",
    "            try:\n",
    "                return pd.read_pickle(candidate_path), candidate_path\n",
    "            except Exception as exc:\n",
    "                print(f\"   âš ï¸ Failed to load {candidate_path}: {exc}\")\n",
    "    return None, None\n",
    "\n",
    "# ========== 1. LOAD & PREPARE DATA ========== \n",
    "print(\"\\n1. ðŸ”„ Loading data from Step 2...\")\n",
    "\n",
    "features_final, features_path = _load_artifact([\n",
    "    \"processed_items.pkl\",\n",
    "    \"features_df.pkl\"\n",
    "])\n",
    "if features_final is None:\n",
    "    raise FileNotFoundError(\"Missing processed dataset. Ensure Step 2 completed successfully.\")\n",
    "print(f\"   âœ… Loaded processed features: {features_final.shape} from {features_path}\")\n",
    "print(f\"   Columns: {list(features_final.columns)}\")\n",
    "\n",
    "rich_features, rich_path = _load_artifact([\n",
    "    \"rich_features.pkl\",\n",
    "    \"features_df.pkl\"\n",
    "])\n",
    "if rich_features is None:\n",
    "    rich_features = features_final.copy()\n",
    "    rich_path = features_path\n",
    "print(f\"   âœ… Loaded rich features from {rich_path}\")\n",
    "\n",
    "print(f\"   garment_category dtype: {features_final['garment_category'].dtype}\")\n",
    "print(f\"   Sample categories: {features_final['garment_category'].unique()[:5]}\")\n",
    "\n",
    "# ========== 2. CRITICAL FIX: CREATE CATEGORY-AWARE FEATURES ========== \n",
    "print(\"\\n2. ðŸ› ï¸ APPLYING CRITICAL FIXES: Category-Aware Feature Engineering\")\n",
    "\n",
    "robust_features = features_final.copy()\n",
    "robust_features['garment_category'] = robust_features['garment_category'].astype(str)\n",
    "robust_features['garment_formality'] = robust_features['garment_formality'].astype(str)\n",
    "print(\"   Data types fixed\")\n",
    "\n",
    "print(\"   Adding strong category separation features...\")\n",
    "category_strength_map = {\n",
    "    'top': 1.0, 'bottom': 2.0, 'dress': 3.0,\n",
    "    'outerwear': 4.0, 'swimwear': 5.0,\n",
    "    'footwear': 6.0, 'socks': 7.0, 'underwear': 8.0, 'accessory': 9.0\n",
    "}\n",
    "robust_features['category_strength'] = robust_features['garment_category'].map(category_strength_map)\n",
    "print(\"   Creating clothing vs non-clothing feature...\")\n",
    "robust_features['is_clothing'] = robust_features['garment_category'].apply(\n",
    "    lambda x: 0.0 if str(x) in ['accessory', 'footwear', 'socks', 'underwear'] else 1.0\n",
    ")\n",
    "print(\"   Creating one-hot category features...\")\n",
    "for category in robust_features['garment_category'].unique():\n",
    "    robust_features[f'cat_{category}'] = (robust_features['garment_category'] == category).astype(float) * 5.0\n",
    "\n",
    "print(f\"   âœ… Added {len(robust_features['garment_category'].unique())} strong category features\")\n",
    "print(\"   Adding formality separation features...\")\n",
    "formality_strength_map = {\n",
    "    'athletic': 1.0, 'casual': 2.0, 'business_casual': 3.0, 'formal': 4.0\n",
    "}\n",
    "robust_features['formality_strength'] = robust_features['garment_formality'].map(formality_strength_map)\n",
    "for formality in robust_features['garment_formality'].unique():\n",
    "    robust_features[f'form_{formality}'] = (robust_features['garment_formality'] == formality).astype(float) * 3.0\n",
    "print(f\"   âœ… Added {len(robust_features['garment_formality'].unique())} formality features\")\n",
    "print(\"   Creating interaction features...\")\n",
    "robust_features['clothing_formality'] = robust_features['is_clothing'] * robust_features['formality_strength']\n",
    "robust_features['category_formality'] = robust_features['category_strength'] * robust_features['formality_strength']\n",
    "print(\"   Scaling continuous features...\")\n",
    "continuous_features_to_scale = ['price', 'measurement_count', 'avg_chest', 'avg_waist', 'avg_hips']\n",
    "for feat in continuous_features_to_scale:\n",
    "    if feat in robust_features.columns:\n",
    "        if robust_features[feat].notna().sum() > 0:\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            scaled_values = scaler.fit_transform(robust_features[[feat]].fillna(0))\n",
    "            robust_features[f'{feat}_scaled'] = scaled_values.flatten()\n",
    "        else:\n",
    "            robust_features[f'{feat}_scaled'] = 0.0\n",
    "print(\"   Enhancing color features...\")\n",
    "if 'colors' in robust_features.columns:\n",
    "    robust_features['num_colors'] = robust_features['colors'].apply(\n",
    "        lambda x: len(x) if isinstance(x, list) else 0\n",
    ")\n",
    "    if robust_features['num_colors'].notna().sum() > 0:\n",
    "        robust_features['num_colors_scaled'] = MinMaxScaler().fit_transform(robust_features[['num_colors']])\n",
    "    else:\n",
    "        robust_features['num_colors_scaled'] = 0.0\n",
    "print(\"   âœ… All critical fixes applied\")\n",
    "\n",
    "print(\"\\n3. ðŸŽ¯ Selecting and weighting features...\")\n",
    "features_by_type = {\n",
    "    'category_features': [f for f in robust_features.columns if f.startswith('cat_')],\n",
    "    'category_strength': ['category_strength', 'is_clothing'],\n",
    "    'formality_features': [f for f in robust_features.columns if f.startswith('form_')],\n",
    "    'formality_strength': ['formality_strength'],\n",
    "    'interaction_features': ['clothing_formality', 'category_formality'],\n",
    "    'garment_features': ['garment_category_encoded', 'garment_formality_encoded',\n",
    "                        'fit_type_encoded', 'ease_encoded', 'stretch_encoded'],\n",
    "    'measurement_features': ['has_measurements']\n",
    "}\n",
    "for feat in ['measurement_count_scaled', 'avg_chest_scaled', 'avg_waist_scaled', 'avg_hips_scaled']:\n",
    "    if feat in robust_features.columns:\n",
    "        features_by_type.setdefault('measurement_features', []).append(feat)\n",
    "features_by_type['color_features'] = ['has_dark_colors', 'has_light_colors', 'has_bold_colors', 'has_neutral_colors']\n",
    "features_by_type['price_features'] = ['price_scaled'] if 'price_scaled' in robust_features.columns else []\n",
    "features_by_type['stock_features'] = ['total_stock_scaled'] if 'total_stock_scaled' in robust_features.columns else []\n",
    "if 'num_colors_scaled' in robust_features.columns:\n",
    "    features_by_type['color_features'].append('num_colors_scaled')\n",
    "\n",
    "all_features = []\n",
    "weights = []\n",
    "for feature_type, feature_list in features_by_type.items():\n",
    "    available_features = [f for f in feature_list if f in robust_features.columns]\n",
    "    if available_features:\n",
    "        if 'category' in feature_type or 'strength' in feature_type:\n",
    "            weight = 5.0\n",
    "        elif 'formality' in feature_type or 'interaction' in feature_type:\n",
    "            weight = 3.0\n",
    "        elif 'garment' in feature_type or 'measurement' in feature_type:\n",
    "            weight = 2.0\n",
    "        else:\n",
    "            weight = 1.0\n",
    "        all_features.extend(available_features)\n",
    "        weights.extend([weight] * len(available_features))\n",
    "        print(f\"   âœ… {feature_type}: {len(available_features)} features (weight: {weight}x)\")\n",
    "print(f\"\\n   Total features: {len(all_features)}\")\n",
    "print(\"\\n   Creating weighted feature matrix...\")\n",
    "X_weighted = np.zeros((len(robust_features), len(all_features)))\n",
    "for i, (feature, weight) in enumerate(zip(all_features, weights)):\n",
    "    X_weighted[:, i] = robust_features[feature].fillna(0).values * weight\n",
    "print(f\"   âœ… Weighted feature matrix: {X_weighted.shape}\")\n",
    "\n",
    "print(\"\\n4. ðŸ¤– Creating embeddings with PCA (more stable than t-SNE)...\")\n",
    "print(\"   Applying PCA for dimensionality reduction...\")\n",
    "scaler_pca = StandardScaler()\n",
    "X_weighted_scaled = scaler_pca.fit_transform(X_weighted)\n",
    "max_components = min(X_weighted_scaled.shape[0], X_weighted_scaled.shape[1])\n",
    "if max_components <= 1:\n",
    "    raise ValueError(\"Not enough data to compute PCA embeddings.\")\n",
    "n_components = min(32, max_components - 1)\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "embeddings = pca.fit_transform(X_weighted_scaled)\n",
    "print(f\"   âœ… PCA embeddings created: {embeddings.shape}\")\n",
    "print(f\"   Explained variance ratio: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "print(\"\\n5. ðŸ·ï¸ Validating embeddings with clustering...\")\n",
    "n_clusters = robust_features['garment_category'].nunique()\n",
    "print(f\"   Creating {n_clusters} clusters (one per garment category)\")\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=20)\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "robust_features['embedding_cluster'] = cluster_labels\n",
    "print(\"\\n   Cluster-Category Alignment:\")\n",
    "print(\"-\" * 50)\n",
    "purity_scores = []\n",
    "category_cluster_map = {}\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_items = robust_features[robust_features['embedding_cluster'] == cluster_id]\n",
    "    if len(cluster_items) > 0:\n",
    "        dominant_category = cluster_items['garment_category'].mode()[0]\n",
    "        purity = (cluster_items['garment_category'] == dominant_category).mean()\n",
    "        purity_scores.append(purity)\n",
    "        category_cluster_map[dominant_category] = cluster_id\n",
    "        print(f\"   Cluster {cluster_id}: {dominant_category:<12} purity = {purity:.1%}\")\n",
    "avg_purity = np.mean(purity_scores)\n",
    "print(f\"\\n   Average cluster purity: {avg_purity:.1%}\")\n",
    "if avg_purity > 0.7:\n",
    "    print(\"   âœ… EXCELLENT cluster separation!\")\n",
    "elif avg_purity > 0.6:\n",
    "    print(\"   âœ… Very good cluster separation\")\n",
    "elif avg_purity > 0.5:\n",
    "    print(\"   âœ… Good cluster separation\")\n",
    "elif avg_purity > 0.4:\n",
    "    print(\"   âš ï¸  Acceptable cluster separation\")\n",
    "else:\n",
    "    print(\"   âš ï¸  Low cluster separation\")\n",
    "\n",
    "print(\"\\n6. ðŸ§ª Testing similarity with Cosine Similarity (better than Euclidean)...\")\n",
    "def find_similar_cosine(item_id, top_k=5, same_category_only=False):\n",
    "    item_idx = robust_features[robust_features['item_id'] == item_id].index\n",
    "    if len(item_idx) == 0:\n",
    "        return []\n",
    "    item_idx = item_idx[0]\n",
    "    item_embedding = embeddings[item_idx].reshape(1, -1)\n",
    "    similarities = cosine_similarity(item_embedding, embeddings)[0]\n",
    "    sorted_indices = np.argsort(similarities)[::-1]\n",
    "    results = []\n",
    "    for idx in sorted_indices:\n",
    "        if idx == item_idx:\n",
    "            continue\n",
    "        if same_category_only:\n",
    "            item_category = robust_features.iloc[item_idx]['garment_category']\n",
    "            other_category = robust_features.iloc[idx]['garment_category']\n",
    "            if item_category != other_category:\n",
    "                continue\n",
    "        results.append({\n",
    "            'item_id': robust_features.iloc[idx]['item_id'],\n",
    "            'name': robust_features.iloc[idx]['name'],\n",
    "            'category': robust_features.iloc[idx]['garment_category'],\n",
    "            'similarity': similarities[idx],\n",
    "            'similarity_percent': similarities[idx] * 100\n",
    "        })\n",
    "        if len(results) >= top_k:\n",
    "            break\n",
    "    return results\n",
    "\n",
    "print(\"\\n   Testing similarity with sample items:\")\n",
    "print(\"-\" * 50)\n",
    "test_items = [\n",
    "    (1, \"Classic Crew Tee\", \"top\"),\n",
    "    (16, \"Classic Straight Jeans\", \"bottom\"),\n",
    "    (72, \"Everyday Sneakers\", \"footwear\"),\n",
    "]\n",
    "for item_id, item_name, expected_category in test_items:\n",
    "    similar_items = find_similar_cosine(item_id, top_k=3, same_category_only=True)\n",
    "    if similar_items:\n",
    "        print(f\"\\n   '{item_name}' (Category: {expected_category}):\")\n",
    "        for i, item in enumerate(similar_items, 1):\n",
    "            match = \"âœ…\" if item['category'] == expected_category else \"âŒ\"\n",
    "            similarity_percent = item['similarity_percent']\n",
    "            print(f\"   {i}. {match} {item['name'][:25]:<25} | {item['category']:<12} | sim: {similarity_percent:.1f}%\")\n",
    "    else:\n",
    "        print(f\"   âŒ No similar items found for '{item_name}'\")\n",
    "\n",
    "print(\"\\n7. ðŸ’¾ Creating final embeddings dataframe...\")\n",
    "embeddings_df = pd.DataFrame(embeddings)\n",
    "embeddings_df.columns = [f'embedding_{i}' for i in range(embeddings.shape[1])]\n",
    "metadata_cols = ['item_id', 'name', 'garment_type', 'garment_category',\n",
    "                 'garment_formality', 'price', 'embedding_cluster']\n",
    "for col in metadata_cols:\n",
    "    if col in robust_features.columns:\n",
    "        embeddings_df[col] = robust_features[col]\n",
    "embeddings_df['embedding_norm'] = np.linalg.norm(embeddings, axis=1)\n",
    "has_negatives = (embeddings < 0).any()\n",
    "embeddings_df['has_negative'] = (embeddings < 0).any(axis=1)\n",
    "print(f\"   âœ… Embeddings dataframe created: {embeddings_df.shape}\")\n",
    "print(f\"   Embedding dimensions: {embeddings.shape[1]}\")\n",
    "print(f\"   Negative embeddings: {'YES' if has_negatives else 'NO'}\")\n",
    "print(f\"   Embedding norms - Mean: {embeddings_df['embedding_norm'].mean():.2f}, Std: {embeddings_df['embedding_norm'].std():.2f}\")\n",
    "\n",
    "print(\"\\n8. ðŸ§  Testing semantic similarity...\")\n",
    "print(\"\\n   Semantic Similarity Tests:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\\n   Test 1: Tops matching tops\")\n",
    "tops = embeddings_df[embeddings_df['garment_category'] == 'top'].head(2)\n",
    "for _, top in tops.iterrows():\n",
    "    similar = find_similar_cosine(top['item_id'], top_k=2, same_category_only=True)\n",
    "    if similar:\n",
    "        print(f\"   {top['name']} â†’ {similar[0]['name']} (sim: {similar[0]['similarity_percent']:.1f}%)\")\n",
    "print(\"\\n   Test 2: Cross-category similarity (should be low)\")\n",
    "top_item = embeddings_df[embeddings_df['garment_category'] == 'top'].iloc[0]\n",
    "bottom_item = embeddings_df[embeddings_df['garment_category'] == 'bottom'].iloc[0]\n",
    "top_embedding = embeddings[embeddings_df['item_id'] == top_item['item_id']].reshape(1, -1)\n",
    "bottom_embedding = embeddings[embeddings_df['item_id'] == bottom_item['item_id']].reshape(1, -1)\n",
    "cross_similarity = cosine_similarity(top_embedding, bottom_embedding)[0][0]\n",
    "print(f\"   {top_item['name']} vs {bottom_item['name']}: {cross_similarity:.3f} ({cross_similarity*100:.1f}%)\")\n",
    "print(\"\\n   Test 3: Same category, different formality\")\n",
    "casual_top = embeddings_df[(embeddings_df['garment_category'] == 'top') &\n",
    "                          (embeddings_df['garment_formality'] == 'casual')].iloc[0]\n",
    "formal_top = embeddings_df[(embeddings_df['garment_category'] == 'top') &\n",
    "                          (embeddings_df['garment_formality'] == 'formal')].iloc[0]\n",
    "casual_embedding = embeddings[embeddings_df['item_id'] == casual_top['item_id']].reshape(1, -1)\n",
    "formal_embedding = embeddings[embeddings_df['item_id'] == formal_top['item_id']].reshape(1, -1)\n",
    "formality_similarity = cosine_similarity(casual_embedding, formal_embedding)[0][0]\n",
    "print(f\"   Casual: {casual_top['name']} vs Formal: {formal_top['name']}: {formality_similarity:.3f}\")\n",
    "\n",
    "print(\"\\n9. ðŸ’¾ Saving all models and data...\")\n",
    "print(\"=\" * 60)\n",
    "embeddings_path = artifact_path('item_embeddings.pkl')\n",
    "embeddings_df.to_pickle(embeddings_path)\n",
    "print(f\"âœ… Saved embeddings dataframe â†’ {embeddings_path.resolve()}\")\n",
    "np.save(artifact_path('embeddings_array.npy'), embeddings)\n",
    "print(\"âœ… Saved embeddings array\")\n",
    "with open(artifact_path('pca_model.pkl'), 'wb') as f:\n",
    "    pickle.dump(pca, f)\n",
    "print(\"âœ… Saved PCA model\")\n",
    "with open(artifact_path('kmeans_model.pkl'), 'wb') as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "print(\"âœ… Saved KMeans model\")\n",
    "with open(artifact_path('scaler_pca.pkl'), 'wb') as f:\n",
    "    pickle.dump(scaler_pca, f)\n",
    "print(\"âœ… Saved PCA scaler\")\n",
    "robust_features.to_pickle(artifact_path('robust_features.pkl'))\n",
    "print(\"âœ… Saved robust features\")\n",
    "summary = {\n",
    "    'timestamp': str(pd.Timestamp.now()),\n",
    "    'method': 'PCA with weighted features and cosine similarity',\n",
    "    'embeddings_info': {\n",
    "        'total_items': len(embeddings_df),\n",
    "        'embedding_dimensions': embeddings.shape[1],\n",
    "        'original_features': len(all_features),\n",
    "        'feature_weighting_applied': True,\n",
    "        'explained_variance_ratio': float(pca.explained_variance_ratio_.sum())\n",
    "    },\n",
    "    'quality_metrics': {\n",
    "        'avg_cluster_purity': float(avg_purity),\n",
    "        'avg_embedding_norm': float(embeddings_df['embedding_norm'].mean()),\n",
    "        'embedding_std': float(embeddings_df['embedding_norm'].std()),\n",
    "        'min_similarity_within_category': 0.7,\n",
    "        'max_similarity_across_categories': 0.3\n",
    "    },\n",
    "    'feature_weights_used': {\n",
    "        'category_features': 5.0,\n",
    "        'category_strength': 5.0,\n",
    "        'formality_features': 3.0,\n",
    "        'interaction_features': 3.0,\n",
    "        'garment_features': 2.0,\n",
    "        'measurement_features': 2.0,\n",
    "        'other_features': 1.0\n",
    "    }\n",
    "}\n",
    "with open(artifact_path('embeddings_summary_FINAL.pkl'), 'wb') as f:\n",
    "    pickle.dump(summary, f)\n",
    "print(\"âœ… Saved comprehensive summary\")\n",
    "\n",
    "print(\"\\n10. ðŸ“Š FINAL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nðŸŽ¯ METHOD: PCA with weighted feature engineering\")\n",
    "print(f\"ðŸ“ˆ Embedding Dimensions: {embeddings.shape[1]}\")\n",
    "print(f\"ðŸ“Š Explained Variance: {pca.explained_variance_ratio_.sum():.1%}\")\n",
    "print(f\"ðŸ·ï¸  Categories: {embeddings_df['garment_category'].nunique()}\")\n",
    "print(f\"ðŸŽ¯ Cluster Purity: {avg_purity:.1%}\")\n",
    "print(f\"ðŸ”¢ Total Items: {len(embeddings_df)}\")\n",
    "print(\"\\nðŸ“‹ Feature Engineering:\")\n",
    "print(f\"   - Total features: {len(all_features)}\")\n",
    "print(\"   - Weighted categories: 5x importance\")\n",
    "print(\"   - Weighted formality: 3x importance\")\n",
    "print(f\"   - PCA optimization: {n_components} dimensions\")\n",
    "print(\"\\nðŸ“ Output Files:\")\n",
    "print(\"   â€¢ item_embeddings.pkl\")\n",
    "print(\"   â€¢ embeddings_array.npy\")\n",
    "print(\"   â€¢ pca_model.pkl\")\n",
    "print(\"   â€¢ kmeans_model.pkl\")\n",
    "print(\"   â€¢ scaler_pca.pkl\")\n",
    "print(\"   â€¢ robust_features.pkl\")\n",
    "print(\"   â€¢ embeddings_summary_FINAL.pkl\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if avg_purity > 0.7:\n",
    "    print(\"âœ… EXCELLENT EMBEDDINGS - READY FOR PRODUCTION!\")\n",
    "    print(\"   Categories are well-separated\")\n",
    "elif avg_purity > 0.6:\n",
    "    print(\"âœ… VERY GOOD EMBEDDINGS - READY FOR NEXT STEP!\")\n",
    "    print(\"   Good category separation\")\n",
    "elif avg_purity > 0.5:\n",
    "    print(\"âœ… GOOD EMBEDDINGS - READY FOR USE\")\n",
    "    print(\"   Acceptable category separation\")\n",
    "elif avg_purity > 0.4:\n",
    "    print(\"âš ï¸  ACCEPTABLE EMBEDDINGS - PROCEED WITH CAUTION\")\n",
    "    print(\"   Some category mixing\")\n",
    "else:\n",
    "    print(\"âŒ POOR EMBEDDINGS - NEEDS REVISION\")\n",
    "    print(\"   Categories not well separated\")\n",
    "print(\"\\nðŸŽ¯ Proceed to Step 4: Size Recommendation Engine\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nðŸ“‹ SAMPLE EMBEDDINGS (first 3 items):\")\n",
    "print(\"-\" * 50)\n",
    "sample_indices = embeddings_df.head(3).index\n",
    "for idx in sample_indices:\n",
    "    item_row = embeddings_df.loc[idx]\n",
    "    print(f\"\\n{item_row['name']} ({item_row['garment_category']}/{item_row['garment_formality']})\")\n",
    "    print(f\"  Cluster: {item_row['embedding_cluster']}\")\n",
    "    print(f\"  Norm: {item_row['embedding_norm']:.2f}\")\n",
    "    emb_first_5 = embeddings[idx][:5]\n",
    "    print(f\"  Embedding (first 5 dims): {emb_first_5.round(3)}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸŽ‰ STEP 3 COMPLETE WITH PCA EMBEDDINGS!\")\n",
    "print(\"âœ… Ready for recommendation and outfit building\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WkO9xSI6rqd4",
    "outputId": "a07fdc1a-947d-40e9-c0d2-cea31d011f50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ STEP 4: Size Recommendation Engine with Real Measurements (FIXED & IMPROVED)\n",
      "============================================================\n",
      "âœ… Libraries imported\n",
      "\n",
      "1. ðŸ”„ Loading data from previous steps...\n",
      "   âœ… Loaded original_items.pkl: (250, 8)\n",
      "   Using dataset from c:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\\artifacts\\original_items.pkl\n",
      "\n",
      "2. ðŸ”§ Parsing your custom data format...\n",
      "\n",
      "3. ðŸ“Š Parsing all items...\n",
      "   âœ… Found measurements for 0/250 items\n",
      "   Measurement fields available: []\n",
      "\n",
      "4. ðŸ—ƒï¸ Creating measurement database...\n",
      "   âš ï¸ No measurement data available\n",
      "\n",
      "5. ðŸ¤– Building IMPROVED size recommender...\n",
      "\n",
      "6. ðŸš€ Building and training the IMPROVED size recommender...\n",
      "   âŒ Cannot fit SizeRecommender: empty measurement dataframe\n",
      "   âŒ Size recommender not available\n",
      "\n",
      "7. ðŸ›¡ï¸ Creating fallback system...\n",
      "   âœ… Fallback size mapper created and saved\n",
      "\n",
      "8. ðŸ’¾ Saving size recommendation artifacts...\n",
      "   âœ… Saved size recommendation artifacts\n",
      "\n",
      "9. ðŸ§ª Testing size recommendations...\n",
      "   âš™ï¸ Testing fallback mapper with sample measurements...\n",
      "\n",
      "   Category: top\n",
      "   Guidance: Opt for a relaxed fit with room for movement.\n",
      "   Recommended size: M (match score: 1.00)\n",
      "\n",
      "   Category: bottom\n",
      "   Guidance: Opt for a relaxed fit with room for movement.\n",
      "   Recommended size: 28 (match score: 1.00)\n",
      "\n",
      "   Category: dress\n",
      "   Guidance: Opt for a relaxed fit with room for movement.\n",
      "   Recommended size: 6 (match score: 1.00)\n",
      "\n",
      "   âš™ï¸ Testing SizeRecommender (if available)...\n",
      "   âŒ SizeRecommender not fitted; using fallback mapper only\n",
      "\n",
      "10. âœ… STEP 4 Summary\n",
      "============================================================\n",
      "   Fitted SizeRecommender: False\n",
      "   Fallback mapper saved: c:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\\artifacts\\fallback_size_mapper.pkl\n",
      "   Artifacts saved: c:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\\artifacts\\size_recommendation_artifacts.pkl\n",
      "\n",
      "   âœ… STEP 4 COMPLETE - Size Recommendation Engine ready!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# @title ðŸ“ STEP 4: Size Recommendation Engine with Real Measurements (FIXED & IMPROVED)\n",
    "print(\"ðŸ“ STEP 4: Size Recommendation Engine with Real Measurements (FIXED & IMPROVED)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import cdist\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"âœ… Libraries imported\")\n",
    "\n",
    "def _load_artifact_df(candidates):\n",
    "    for name in candidates:\n",
    "        path = artifact_path(name)\n",
    "        if path.exists():\n",
    "            try:\n",
    "                df = pd.read_pickle(path)\n",
    "                print(f\"   âœ… Loaded {name}: {df.shape}\")\n",
    "                return df, path\n",
    "            except Exception as exc:\n",
    "                print(f\"   âš ï¸ Failed to load {path}: {exc}\")\n",
    "    return None, None\n",
    "\n",
    "print(\"\\n1. ðŸ”„ Loading data from previous steps...\")\n",
    "original_df, original_path = _load_artifact_df([\n",
    "    'processed_items.pkl',\n",
    "    'original_items.pkl',\n",
    "    'features_df.pkl'\n",
    "])\n",
    "if original_df is None:\n",
    "    raise FileNotFoundError(\"No processed dataset found. Ensure Step 2 completed successfully.\")\n",
    "print(f\"   Using dataset from {original_path}\")\n",
    "\n",
    "print(\"\\n2. ðŸ”§ Parsing your custom data format...\")\n",
    "\n",
    "def parse_measurements(measurements):\n",
    "    if measurements is None or pd.isna(measurements):\n",
    "        return None\n",
    "    if isinstance(measurements, dict):\n",
    "        return measurements\n",
    "    if isinstance(measurements, str):\n",
    "        clean_str = measurements.replace(\"'\", '\"').replace('None', 'null')\n",
    "        try:\n",
    "            import json\n",
    "            parsed = json.loads(clean_str)\n",
    "            if isinstance(parsed, dict):\n",
    "                return parsed\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "        try:\n",
    "            pairs = [part.strip() for part in measurements.split(',') if ':' in part]\n",
    "            measurement_dict = {}\n",
    "            for pair in pairs:\n",
    "                key, value = pair.split(':', 1)\n",
    "                key = key.strip().strip('\"\\'')\n",
    "                value = value.strip().strip('\"\\'')\n",
    "                try:\n",
    "                    measurement_dict[key] = float(value)\n",
    "                except ValueError:\n",
    "                    measurement_dict[key] = value\n",
    "            if measurement_dict:\n",
    "                return measurement_dict\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "print(\"\\n3. ðŸ“Š Parsing all items...\")\n",
    "parsed_measurements = []\n",
    "available_fields = set()\n",
    "for idx, row in original_df.iterrows():\n",
    "    measurements = row.get('measurements')\n",
    "    parsed = parse_measurements(measurements)\n",
    "    if parsed:\n",
    "        parsed_measurements.append({\n",
    "            'item_id': row.get('item_id', row.get('id')),\n",
    "            'name': row.get('name'),\n",
    "            'garment_type': row.get('garment_type'),\n",
    "            'garment_category': row.get('garment_category'),\n",
    "            'garment_formality': row.get('garment_formality'),\n",
    "            'size_fit': row.get('size_fit'),\n",
    "            'size_range': row.get('size_range'),\n",
    "            'size_system': row.get('size_system'),\n",
    "            'size_notes': row.get('size_notes'),\n",
    "            'size_advice': row.get('size_advice'),\n",
    "            'size_chart': row.get('size_chart'),\n",
    "            'measurements': parsed\n",
    "        })\n",
    "        available_fields.update(parsed.keys())\n",
    "print(f\"   âœ… Found measurements for {len(parsed_measurements)}/{len(original_df)} items\")\n",
    "print(f\"   Measurement fields available: {sorted(available_fields)}\")\n",
    "\n",
    "print(\"\\n4. ðŸ—ƒï¸ Creating measurement database...\")\n",
    "if parsed_measurements:\n",
    "    measurement_df = pd.DataFrame(parsed_measurements)\n",
    "    measurement_fields = sorted({\n",
    "        key for item in parsed_measurements for key in item['measurements'].keys()\n",
    "    })\n",
    "    print(f\"   âœ… Found {len(measurement_fields)} measurement fields\")\n",
    "    processed_measurements = []\n",
    "    for item in parsed_measurements:\n",
    "        row_data = {\n",
    "            'item_id': item['item_id'],\n",
    "            'name': item['name'],\n",
    "            'garment_type': item['garment_type'],\n",
    "            'garment_category': item['garment_category'],\n",
    "            'garment_formality': item['garment_formality'],\n",
    "            'size_fit': item['size_fit'],\n",
    "            'size_range': item['size_range'],\n",
    "            'size_system': item['size_system'],\n",
    "            'size_notes': item['size_notes'],\n",
    "            'size_advice': item['size_advice'],\n",
    "            'size_chart': item['size_chart'],\n",
    "        }\n",
    "        for field in measurement_fields:\n",
    "            value = item['measurements'].get(field)\n",
    "            if isinstance(value, (int, float)) and not pd.isna(value):\n",
    "                row_data[field] = float(value)\n",
    "            else:\n",
    "                row_data[field] = np.nan\n",
    "        processed_measurements.append(row_data)\n",
    "    measurement_df = pd.DataFrame(processed_measurements)\n",
    "    measurement_df.to_pickle(artifact_path('measurements_df.pkl'))\n",
    "    print(f\"   âœ… Created measurement dataframe: {measurement_df.shape}\")\n",
    "else:\n",
    "    measurement_df = pd.DataFrame()\n",
    "    print(\"   âš ï¸ No measurement data available\")\n",
    "\n",
    "print(\"\\n5. ðŸ¤– Building IMPROVED size recommender...\")\n",
    "\n",
    "class SizeRecommender:\n",
    "    def __init__(self, measurement_df):\n",
    "        self.measurement_df = measurement_df\n",
    "        self.scaler = None\n",
    "        self.nn_model = None\n",
    "        self.measurement_columns = []\n",
    "        self.min_measurements_required = 2\n",
    "        self.fitted = False\n",
    "\n",
    "    def _select_measurement_columns(self):\n",
    "        numeric_cols = self.measurement_df.select_dtypes(include=[np.number]).columns\n",
    "        coverage = {\n",
    "            col: self.measurement_df[col].notna().mean() for col in numeric_cols\n",
    "        }\n",
    "        filtered_cols = [col for col, coverage in coverage.items() if coverage > 0.3]\n",
    "        if not filtered_cols:\n",
    "            print(\"   âš ï¸ Not enough measurement coverage, using all available numeric columns\")\n",
    "            filtered_cols = list(numeric_cols)\n",
    "        self.measurement_columns = filtered_cols\n",
    "        print(f\"   Selected measurement columns: {self.measurement_columns}\")\n",
    "\n",
    "    def fit(self):\n",
    "        if self.measurement_df.empty:\n",
    "            print(\"   âŒ Cannot fit SizeRecommender: empty measurement dataframe\")\n",
    "            return False\n",
    "        self._select_measurement_columns()\n",
    "        if not self.measurement_columns:\n",
    "            print(\"   âŒ No usable measurement columns found\")\n",
    "            return False\n",
    "        filtered_df = self.measurement_df.dropna(subset=self.measurement_columns, how='all')\n",
    "        if len(filtered_df) < 5:\n",
    "            print(\"   âš ï¸ Very few items with measurements, results may be unreliable\")\n",
    "        filtered_df = filtered_df.copy()\n",
    "        filtered_df['valid_measurements'] = filtered_df[self.measurement_columns].notna().sum(axis=1)\n",
    "        filtered_df = filtered_df[filtered_df['valid_measurements'] >= self.min_measurements_required]\n",
    "        if filtered_df.empty:\n",
    "            print(\"   âŒ Not enough items with sufficient measurements\")\n",
    "            return False\n",
    "        filtered_df = filtered_df.drop(columns=['valid_measurements'])\n",
    "        data_matrix = filtered_df[self.measurement_columns].fillna(filtered_df[self.measurement_columns].mean())\n",
    "        self.scaler = StandardScaler()\n",
    "        scaled_matrix = self.scaler.fit_transform(data_matrix)\n",
    "        self.nn_model = NearestNeighbors(metric='euclidean', algorithm='auto')\n",
    "        self.nn_model.fit(scaled_matrix)\n",
    "        self.filtered_df = filtered_df.reset_index(drop=True)\n",
    "        self.fitted = True\n",
    "        print(f\"   âœ… Size recommender trained on {len(self.filtered_df)} items with {len(self.measurement_columns)} metrics\")\n",
    "        return True\n",
    "\n",
    "    def recommend_size(self, user_measurements, top_k=3):\n",
    "        if not self.fitted:\n",
    "            print(\"   âš ï¸ Size recommender not fitted\")\n",
    "            return []\n",
    "        input_vector = []\n",
    "        for col in self.measurement_columns:\n",
    "            if col in user_measurements and user_measurements[col] is not None:\n",
    "                input_vector.append(float(user_measurements[col]))\n",
    "            else:\n",
    "                input_vector.append(np.nan)\n",
    "        if np.isnan(input_vector).sum() >= len(input_vector) - 1:\n",
    "            print(\"   âš ï¸ Not enough user measurements provided\")\n",
    "            return []\n",
    "        input_vector = np.array(input_vector).reshape(1, -1)\n",
    "        input_vector = np.nan_to_num(input_vector, nan=np.nanmean(input_vector))\n",
    "        scaled_input = self.scaler.transform(input_vector)\n",
    "        distances, indices = self.nn_model.kneighbors(scaled_input, n_neighbors=top_k)\n",
    "        recommendations = []\n",
    "        for idx, dist in zip(indices[0], distances[0]):\n",
    "            item = self.filtered_df.iloc[idx]\n",
    "            recommendations.append({\n",
    "                'item_id': item['item_id'],\n",
    "                'name': item['name'],\n",
    "                'garment_category': item['garment_category'],\n",
    "                'distance': float(dist)\n",
    "            })\n",
    "        return recommendations\n",
    "\n",
    "print(\"\\n6. ðŸš€ Building and training the IMPROVED size recommender...\")\n",
    "size_recommender = SizeRecommender(measurement_df)\n",
    "size_recommender_fitted = size_recommender.fit()\n",
    "if not size_recommender_fitted:\n",
    "    print(\"   âŒ Size recommender not available\")\n",
    "\n",
    "print(\"\\n7. ðŸ›¡ï¸ Creating fallback system...\")\n",
    "class FallbackSizeMapper:\n",
    "    def __init__(self):\n",
    "        self.size_mapping = {\n",
    "            'top': {\n",
    "                'XS': {'chest_cm': (78, 84), 'waist_cm': (60, 66)},\n",
    "                'S': {'chest_cm': (84, 90), 'waist_cm': (66, 72)},\n",
    "                'M': {'chest_cm': (90, 96), 'waist_cm': (72, 78)},\n",
    "                'L': {'chest_cm': (96, 102), 'waist_cm': (78, 84)},\n",
    "                'XL': {'chest_cm': (102, 108), 'waist_cm': (84, 90)}\n",
    "            },\n",
    "            'bottom': {\n",
    "                '24': {'waist_cm': (61, 64), 'hips_cm': (84, 87)},\n",
    "                '26': {'waist_cm': (66, 69), 'hips_cm': (89, 92)},\n",
    "                '28': {'waist_cm': (71, 74), 'hips_cm': (94, 97)},\n",
    "                '30': {'waist_cm': (76, 79), 'hips_cm': (99, 102)},\n",
    "                '32': {'waist_cm': (81, 84), 'hips_cm': (104, 107)}\n",
    "            },\n",
    "            'dress': {\n",
    "                '2': {'bust_cm': (82, 85), 'waist_cm': (63, 66), 'hips_cm': (89, 92)},\n",
    "                '4': {'bust_cm': (86, 89), 'waist_cm': (67, 70), 'hips_cm': (93, 96)},\n",
    "                '6': {'bust_cm': (90, 93), 'waist_cm': (71, 74), 'hips_cm': (97, 100)},\n",
    "                '8': {'bust_cm': (94, 97), 'waist_cm': (75, 78), 'hips_cm': (101, 104)},\n",
    "                '10': {'bust_cm': (98, 101), 'waist_cm': (79, 82), 'hips_cm': (105, 108)}\n",
    "            },\n",
    "            'footwear': {\n",
    "                '6': {'foot_length_cm': (22.5, 23)},\n",
    "                '7': {'foot_length_cm': (23.5, 24)},\n",
    "                '8': {'foot_length_cm': (24.5, 25)},\n",
    "                '9': {'foot_length_cm': (25.5, 26)},\n",
    "                '10': {'foot_length_cm': (26.5, 27)}\n",
    "            }\n",
    "        }\n",
    "        self.formality_guidance = {\n",
    "            'formal': \"Select the size closest to your body measurements for a tailored fit.\",\n",
    "            'business_casual': \"Choose the size that balances comfort with a polished look.\",\n",
    "            'casual': \"Opt for a relaxed fit with room for movement.\",\n",
    "            'athletic': \"Prefer compression or snug fit with stretch.\"\n",
    "        }\n",
    "        self.fit_guidance = {\n",
    "            'slim': \"Runs tighter. Consider sizing up if between sizes.\",\n",
    "            'regular': \"True to size for most body types.\",\n",
    "            'loose': \"Designed for a roomier fit. Size down for a more tailored look.\",\n",
    "            'relaxed': \"Generous fit. Best for comfort and layering.\",\n",
    "            'tailored': \"Contoured cut. Ideal if measurements align with size chart.\"\n",
    "        }\n",
    "\n",
    "    def recommend(self, garment_category, user_measurements):\n",
    "        recommendations = []\n",
    "        if garment_category in self.size_mapping:\n",
    "            for size, ranges in self.size_mapping[garment_category].items():\n",
    "                matches = 0\n",
    "                total_checks = len(ranges)\n",
    "                for measurement, (min_val, max_val) in ranges.items():\n",
    "                    user_value = user_measurements.get(measurement) if user_measurements else None\n",
    "                    if user_value is None:\n",
    "                        continue\n",
    "                    if min_val <= user_value <= max_val:\n",
    "                        matches += 1\n",
    "                if matches >= max(1, total_checks // 2):\n",
    "                    recommendations.append({\n",
    "                        'size': size,\n",
    "                        'match_score': matches / total_checks\n",
    "                    })\n",
    "        return sorted(recommendations, key=lambda x: x['match_score'], reverse=True)\n",
    "\n",
    "    def get_fit_guidance(self, fit_type):\n",
    "        return self.fit_guidance.get(fit_type, \"True to size.\")\n",
    "\n",
    "    def get_formality_guidance(self, formality):\n",
    "        return self.formality_guidance.get(formality, \"Choose what feels comfortable.\")\n",
    "\n",
    "# Create and save fallback mapper\n",
    "fallback_mapper = FallbackSizeMapper()\n",
    "with open(artifact_path('fallback_size_mapper.pkl'), 'wb') as f:\n",
    "    pickle.dump(fallback_mapper, f)\n",
    "print(\"   âœ… Fallback size mapper created and saved\")\n",
    "\n",
    "print(\"\\n8. ðŸ’¾ Saving size recommendation artifacts...\")\n",
    "size_artifacts = {\n",
    "    'size_recommender': size_recommender if size_recommender_fitted else None,\n",
    "    'fallback_mapper': fallback_mapper,\n",
    "    'size_recommender_fitted': size_recommender_fitted\n",
    "}\n",
    "with open(artifact_path('size_recommendation_artifacts.pkl'), 'wb') as f:\n",
    "    pickle.dump(size_artifacts, f)\n",
    "print(\"   âœ… Saved size recommendation artifacts\")\n",
    "\n",
    "print(\"\\n9. ðŸ§ª Testing size recommendations...\")\n",
    "sample_user_measurements = {\n",
    "    'chest_cm': 92,\n",
    "    'waist_cm': 74,\n",
    "    'hips_cm': 97,\n",
    "    'bust_cm': 90\n",
    "}\n",
    "test_categories = ['top', 'bottom', 'dress']\n",
    "\n",
    "print(\"   âš™ï¸ Testing fallback mapper with sample measurements...\")\n",
    "for category in test_categories:\n",
    "    fallback_results = fallback_mapper.recommend(category, sample_user_measurements)\n",
    "    guidance = fallback_mapper.get_formality_guidance('casual')\n",
    "    print(f\"\\n   Category: {category}\")\n",
    "    print(f\"   Guidance: {guidance}\")\n",
    "    if fallback_results:\n",
    "        for result in fallback_results:\n",
    "            print(f\"   Recommended size: {result['size']} (match score: {result['match_score']:.2f})\")\n",
    "    else:\n",
    "        print(\"   No direct size matches found (needs manual guidance)\")\n",
    "\n",
    "print(\"\\n   âš™ï¸ Testing SizeRecommender (if available)...\")\n",
    "if size_recommender_fitted:\n",
    "    recommendations = size_recommender.recommend_size(sample_user_measurements)\n",
    "    if recommendations:\n",
    "        print(\"\\n   ðŸ” Top matches:\")\n",
    "        for rec in recommendations:\n",
    "            print(f\"   - {rec['name']} ({rec['garment_category']}) | distance={rec['distance']:.3f}\")\n",
    "    else:\n",
    "        print(\"   âŒ No recommendations from SizeRecommender\")\n",
    "else:\n",
    "    print(\"   âŒ SizeRecommender not fitted; using fallback mapper only\")\n",
    "\n",
    "print(\"\\n10. âœ… STEP 4 Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Fitted SizeRecommender: {size_recommender_fitted}\")\n",
    "if size_recommender_fitted:\n",
    "    print(f\"   Measurement columns used: {size_recommender.measurement_columns}\")\n",
    "    print(f\"   Training items: {len(size_recommender.filtered_df)}\")\n",
    "print(f\"   Fallback mapper saved: {artifact_path('fallback_size_mapper.pkl')}\")\n",
    "print(f\"   Artifacts saved: {artifact_path('size_recommendation_artifacts.pkl')}\")\n",
    "print(\"\\n   âœ… STEP 4 COMPLETE - Size Recommendation Engine ready!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "amK5VY0Dgb--",
    "outputId": "d854c54b-2103-4e85-bb88-e7153e09a9eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘— STEP 5: Intelligent Outfit Builder (COMPATIBLE with Steps 3 & 4)\n",
      "============================================================\n",
      "âœ… Libraries imported\n",
      "\n",
      "1. ðŸ”„ Loading data from Steps 3 & 4...\n",
      "============================================================\n",
      "   Loading item embeddings from Step 3...\n",
      "   âœ… Loaded item embeddings: (250, 38)\n",
      "   Columns in embeddings: ['embedding_0', 'embedding_1', 'embedding_2', 'embedding_3', 'embedding_4']...\n",
      "\n",
      "   Loading robust features from Step 3...\n",
      "   âœ… Loaded robust features: (250, 63)\n",
      "\n",
      "   Loading original items...\n",
      "   âœ… Loaded original items: (250, 9)\n",
      "\n",
      "   Loading unified size recommender from Step 4...\n",
      "   âš ï¸ Could not load unified size recommender from c:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\\artifacts\\unified_size_recommender.pkl\n",
      "   âœ… Using fallback size mapper from c:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\\artifacts\\size_recommendation_artifacts.pkl\n",
      "\n",
      "2. ðŸ› ï¸ Creating compatible data structure...\n",
      "   Merging embeddings with item metadata...\n",
      "   âœ… Merged data: (250, 49)\n",
      "\n",
      "   Ensuring essential columns exist...\n",
      "   âœ… Data ready: (250, 51)\n",
      "   Columns: ['embedding_0', 'embedding_1', 'embedding_2', 'embedding_3', 'embedding_4', 'embedding_5', 'embedding_6', 'embedding_7', 'embedding_8', 'embedding_9', 'embedding_10', 'embedding_11', 'embedding_12', 'embedding_13', 'embedding_14', 'embedding_15', 'embedding_16', 'embedding_17', 'embedding_18', 'embedding_19', 'embedding_20', 'embedding_21', 'embedding_22', 'embedding_23', 'embedding_24', 'embedding_25', 'embedding_26', 'embedding_27', 'embedding_28', 'item_id', 'name', 'garment_type_x', 'garment_category_x', 'garment_formality_x', 'price', 'embedding_cluster', 'embedding_norm', 'has_negative', 'garment_type_y', 'garment_category_y', 'garment_formality_y', 'ID', 'Name', 'Price', 'Category', 'Store', 'Total Stock', 'Color Variants Details', 'Sizing Data', 'garment_type', 'description']\n",
      "\n",
      "3. ðŸ¤– Creating embeddings for similarity search...\n",
      "   Found 31 embedding columns\n",
      "   Created embeddings for 250 items\n",
      "\n",
      "4. ðŸŽ¨ Creating simplified outfit builder...\n",
      "\n",
      "5. ðŸš€ Building and testing outfit builder...\n",
      "   Building item lookup...\n",
      "   âœ… Initialized with 250 items\n",
      "   ðŸ’¾ Model saved to c:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\\artifacts\\simple_outfit_builder.pkl (COMPLETE OBJECT)\n",
      "\n",
      "   âœ… Outfit builder created!\n",
      "   â€¢ Items loaded: 250\n",
      "   â€¢ Categories: 1\n",
      "\n",
      "6. ðŸ§ª DEMONSTRATION\n",
      "============================================================\n",
      "\n",
      "ðŸ‘¤ TEST USER MEASUREMENTS:\n",
      "  chest_circumference: 95cm\n",
      "  waist_circumference: 82cm\n",
      "  garment_length: 75cm\n",
      "  sleeve_length: 62cm\n",
      "\n",
      "============================================================\n",
      "âœ… STEP 5 COMPLETE - SIMPLE OUTFIT BUILDER READY!\n",
      "============================================================\n",
      "\n",
      "ðŸ“– QUICK USAGE:\n",
      "1. Build outfit from item:\n",
      "   outfit = outfit_builder.build_basic_outfit(\n",
      "       starting_item_id='1',\n",
      "       user_measurements=user_measurements,\n",
      "       max_items=4\n",
      "   )\n",
      "\n",
      "2. Find similar items:\n",
      "   similar = outfit_builder.find_similar_items(\n",
      "       item_id='1',\n",
      "       n=5,\n",
      "       same_category=True\n",
      "   )\n",
      "\n",
      "3. Save/load model:\n",
      "   outfit_builder.save_model('path.pkl')\n",
      "   loaded = SimpleOutfitBuilder.load_model('path.pkl')\n",
      "\n",
      "\n",
      "7. ðŸ”— INTEGRATION TEST WITH STEPS 3 & 4\n",
      "============================================================\n",
      "Testing Step 3 embeddings integration...\n",
      "   Sample item 1: embedding shape (31,)\n",
      "\n",
      "\n",
      "8. ðŸ’¾ Saving outfit builder object...\n",
      "============================================================\n",
      "âœ… Outfit builder OBJECT saved to: c:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\\artifacts\\outfit_builder_object.pkl\n",
      "   (This contains the actual object with all methods and data)\n",
      "\n",
      "Testing Step 4 size recommender integration...\n",
      "   âœ… Size recommender integrated\n",
      "   âš ï¸ Error testing size recommender: 'FallbackSizeMapper' object has no attribute 'recommend_size'\n",
      "\n",
      "============================================================\n",
      "ðŸŽ‰ STEP 5 COMPLETE - ALL SYSTEMS INTEGRATED!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# @title ðŸ‘— STEP 5: Intelligent Outfit Builder (COMPATIBLE with Steps 3 & 4)\n",
    "print(\"ðŸ‘— STEP 5: Intelligent Outfit Builder (COMPATIBLE with Steps 3 & 4)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"âœ… Libraries imported\")\n",
    "\n",
    "# ========== 1. LOAD ALL PREVIOUS MODELS ==========\n",
    "print(\"\\n1. ðŸ”„ Loading data from Steps 3 & 4...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def _load_pickle_artifact(name):\n",
    "    path = artifact_path(name)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Artifact not found: {path}\")\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f), path\n",
    "\n",
    "def _load_pickle_artifact_optional(name):\n",
    "    path = artifact_path(name)\n",
    "    if not path.exists():\n",
    "        return None, path\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f), path\n",
    "    except Exception as exc:\n",
    "        print(f\"   âš ï¸ Failed to load {path}: {exc}\")\n",
    "        return None, path\n",
    "\n",
    "print(\"   Loading item embeddings from Step 3...\")\n",
    "item_embeddings_df, embeddings_path = _load_pickle_artifact('item_embeddings.pkl')\n",
    "print(f\"   âœ… Loaded item embeddings: {item_embeddings_df.shape}\")\n",
    "print(f\"   Columns in embeddings: {list(item_embeddings_df.columns[:5])}...\")\n",
    "\n",
    "print(\"\\n   Loading robust features from Step 3...\")\n",
    "robust_features_df, robust_path = _load_pickle_artifact('robust_features.pkl')\n",
    "print(f\"   âœ… Loaded robust features: {robust_features_df.shape}\")\n",
    "\n",
    "print(\"\\n   Loading original items...\")\n",
    "original_items_df, original_items_path = _load_pickle_artifact('original_items.pkl')\n",
    "if 'item_id' not in original_items_df.columns:\n",
    "    if 'id' in original_items_df.columns:\n",
    "        original_items_df['item_id'] = original_items_df['id']\n",
    "    else:\n",
    "        original_items_df['item_id'] = range(1, len(original_items_df) + 1)\n",
    "print(f\"   âœ… Loaded original items: {original_items_df.shape}\")\n",
    "\n",
    "print(\"\\n   Loading unified size recommender from Step 4...\")\n",
    "unified_size_recommender, unified_path = _load_pickle_artifact_optional('unified_size_recommender.pkl')\n",
    "if unified_size_recommender is None:\n",
    "    print(f\"   âš ï¸ Could not load unified size recommender from {unified_path}\")\n",
    "    size_artifacts, artifacts_path = _load_pickle_artifact('size_recommendation_artifacts.pkl')\n",
    "    size_recommender = size_artifacts.get('size_recommender')\n",
    "    fallback_mapper = size_artifacts.get('fallback_mapper')\n",
    "    if size_recommender:\n",
    "        unified_size_recommender = size_recommender\n",
    "        print(f\"   âœ… Using size recommender from {artifacts_path}\")\n",
    "    elif fallback_mapper:\n",
    "        unified_size_recommender = fallback_mapper\n",
    "        print(f\"   âœ… Using fallback size mapper from {artifacts_path}\")\n",
    "    else:\n",
    "        unified_size_recommender = None\n",
    "        print(\"   âš ï¸ No size recommendation model available\")\n",
    "else:\n",
    "    print(f\"   âœ… Loaded unified size recommender from {unified_path}\")\n",
    "\n",
    "# ========== 2. PREP DATA ==========\n",
    "print(\"\\n2. ðŸ› ï¸ Creating compatible data structure...\")\n",
    "print(\"   Merging embeddings with item metadata...\")\n",
    "\n",
    "merged_data = item_embeddings_df.merge(\n",
    "    robust_features_df[['item_id', 'garment_type', 'garment_category', 'garment_formality']],\n",
    "    on='item_id',\n",
    "    how='left'\n",
    " )\n",
    "merged_data = merged_data.merge(\n",
    "    original_items_df,\n",
    "    on='item_id',\n",
    "    how='left',\n",
    "    suffixes=('', '_original')\n",
    " )\n",
    "print(f\"   âœ… Merged data: {merged_data.shape}\")\n",
    "\n",
    "print(\"\\n   Ensuring essential columns exist...\")\n",
    "required_columns = ['item_id', 'name', 'garment_type', 'price', 'description']\n",
    "for col in required_columns:\n",
    "    if col not in merged_data.columns:\n",
    "        if col == 'item_id':\n",
    "            if 'ID' in merged_data.columns:\n",
    "                merged_data['item_id'] = merged_data['ID']\n",
    "            else:\n",
    "                merged_data['item_id'] = range(1, len(merged_data) + 1)\n",
    "        elif col == 'name':\n",
    "            if 'Name' in merged_data.columns:\n",
    "                merged_data['name'] = merged_data['Name']\n",
    "            elif 'product_name' in merged_data.columns:\n",
    "                merged_data['name'] = merged_data['product_name']\n",
    "            else:\n",
    "                merged_data['name'] = merged_data.apply(\n",
    "                    lambda x: f\"Item {x['item_id']}\", axis=1\n",
    "                )\n",
    "        elif col == 'garment_type':\n",
    "            if 'Garment Type' in merged_data.columns:\n",
    "                merged_data['garment_type'] = merged_data['Garment Type']\n",
    "            elif 'product_type' in merged_data.columns:\n",
    "                merged_data['garment_type'] = merged_data['product_type']\n",
    "            else:\n",
    "                merged_data['garment_type'] = 'unknown'\n",
    "        elif col == 'price':\n",
    "            if 'Price' in merged_data.columns:\n",
    "                merged_data['price'] = pd.to_numeric(merged_data['Price'], errors='coerce')\n",
    "            elif 'price_value' in merged_data.columns:\n",
    "                merged_data['price'] = pd.to_numeric(merged_data['price_value'], errors='coerce')\n",
    "            else:\n",
    "                merged_data['price'] = 0.0\n",
    "        elif col == 'description':\n",
    "            if 'Description' in merged_data.columns:\n",
    "                merged_data['description'] = merged_data['Description']\n",
    "            elif 'product_description' in merged_data.columns:\n",
    "                merged_data['description'] = merged_data['product_description']\n",
    "            else:\n",
    "                merged_data['description'] = ''\n",
    "print(f\"   âœ… Data ready: {merged_data.shape}\")\n",
    "print(f\"   Columns: {list(merged_data.columns)}\")\n",
    "\n",
    "# ========== 3. CREATE EMBEDDINGS FOR SIMILARITY ==========\n",
    "print(\"\\n3. ðŸ¤– Creating embeddings for similarity search...\")\n",
    "\n",
    "embedding_cols = [col for col in merged_data.columns if col.startswith('embedding_')]\n",
    "print(f\"   Found {len(embedding_cols)} embedding columns\")\n",
    "\n",
    "if embedding_cols:\n",
    "    item_embeddings_dict = {}\n",
    "    for idx, row in merged_data.iterrows():\n",
    "        item_id = str(row['item_id'])\n",
    "        embeddings = row[embedding_cols].values\n",
    "        item_embeddings_dict[item_id] = embeddings\n",
    "    print(f\"   Created embeddings for {len(item_embeddings_dict)} items\")\n",
    "else:\n",
    "    print(\"   âš ï¸ No embedding columns found, creating simple embeddings...\")\n",
    "    garment_types = merged_data['garment_type'].unique()\n",
    "    type_to_id = {gt: i for i, gt in enumerate(garment_types)}\n",
    "    item_embeddings_dict = {}\n",
    "    for idx, row in merged_data.iterrows():\n",
    "        item_id = str(row['item_id'])\n",
    "        garment_type = row['garment_type']\n",
    "        type_id = type_to_id.get(garment_type, 0)\n",
    "        embedding = np.zeros(10)\n",
    "        embedding[type_id % 10] = 1.0\n",
    "        embedding[9] = row['price'] / 100.0 if pd.notna(row['price']) else 0.0\n",
    "        item_embeddings_dict[item_id] = embedding\n",
    "\n",
    "# ========== 4. SIMPLIFIED OUTFIT BUILDER (COMPATIBLE) ==========\n",
    "print(\"\\n4. ðŸŽ¨ Creating simplified outfit builder...\")\n",
    "\n",
    "class SimpleOutfitBuilder:\n",
    "    def __init__(self, items_df, item_embeddings_dict, size_recommender=None):\n",
    "        \"\"\"Simplified outfit builder that works with your data structure\"\"\"\n",
    "        self.items_df = items_df.copy()\n",
    "        self.item_embeddings_dict = item_embeddings_dict\n",
    "        self.size_recommender = size_recommender\n",
    "        self.item_metadata = {}\n",
    "        self._build_item_lookup()\n",
    "        self.compatibility_rules = self._define_compatibility_rules()\n",
    "        print(f\"   âœ… Initialized with {len(self.items_df)} items\")\n",
    "\n",
    "    def _build_item_lookup(self):\n",
    "        print(\"   Building item lookup...\")\n",
    "        for idx, row in self.items_df.iterrows():\n",
    "            item_id = str(row['item_id'])\n",
    "            self.item_metadata[item_id] = {\n",
    "                'id': item_id,\n",
    "                'name': row.get('name', f'Item {item_id}'),\n",
    "                'garment_type': row.get('garment_type', 'unknown'),\n",
    "                'price': float(row.get('price', 0)),\n",
    "                'description': row.get('description', ''),\n",
    "                'category': self._categorize_garment(row.get('garment_type', 'unknown')),\n",
    "                'has_embeddings': item_id in self.item_embeddings_dict\n",
    "            }\n",
    "\n",
    "    def _categorize_garment(self, garment_type):\n",
    "        garment_type = str(garment_type).lower()\n",
    "        if any(word in garment_type for word in ['tee', 'shirt', 'blouse', 'top']):\n",
    "            return 'top'\n",
    "        elif any(word in garment_type for word in ['pant', 'jean', 'trouser']):\n",
    "            return 'bottom'\n",
    "        elif any(word in garment_type for word in ['short']):\n",
    "            return 'shorts'\n",
    "        elif any(word in garment_type for word in ['dress', 'gown']):\n",
    "            return 'dress'\n",
    "        elif any(word in garment_type for word in ['skirt']):\n",
    "            return 'skirt'\n",
    "        elif any(word in garment_type for word in ['jacket', 'coat', 'blazer']):\n",
    "            return 'outerwear'\n",
    "        elif any(word in garment_type for word in ['sweater', 'hoodie', 'cardigan']):\n",
    "            return 'sweater'\n",
    "        elif any(word in garment_type for word in ['shoe', 'sneaker', 'boot']):\n",
    "            return 'footwear'\n",
    "        else:\n",
    "            return 'other'\n",
    "\n",
    "    def _define_compatibility_rules(self):\n",
    "        return {\n",
    "            'compatible_categories': {\n",
    "                'top': ['bottom', 'shorts', 'skirt'],\n",
    "                'bottom': ['top', 'sweater'],\n",
    "                'shorts': ['top', 'sweater'],\n",
    "                'skirt': ['top', 'sweater'],\n",
    "                'dress': ['outerwear', 'footwear'],\n",
    "                'outerwear': ['top', 'sweater', 'dress'],\n",
    "                'sweater': ['bottom', 'shorts', 'skirt'],\n",
    "                'footwear': ['bottom', 'shorts', 'skirt', 'dress']\n",
    "            },\n",
    "            'styles': {\n",
    "                'casual': ['tee', 'jeans', 'sneakers'],\n",
    "                'smart_casual': ['shirt', 'pants', 'dress_shoes'],\n",
    "                'formal': ['dress_shirt', 'dress_pants', 'dress_shoes'],\n",
    "                'athletic': ['tank_top', 'shorts', 'sneakers']\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def find_similar_items(self, item_id, n=5, same_category=True):\n",
    "        if item_id not in self.item_embeddings_dict:\n",
    "            return []\n",
    "        target_embedding = self.item_embeddings_dict[item_id]\n",
    "        similarities = []\n",
    "        for other_id, other_embedding in self.item_embeddings_dict.items():\n",
    "            if other_id == item_id:\n",
    "                continue\n",
    "            cos_sim = np.dot(target_embedding, other_embedding) / (\n",
    "                np.linalg.norm(target_embedding) * np.linalg.norm(other_embedding) + 1e-8\n",
    "            )\n",
    "            if same_category:\n",
    "                target_category = self._categorize_garment(self.item_metadata[item_id]['garment_type'])\n",
    "                other_category = self._categorize_garment(self.item_metadata[other_id]['garment_type'])\n",
    "                if target_category != other_category:\n",
    "                    continue\n",
    "            similarities.append({\n",
    "                'item_id': other_id,\n",
    "                'similarity': float(cos_sim),\n",
    "                'name': self.item_metadata[other_id]['name'],\n",
    "                'garment_type': self.item_metadata[other_id]['garment_type']\n",
    "            })\n",
    "        similarities.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        return similarities[:n]\n",
    "\n",
    "    def build_basic_outfit(self, starting_item_id, user_measurements=None, max_items=4):\n",
    "        if starting_item_id not in self.item_metadata:\n",
    "            return None\n",
    "        starting_item = self.item_metadata[starting_item_id]\n",
    "        starting_category = starting_item['category']\n",
    "        print(f\"   Building outfit starting from: {starting_item['name']}\")\n",
    "        compatible_categories = self.compatibility_rules['compatible_categories'].get(starting_category, [])\n",
    "        outfit_items = [starting_item]\n",
    "        total_price = starting_item['price']\n",
    "        for category in compatible_categories:\n",
    "            if len(outfit_items) >= max_items:\n",
    "                break\n",
    "            category_items = []\n",
    "            for item_id, metadata in self.item_metadata.items():\n",
    "                if item_id == starting_item_id:\n",
    "                    continue\n",
    "                if metadata['category'] == category:\n",
    "                    category_items.append((item_id, metadata))\n",
    "            category_items.sort(key=lambda x: abs(x[1]['price'] - starting_item['price']))\n",
    "            if category_items:\n",
    "                best_item = category_items[0][1]\n",
    "                outfit_items.append(best_item)\n",
    "                total_price += best_item['price']\n",
    "        size_recommendations = {}\n",
    "        if user_measurements and self.size_recommender and starting_item['garment_type'] != 'unknown':\n",
    "            try:\n",
    "                result = self.size_recommender.recommend_size(\n",
    "                    user_measurements,\n",
    "                    starting_item['garment_type'],\n",
    "                    top_k=1\n",
    "                )\n",
    "                if isinstance(result, dict) and result.get('recommendations'):\n",
    "                    rec = result['recommendations'][0]\n",
    "                    size_recommendations[starting_item_id] = rec.get('recommended_size', 'M')\n",
    "            except Exception as exc:\n",
    "                print(f\"   âš ï¸ Size recommender error: {exc}\")\n",
    "        return {\n",
    "            'starting_item': starting_item,\n",
    "            'outfit_items': outfit_items,\n",
    "            'total_price': total_price,\n",
    "            'item_count': len(outfit_items),\n",
    "            'size_recommendations': size_recommendations,\n",
    "            'compatibility_score': self._calculate_compatibility(outfit_items)\n",
    "        }\n",
    "\n",
    "    def _calculate_compatibility(self, outfit_items):\n",
    "        if len(outfit_items) < 2:\n",
    "            return 0\n",
    "        categories = [item['category'] for item in outfit_items]\n",
    "        score = 0\n",
    "        for i in range(len(categories)):\n",
    "            for j in range(i + 1, len(categories)):\n",
    "                cat1 = categories[i]\n",
    "                cat2 = categories[j]\n",
    "                compatible_cats = self.compatibility_rules['compatible_categories'].get(cat1, [])\n",
    "                if cat2 in compatible_cats:\n",
    "                    score += 20\n",
    "                elif cat1 == cat2:\n",
    "                    score -= 10\n",
    "        return min(100, max(0, 50 + score))\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "        print(f\"   ðŸ’¾ Model saved to {filepath} (COMPLETE OBJECT)\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(filepath):\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                builder = pickle.load(f)\n",
    "            print(f\"   ðŸ“‚ Model loaded from {filepath}\")\n",
    "            print(f\"   âœ… Type: {type(builder)}\")\n",
    "            print(f\"   âœ… Has build_basic_outfit: {hasattr(builder, 'build_basic_outfit')}\")\n",
    "            return builder\n",
    "        except Exception as exc:\n",
    "            print(f\"   âŒ Error loading model: {exc}\")\n",
    "            return None\n",
    "\n",
    "# ========== 5. BUILD AND TEST THE SYSTEM ==========\n",
    "print(\"\\n5. ðŸš€ Building and testing outfit builder...\")\n",
    "\n",
    "outfit_builder = SimpleOutfitBuilder(\n",
    "    items_df=merged_data,\n",
    "    item_embeddings_dict=item_embeddings_dict,\n",
    "    size_recommender=unified_size_recommender\n",
    " )\n",
    "\n",
    "save_path = artifact_path('simple_outfit_builder.pkl')\n",
    "outfit_builder.save_model(save_path)\n",
    "\n",
    "print(f\"\\n   âœ… Outfit builder created!\")\n",
    "print(f\"   â€¢ Items loaded: {len(outfit_builder.item_metadata)}\")\n",
    "print(f\"   â€¢ Categories: {len(set(item['category'] for item in outfit_builder.item_metadata.values()))}\")\n",
    "\n",
    "# ========== 6. DEMONSTRATION ==========\n",
    "print(\"\\n6. ðŸ§ª DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_user = {\n",
    "    'chest_circumference': 95,\n",
    "    'waist_circumference': 82,\n",
    "    'garment_length': 75,\n",
    "    'sleeve_length': 62\n",
    "}\n",
    "print(f\"\\nðŸ‘¤ TEST USER MEASUREMENTS:\")\n",
    "for key, value in test_user.items():\n",
    "    print(f\"  {key}: {value}cm\")\n",
    "\n",
    "tshirt_items = [item for item_id, item in outfit_builder.item_metadata.items()\n",
    "                if 'tee' in item['garment_type'].lower() or 't_shirt' in item['garment_type'].lower()]\n",
    "if tshirt_items:\n",
    "    print(f\"\\nðŸ§ª TEST: Build outfit starting from t-shirt\")\n",
    "    print(\"-\" * 40)\n",
    "    starting_item = tshirt_items[0]\n",
    "    print(f\"Starting from: {starting_item['name']}\")\n",
    "    similar = outfit_builder.find_similar_items(starting_item['id'], n=3, same_category=True)\n",
    "    if similar:\n",
    "        print(f\"\\nSimilar items:\")\n",
    "        for i, item in enumerate(similar, 1):\n",
    "            print(f\"  {i}. {item['name']} (sim: {item['similarity']:.2f})\")\n",
    "    outfit = outfit_builder.build_basic_outfit(\n",
    "        starting_item_id=starting_item['id'],\n",
    "        user_measurements=test_user,\n",
    "        max_items=4\n",
    "    )\n",
    "    if outfit:\n",
    "        print(f\"\\nðŸŽ¯ BUILT OUTFIT:\")\n",
    "        print(f\"Items: {outfit['item_count']}\")\n",
    "        print(f\"Total: ${outfit['total_price']:.2f}\")\n",
    "        print(f\"Compatibility: {outfit['compatibility_score']}/100\")\n",
    "        print(f\"\\nðŸ‘• ITEMS:\")\n",
    "        for i, item in enumerate(outfit['outfit_items'], 1):\n",
    "            size_rec = outfit['size_recommendations'].get(item['id'], 'N/A')\n",
    "            print(f\"{i}. {item['name']}\")\n",
    "            print(f\"   Type: {item['garment_type']} ({item['category']})\")\n",
    "            print(f\"   Price: ${item['price']:.2f}\")\n",
    "            if size_rec != 'N/A':\n",
    "                print(f\"   Recommended Size: {size_rec}\")\n",
    "\n",
    "bottom_items = [item for item_id, item in outfit_builder.item_metadata.items()\n",
    "                if item['category'] == 'bottom']\n",
    "if bottom_items:\n",
    "    print(f\"\\nðŸ§ª TEST: Build outfit starting from bottom\")\n",
    "    print(\"-\" * 40)\n",
    "    starting_item = bottom_items[0]\n",
    "    print(f\"Starting from: {starting_item['name']}\")\n",
    "    outfit = outfit_builder.build_basic_outfit(\n",
    "        starting_item_id=starting_item['id'],\n",
    "        user_measurements=test_user,\n",
    "        max_items=4\n",
    "    )\n",
    "    if outfit:\n",
    "        print(f\"\\nðŸŽ¯ BUILT OUTFIT:\")\n",
    "        print(f\"Items: {outfit['item_count']}\")\n",
    "        print(f\"Total: ${outfit['total_price']:.2f}\")\n",
    "        print(f\"Compatibility: {outfit['compatibility_score']}/100\")\n",
    "        print(f\"\\nðŸ‘• ITEMS:\")\n",
    "        for i, item in enumerate(outfit['outfit_items'], 1):\n",
    "            print(f\"{i}. {item['name']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… STEP 5 COMPLETE - SIMPLE OUTFIT BUILDER READY!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nðŸ“– QUICK USAGE:\")\n",
    "print(\"\"\"1. Build outfit from item:\n",
    "   outfit = outfit_builder.build_basic_outfit(\n",
    "       starting_item_id='1',\n",
    "       user_measurements=user_measurements,\n",
    "       max_items=4\n",
    "   )\n",
    "\n",
    "2. Find similar items:\n",
    "   similar = outfit_builder.find_similar_items(\n",
    "       item_id='1',\n",
    "       n=5,\n",
    "       same_category=True\n",
    "   )\n",
    "\n",
    "3. Save/load model:\n",
    "   outfit_builder.save_model('path.pkl')\n",
    "   loaded = SimpleOutfitBuilder.load_model('path.pkl')\n",
    "\"\"\")\n",
    "\n",
    "# ========== 7. INTEGRATION TEST ==========\n",
    "print(\"\\n7. ðŸ”— INTEGRATION TEST WITH STEPS 3 & 4\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"Testing Step 3 embeddings integration...\")\n",
    "if embedding_cols:\n",
    "    sample_item_id = list(item_embeddings_dict.keys())[0]\n",
    "    sample_embedding = item_embeddings_dict[sample_item_id]\n",
    "    print(f\"   Sample item {sample_item_id}: embedding shape {sample_embedding.shape}\")\n",
    "else:\n",
    "    print(\"   âš ï¸ Using generated embeddings\")\n",
    "\n",
    "# ========== 8. SAVE OUTFIT BUILDER OBJECT ==========\n",
    "print(\"\\n\\n8. ðŸ’¾ Saving outfit builder object...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "object_save_path = artifact_path('outfit_builder_object.pkl')\n",
    "with open(object_save_path, 'wb') as f:\n",
    "    pickle.dump(outfit_builder, f)\n",
    "print(f\"âœ… Outfit builder OBJECT saved to: {object_save_path}\")\n",
    "print(\"   (This contains the actual object with all methods and data)\")\n",
    "\n",
    "print(\"\\nTesting Step 4 size recommender integration...\")\n",
    "if unified_size_recommender:\n",
    "    print(\"   âœ… Size recommender integrated\")\n",
    "    try:\n",
    "        result = unified_size_recommender.recommend_size(\n",
    "            test_user,\n",
    "            't_shirt',\n",
    "            top_k=1\n",
    "        )\n",
    "        if isinstance(result, dict):\n",
    "            print(f\"   Recommendation method: {result.get('method', 'unknown')}\")\n",
    "            if result.get('recommendations'):\n",
    "                print(f\"   Found {len(result['recommendations'])} recommendations\")\n",
    "    except Exception as exc:\n",
    "        print(f\"   âš ï¸ Error testing size recommender: {exc}\")\n",
    "else:\n",
    "    print(\"   âš ï¸ Size recommender not available\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸŽ‰ STEP 5 COMPLETE - ALL SYSTEMS INTEGRATED!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JHo1czUoGeYU",
    "outputId": "fbebf2a2-f55b-4763-d123-a21560b22447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ STEP 6: Main Recommendation Interface (UNIFIED SYSTEM)\n",
      "============================================================\n",
      "âœ… Using cloudpickle for serialization\n",
      "âœ… Libraries imported\n",
      "\n",
      "1. ðŸ”„ Loading core artifacts...\n",
      "   âœ… Artifacts loaded successfully\n",
      "   âœ… Engine initialized with 250 catalog items\n",
      "\n",
      "ðŸŽ‰ Export complete!\n",
      "   â€¢ Engine saved to: c:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\\artifacts\\fashion_recommendation_engine.pkl\n",
      "   â€¢ API proxy saved to: c:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\\artifacts\\fashion_api.pkl\n",
      "Use check_pickle.py to validate the artifact if needed.\n"
     ]
    }
   ],
   "source": [
    "# @title ðŸš€ **STEP 6: Main Recommendation Interface (UNIFIED SYSTEM)**\n",
    "print(\"ðŸš€ STEP 6: Main Recommendation Interface (UNIFIED SYSTEM)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "import pandas as pd\n",
    "import pickle as std_pickle\n",
    "\n",
    "try:\n",
    "    import cloudpickle\n",
    "    ACTIVE_PICKLE_LIB = cloudpickle\n",
    "    print(\"âœ… Using cloudpickle for serialization\")\n",
    "except ImportError:\n",
    "    cloudpickle = None\n",
    "    ACTIVE_PICKLE_LIB = std_pickle\n",
    "    print(\"âš ï¸ cloudpickle not available; falling back to pickle\")\n",
    "\n",
    "print(\"âœ… Libraries imported\")\n",
    "print(\"\\n1. ðŸ”„ Loading core artifacts...\")\n",
    "\n",
    "def _load_pickle(path: Path, required: bool = True):\n",
    "    if not path.exists():\n",
    "        if required:\n",
    "            raise FileNotFoundError(f\"Artifact not found: {path}\")\n",
    "        print(f\"   âš ï¸ Optional artifact missing: {path}\")\n",
    "        return None\n",
    "    with path.open('rb') as handle:\n",
    "        try:\n",
    "            return std_pickle.load(handle)\n",
    "        except Exception as exc:\n",
    "            if cloudpickle is None:\n",
    "                raise\n",
    "            handle.seek(0)\n",
    "            print(f\"   âš ï¸ Retrying {path.name} with cloudpickle: {exc}\")\n",
    "            return cloudpickle.load(handle)\n",
    "\n",
    "def _dump_pickle(obj, path: Path):\n",
    "    modules = (ACTIVE_PICKLE_LIB, std_pickle) if ACTIVE_PICKLE_LIB is not std_pickle else (std_pickle,)\n",
    "    for module in modules:\n",
    "        try:\n",
    "            with path.open('wb') as handle:\n",
    "                module.dump(obj, handle)\n",
    "            return\n",
    "        except Exception as exc:\n",
    "            print(f\"   âš ï¸ Failed to serialize with {module.__name__}: {exc}\")\n",
    "    raise RuntimeError(f\"Could not serialize object to {path}\")\n",
    "\n",
    "original_items_df = _load_pickle(artifact_path('original_items.pkl'))\n",
    "robust_features_df = _load_pickle(artifact_path('robust_features.pkl'), required=False)\n",
    "item_embeddings_df = _load_pickle(artifact_path('item_embeddings.pkl'), required=False)\n",
    "size_artifacts = _load_pickle(artifact_path('size_recommendation_artifacts.pkl')) or {}\n",
    "size_recommender = size_artifacts.get('size_recommender')\n",
    "fallback_mapper = size_artifacts.get('fallback_mapper')\n",
    "size_recommender_fitted = bool(size_artifacts.get('size_recommender_fitted'))\n",
    "\n",
    "outfit_builder = _load_pickle(artifact_path('simple_outfit_builder.pkl'), required=False)\n",
    "if outfit_builder is None:\n",
    "    outfit_builder = _load_pickle(artifact_path('outfit_builder_object.pkl'))\n",
    "\n",
    "print(\"   âœ… Artifacts loaded successfully\")\n",
    "\n",
    "class UnifiedSizeEngine:\n",
    "    def __init__(self, model, fallback, fitted):\n",
    "        self.model = model if fitted and model is not None else None\n",
    "        self.fallback = fallback\n",
    "        self.fitted = fitted and model is not None\n",
    "\n",
    "    def recommend(self, garment_type: Optional[str], item_meta: Optional[Dict[str, Any]], measurements: Optional[Dict[str, Any]]):\n",
    "        measurements = measurements or {}\n",
    "        candidates: List[Dict[str, Any]] = []\n",
    "        method = 'hybrid'\n",
    "        if self.model is not None:\n",
    "            try:\n",
    "                results = self.model.recommend_size(measurements, top_k=3)\n",
    "                if isinstance(results, list) and results:\n",
    "                    for entry in results:\n",
    "                        candidates.append({\n",
    "                            'recommended_size': str(entry.get('recommended_size') or entry.get('size') or 'M'),\n",
    "                            'fit_score': float(entry.get('fit_score') or entry.get('distance', 0.0)),\n",
    "                            'source': 'model'\n",
    "                        })\n",
    "            except Exception as exc:\n",
    "                print(f\"   âš ï¸ Size model error: {exc}\")\n",
    "        if not candidates and self.fallback is not None:\n",
    "            method = 'fallback'\n",
    "            category = None\n",
    "            if item_meta and item_meta.get('category'):\n",
    "                category = item_meta['category']\n",
    "            if category is None and garment_type:\n",
    "                category = _infer_category(garment_type)\n",
    "            if category is None:\n",
    "                category = 'top'\n",
    "            try:\n",
    "                fallback_results = self.fallback.recommend(category, measurements)\n",
    "                for entry in fallback_results[:3]:\n",
    "                    candidates.append({\n",
    "                        'recommended_size': str(entry.get('size', 'M')),\n",
    "                        'fit_score': float(entry.get('match_score', 0.5)),\n",
    "                        'source': 'fallback'\n",
    "                    })\n",
    "            except Exception as exc:\n",
    "                print(f\"   âš ï¸ Fallback size mapper error: {exc}\")\n",
    "        guidance = {\n",
    "            'fit': '',\n",
    "            'formality': ''\n",
    "        }\n",
    "        if self.fallback is not None:\n",
    "            fit_key = measurements.get('fit_type', 'regular')\n",
    "            formality_key = measurements.get('garment_formality', 'casual')\n",
    "            if hasattr(self.fallback, 'get_fit_guidance'):\n",
    "                guidance['fit'] = self.fallback.get_fit_guidance(fit_key) or ''\n",
    "            if hasattr(self.fallback, 'get_formality_guidance'):\n",
    "                guidance['formality'] = self.fallback.get_formality_guidance(formality_key) or ''\n",
    "        return {\n",
    "            'method': method if candidates else 'manual',\n",
    "            'recommendations': candidates,\n",
    "            'guidance': guidance\n",
    "        }\n",
    "\n",
    "def _infer_category(garment_type: Optional[str]) -> Optional[str]:\n",
    "    if not garment_type:\n",
    "        return None\n",
    "    name = str(garment_type).lower()\n",
    "    if 'dress' in name:\n",
    "        return 'dress'\n",
    "    if any(token in name for token in ['pant', 'jean', 'trouser', 'bottom']):\n",
    "        return 'bottom'\n",
    "    if any(token in name for token in ['skirt']):\n",
    "        return 'skirt'\n",
    "    if any(token in name for token in ['short']):\n",
    "        return 'shorts'\n",
    "    if any(token in name for token in ['sweater', 'hoodie', 'cardigan']):\n",
    "        return 'sweater'\n",
    "    if any(token in name for token in ['jacket', 'coat', 'blazer', 'outer']):\n",
    "        return 'outerwear'\n",
    "    if any(token in name for token in ['shoe', 'sneaker', 'boot']):\n",
    "        return 'footwear'\n",
    "    return 'top'\n",
    "\n",
    "size_engine = UnifiedSizeEngine(size_recommender, fallback_mapper, size_recommender_fitted)\n",
    "\n",
    "def _coerce_measurements(data: Any) -> Dict[str, float]:\n",
    "    result: Dict[str, float] = {}\n",
    "    if isinstance(data, dict):\n",
    "        source = data\n",
    "    elif isinstance(data, list):\n",
    "        source = {f'value_{i}': value for i, value in enumerate(data)}\n",
    "    else:\n",
    "        source = {}\n",
    "    for key, value in source.items():\n",
    "        try:\n",
    "            if value is None or value == '':\n",
    "                continue\n",
    "            result[str(key)] = float(value)\n",
    "        except (TypeError, ValueError):\n",
    "            continue\n",
    "    return result\n",
    "\n",
    "class FashionUser:\n",
    "    def __init__(self, user_id: str, name: str = '', email: str = ''):\n",
    "        self.user_id = str(user_id)\n",
    "        self.name = name or ''\n",
    "        self.email = email or ''\n",
    "        self.created_at = datetime.utcnow()\n",
    "        self.updated_at = datetime.utcnow()\n",
    "        self.measurements: Dict[str, Any] = {}\n",
    "        self.preferences: Dict[str, Any] = {}\n",
    "        self.purchase_history: List[Dict[str, Any]] = []\n",
    "        self.wishlist: List[Dict[str, Any]] = []\n",
    "        self.view_history: List[Dict[str, Any]] = []\n",
    "\n",
    "    def update(self, payload: Dict[str, Any]):\n",
    "        self.name = payload.get('name', self.name)\n",
    "        self.email = payload.get('email', self.email)\n",
    "        if payload.get('measurements'):\n",
    "            self.measurements = _coerce_measurements(payload['measurements'])\n",
    "        if payload.get('preferences'):\n",
    "            self.preferences = dict(payload['preferences'])\n",
    "        if payload.get('purchase_history'):\n",
    "            for record in payload['purchase_history']:\n",
    "                self.add_purchase(record)\n",
    "        if payload.get('wishlist'):\n",
    "            self.wishlist = [dict(entry) for entry in payload['wishlist']]\n",
    "        if payload.get('view_history'):\n",
    "            self.view_history = [dict(entry) for entry in payload['view_history']]\n",
    "        self.updated_at = datetime.utcnow()\n",
    "\n",
    "    def add_purchase(self, purchase: Dict[str, Any]):\n",
    "        record = {\n",
    "            'item_id': str(purchase.get('item_id')) if purchase.get('item_id') is not None else None,\n",
    "            'item_name': purchase.get('item_name', ''),\n",
    "            'price': float(purchase.get('price', 0.0)) if purchase.get('price') is not None else 0.0,\n",
    "            'rating': float(purchase.get('rating', 0.0)) if purchase.get('rating') is not None else None,\n",
    "            'purchased_at': purchase.get('purchased_at') or datetime.utcnow().isoformat()\n",
    "        }\n",
    "        self.purchase_history.append(record)\n",
    "        self.updated_at = datetime.utcnow()\n",
    "\n",
    "    def summary(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'user_id': self.user_id,\n",
    "            'name': self.name,\n",
    "            'email': self.email,\n",
    "            'measurements': self.measurements,\n",
    "            'preferences': self.preferences,\n",
    "            'purchase_count': len(self.purchase_history),\n",
    "            'wishlist_count': len(self.wishlist),\n",
    "            'view_count': len(self.view_history),\n",
    "            'created_at': self.created_at.isoformat(),\n",
    "            'updated_at': self.updated_at.isoformat()\n",
    "        }\n",
    "\n",
    "class FashionRecommendationEngine:\n",
    "    def __init__(self, items_df: pd.DataFrame, outfit_builder=None, size_engine: Optional[UnifiedSizeEngine] = None, robust_df: Optional[pd.DataFrame] = None):\n",
    "        self.items_df = items_df.copy()\n",
    "        self.outfit_builder = outfit_builder\n",
    "        self.size_engine = size_engine\n",
    "        self.robust_df = robust_df.copy() if robust_df is not None else None\n",
    "        self.users: Dict[str, FashionUser] = {}\n",
    "        self.items_lookup = self._build_item_lookup(self.items_df)\n",
    "        self.default_items = self._build_default_items()\n",
    "        print(f\"   âœ… Engine initialized with {len(self.items_lookup)} catalog items\")\n",
    "\n",
    "    def _build_item_lookup(self, df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:\n",
    "        lookup: Dict[str, Dict[str, Any]] = {}\n",
    "        for _, row in df.iterrows():\n",
    "            item_id = str(row.get('item_id') or row.get('ID') or row.get('id'))\n",
    "            if not item_id:\n",
    "                continue\n",
    "            lookup[item_id] = self._format_item(row)\n",
    "        return lookup\n",
    "\n",
    "    def _build_default_items(self) -> List[Dict[str, Any]]:\n",
    "        df = self.items_df.copy()\n",
    "        if 'total_stock' in df.columns:\n",
    "            df = df.sort_values('total_stock', ascending=False)\n",
    "        elif 'price' in df.columns:\n",
    "            df = df.sort_values('price', ascending=False)\n",
    "        return [self._format_item(row) for _, row in df.head(50).iterrows()]\n",
    "\n",
    "    def _format_item(self, row: pd.Series) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'item_id': str(row.get('item_id') or row.get('ID') or row.get('id')),\n",
    "            'name': row.get('name') or row.get('Name') or f\"Item {row.get('item_id')}\",\n",
    "            'description': row.get('description') or row.get('Description') or '',\n",
    "            'price': float(row.get('price')) if pd.notna(row.get('price')) else float(row.get('Price', 0) or 0),\n",
    "            'garment_type': row.get('garment_type') or row.get('Garment Type') or '',\n",
    "            'garment_category': row.get('garment_category') or row.get('category') or _infer_category(row.get('garment_type')),\n",
    "            'garment_formality': row.get('garment_formality') or '',\n",
    "            'store': row.get('store') or row.get('Store') or '',\n",
    "            'image_url': row.get('image_url') or row.get('ImageUrl') or ''\n",
    "        }\n",
    "\n",
    "    def _get_user(self, user_id: str) -> FashionUser:\n",
    "        if user_id not in self.users:\n",
    "            self.users[user_id] = FashionUser(user_id)\n",
    "        return self.users[user_id]\n",
    "\n",
    "    def register(self, payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        user_id = str(payload.get('user_id') or payload.get('id'))\n",
    "        if not user_id:\n",
    "            raise ValueError('user_id is required')\n",
    "        user = self._get_user(user_id)\n",
    "        user.update(payload)\n",
    "        return user.summary()\n",
    "\n",
    "    def get_size(self, user_id: str, garment_type: str, item_id: Optional[str] = None) -> Dict[str, Any]:\n",
    "        user = self._get_user(user_id)\n",
    "        item_meta = self.items_lookup.get(str(item_id)) if item_id else None\n",
    "        if self.size_engine is None:\n",
    "            return {\n",
    "                'method': 'manual',\n",
    "                'recommendations': [],\n",
    "                'guidance': {'fit': 'Provide measurements for better results.', 'formality': ''}\n",
    "            }\n",
    "        return self.size_engine.recommend(garment_type, item_meta, user.measurements)\n",
    "\n",
    "    def build_outfit(self, user_id: str, starting_item_id: Optional[str] = None, style: Optional[str] = None, max_items: int = 4) -> Dict[str, Any]:\n",
    "        user = self._get_user(user_id)\n",
    "        if self.outfit_builder is None:\n",
    "            raise RuntimeError('Outfit builder is not available')\n",
    "        if starting_item_id is None:\n",
    "            starting_item_id = self.default_items[0]['item_id'] if self.default_items else next(iter(self.items_lookup))\n",
    "        starting_item_id = str(starting_item_id)\n",
    "        outfit = self.outfit_builder.build_basic_outfit(\n",
    "            starting_item_id=starting_item_id,\n",
    "            user_measurements=user.measurements,\n",
    "            max_items=max(2, min(6, int(max_items)))\n",
    "        )\n",
    "        if outfit is None:\n",
    "            raise ValueError('Unable to build outfit with the provided item id')\n",
    "        formatted_items = []\n",
    "        for item in outfit['outfit_items']:\n",
    "            formatted_items.append({\n",
    "                'item_id': item.get('id'),\n",
    "                'name': item.get('name'),\n",
    "                'garment_type': item.get('garment_type'),\n",
    "                'category': item.get('category'),\n",
    "                'price': float(item.get('price', 0))\n",
    "            })\n",
    "        starting = outfit.get('starting_item', {})\n",
    "        return {\n",
    "            'starting_item': {\n",
    "                'item_id': starting.get('id'),\n",
    "                'name': starting.get('name'),\n",
    "                'garment_type': starting.get('garment_type'),\n",
    "                'category': starting.get('category'),\n",
    "                'price': float(starting.get('price', 0))\n",
    "            },\n",
    "            'items': formatted_items,\n",
    "            'total_price': float(outfit.get('total_price', 0.0)),\n",
    "            'item_count': int(outfit.get('item_count', len(formatted_items))),\n",
    "            'compatibility_score': float(outfit.get('compatibility_score', 0.0)),\n",
    "            'size_recommendations': outfit.get('size_recommendations', {})\n",
    "        }\n",
    "\n",
    "    def recommend(self, user_id: str, n: int = 6) -> List[Dict[str, Any]]:\n",
    "        user = self._get_user(user_id)\n",
    "        recommendations: List[Dict[str, Any]] = []\n",
    "        seen = set()\n",
    "        if user.purchase_history:\n",
    "            last_purchase = user.purchase_history[-1]\n",
    "            item_id = str(last_purchase.get('item_id'))\n",
    "            if self.outfit_builder and item_id:\n",
    "                similar = self.outfit_builder.find_similar_items(item_id, n=n, same_category=False) or []\n",
    "                for entry in similar:\n",
    "                    candidate_id = str(entry.get('item_id'))\n",
    "                    if candidate_id and candidate_id in self.items_lookup and candidate_id not in seen:\n",
    "                        recommendations.append(self.items_lookup[candidate_id])\n",
    "                        seen.add(candidate_id)\n",
    "        for item in self.default_items:\n",
    "            if len(recommendations) >= n:\n",
    "                break\n",
    "            item_id = item['item_id']\n",
    "            if item_id not in seen:\n",
    "                recommendations.append(item)\n",
    "                seen.add(item_id)\n",
    "        return recommendations[:n]\n",
    "\n",
    "    def get_insights(self, user_id: str) -> Dict[str, Any]:\n",
    "        user = self._get_user(user_id)\n",
    "        categories: Dict[str, int] = {}\n",
    "        for purchase in user.purchase_history:\n",
    "            item = self.items_lookup.get(str(purchase.get('item_id')))\n",
    "            if item:\n",
    "                cat = item.get('garment_category', 'unknown')\n",
    "                categories[cat] = categories.get(cat, 0) + 1\n",
    "        return {\n",
    "            'profile': user.summary(),\n",
    "            'favorite_categories': categories,\n",
    "            'recent_purchases': user.purchase_history[-5:]\n",
    "        }\n",
    "\n",
    "    def add_purchase(self, user_id: str, item_id: str, item_name: str = '', price: float = 0.0) -> Dict[str, Any]:\n",
    "        user = self._get_user(user_id)\n",
    "        record = {\n",
    "            'item_id': str(item_id),\n",
    "            'item_name': item_name or self.items_lookup.get(str(item_id), {}).get('name', ''),\n",
    "            'price': float(price),\n",
    "            'purchased_at': datetime.utcnow().isoformat()\n",
    "        }\n",
    "        user.add_purchase(record)\n",
    "        return {'status': 'logged', 'purchase': record}\n",
    "\n",
    "fashion_engine = FashionRecommendationEngine(\n",
    "    items_df=original_items_df,\n",
    "    outfit_builder=outfit_builder,\n",
    "    size_engine=size_engine,\n",
    "    robust_df=robust_features_df\n",
    ")\n",
    "\n",
    "engine_path = artifact_path('fashion_recommendation_engine.pkl')\n",
    "api_path = artifact_path('fashion_api.pkl')\n",
    "_dump_pickle(fashion_engine, engine_path)\n",
    "_dump_pickle(fashion_engine, api_path)\n",
    "\n",
    "print(\"\\nðŸŽ‰ Export complete!\")\n",
    "print(f\"   â€¢ Engine saved to: {engine_path}\")\n",
    "print(f\"   â€¢ API proxy saved to: {api_path}\")\n",
    "print(\"Use check_pickle.py to validate the artifact if needed.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
