{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9ReZGxognZBe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Fashion_Recommendation_System.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "# üõçÔ∏è **Complete Fashion Recommendation & Outfit Building System**\n",
    "\n",
    "**System Features:**\n",
    "1. Personalized recommendations based on purchase history\n",
    "2. Size/fit recommendations based on body measurements\n",
    "3. Outfit building and compatibility scoring\n",
    "4. Cold-start recommendations for new users\n",
    "5. Style-based filtering\n",
    "---\n",
    "\"\"\" \n",
    "\n",
    "# @title ‚öôÔ∏è **Step 0: Install & Import Required Libraries**\n",
    "\n",
    "!pip install pandas numpy scikit-learn tensorflow openpyxl sqlalchemy pymysql python-dotenv -q\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "import builtins\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine.url import URL\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# TensorFlow for embeddings\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "ARTIFACTS_DIR = Path.cwd() / \"artifacts\"\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def artifact_path(filename: str) -> Path:\n",
    "    return ARTIFACTS_DIR / filename\n",
    "\n",
    "def find_project_file(filename: str, start: Path | None = None) -> Path | None:\n",
    "    current = (start or Path.cwd()).resolve()\n",
    "    for parent in [current, *current.parents]:\n",
    "        candidate = parent / filename\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "def _resolve_artifact_path(path):\n",
    "    if isinstance(path, (str, Path)):\n",
    "        path_str = str(path)\n",
    "        if path_str.startswith(\"/content/\"):\n",
    "            return artifact_path(path_str.replace(\"/content/\", \"\", 1))\n",
    "    return path\n",
    "\n",
    "if not hasattr(builtins, \"_artifact_original_open\"):\n",
    "    builtins._artifact_original_open = builtins.open\n",
    "    def _artifact_open(file, *args, **kwargs):\n",
    "        resolved = _resolve_artifact_path(file)\n",
    "        return builtins._artifact_original_open(resolved, *args, **kwargs)\n",
    "    builtins.open = _artifact_open\n",
    "\n",
    "if not hasattr(builtins, \"_artifact_original_print\"):\n",
    "    builtins._artifact_original_print = builtins.print\n",
    "    def _artifact_print(*args, **kwargs):\n",
    "        prefix = f\"{ARTIFACTS_DIR.resolve()}/\"\n",
    "        updated_args = []\n",
    "        for arg in args:\n",
    "            if isinstance(arg, str):\n",
    "                updated_args.append(arg.replace(\"/content/\", prefix))\n",
    "            else:\n",
    "                updated_args.append(arg)\n",
    "        builtins._artifact_original_print(*updated_args, **kwargs)\n",
    "    builtins.print = _artifact_print\n",
    "\n",
    "if not hasattr(pd, \"_artifact_original_read_pickle\"):\n",
    "    pd._artifact_original_read_pickle = pd.read_pickle\n",
    "    def _artifact_read_pickle(path, *args, **kwargs):\n",
    "        resolved = _resolve_artifact_path(path)\n",
    "        return pd._artifact_original_read_pickle(resolved, *args, **kwargs)\n",
    "    pd.read_pickle = _artifact_read_pickle\n",
    "\n",
    "if not hasattr(pd.DataFrame, \"_artifact_original_to_pickle\"):\n",
    "    pd.DataFrame._artifact_original_to_pickle = pd.DataFrame.to_pickle\n",
    "    def _artifact_to_pickle(self, path, *args, **kwargs):\n",
    "        resolved = _resolve_artifact_path(path)\n",
    "        return pd.DataFrame._artifact_original_to_pickle(self, resolved, *args, **kwargs)\n",
    "    pd.DataFrame.to_pickle = _artifact_to_pickle\n",
    "\n",
    "if not hasattr(np, \"_artifact_original_save\"):\n",
    "    np._artifact_original_save = np.save\n",
    "    def _artifact_np_save(file, arr, *args, **kwargs):\n",
    "        resolved = _resolve_artifact_path(file)\n",
    "        return np._artifact_original_save(resolved, arr, *args, **kwargs)\n",
    "    np.save = _artifact_np_save\n",
    "\n",
    "ORIGINAL_DATA_PICKLE = artifact_path(\"original_items.pkl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XLRQaMhinb54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database libraries imported successfully\n",
      "üîÑ Connecting to your MySQL database...\n",
      "============================================================\n",
      "üîç Attempting to connect to MySQL database...\n",
      "üìä Using database connection:\n",
      "   Host: 127.0.0.1:3306\n",
      "   Database: fitfast\n",
      "   Username: root\n",
      "üóÑÔ∏è Connecting to MySQL...\n",
      "‚úÖ Database connection successful!\n",
      "‚úÖ Retrieved 250 items from MySQL\n",
      "üìã Columns retrieved: 14\n",
      "    1. ID\n",
      "    2. Name\n",
      "    3. Description\n",
      "    4. Store\n",
      "    5. Price\n",
      "    6. Category\n",
      "    7. Garment Type\n",
      "    8. Total Stock\n",
      "    9. Color Variants\n",
      "   10. Sizing Data\n",
      "   11. Size Stock\n",
      "   12. Variants\n",
      "   13. Created At\n",
      "   14. Updated At\n",
      "üìù Processed JSON column: Color Variants\n",
      "üìù Processed JSON column: Sizing Data\n",
      "üìù Processed JSON column: Size Stock\n",
      "üìù Processed JSON column: Variants\n",
      "\n",
      "============================================================\n",
      "‚úÖ SUCCESS! Data loaded: 250 rows, 14 columns\n",
      "============================================================\n",
      "\n",
      "üìä Sample data (first 3 rows):\n",
      "   ID               Name                                        Description  \\\n",
      "0   1   Classic Crew Tee  Classic Crew Tee. Made from High-quality linen...   \n",
      "1   2         V-Neck Tee  V-Neck Tee. Made from Premium linen for easy c...   \n",
      "2   3  Graphic Print Tee  Graphic Print Tee. Made from Premium cotton fo...   \n",
      "\n",
      "             Store  Price  Category Garment Type  Total Stock  \\\n",
      "0  Fashion Store 6  19.99  T-Shirts      t_shirt          107   \n",
      "1  Fashion Store 3  21.99  T-Shirts   v_neck_tee          116   \n",
      "2  Fashion Store 3  24.99  T-Shirts      t_shirt           99   \n",
      "\n",
      "                                      Color Variants  \\\n",
      "0  {\"Black\":{\"name\":\"Black\",\"stock\":37},\"White\":{...   \n",
      "1  {\"Gray\":{\"name\":\"Gray\",\"stock\":82},\"Navy\":{\"na...   \n",
      "2  {\"Black\":{\"name\":\"Black\",\"stock\":55},\"White\":{...   \n",
      "\n",
      "                                         Sizing Data  \\\n",
      "0  {\"garment_type\":\"t_shirt\",\"measurements_cm\":{\"...   \n",
      "1  {\"garment_type\":\"v_neck_tee\",\"measurements_cm\"...   \n",
      "2  {\"garment_type\":\"t_shirt\",\"measurements_cm\":{\"...   \n",
      "\n",
      "                                       Size Stock  \\\n",
      "0  {\"XS\":7,\"S\":21,\"M\":28,\"L\":21,\"XL\":20,\"XXL\":10}   \n",
      "1  {\"XS\":13,\"S\":23,\"M\":38,\"L\":26,\"XL\":11,\"XXL\":5}   \n",
      "2  {\"XS\":11,\"S\":14,\"M\":32,\"L\":16,\"XL\":17,\"XXL\":9}   \n",
      "\n",
      "                                            Variants          Created At  \\\n",
      "0  [{\"color\":\"Black\",\"size\":\"XL\",\"stock\":14},{\"co... 2025-11-04 15:06:59   \n",
      "1  [{\"color\":\"Gray\",\"size\":\"S\",\"stock\":15},{\"colo... 2025-11-13 15:06:59   \n",
      "2  [{\"color\":\"Black\",\"size\":\"L\",\"stock\":1},{\"colo... 2025-12-07 15:06:59   \n",
      "\n",
      "           Updated At  \n",
      "0 2026-01-16 15:06:59  \n",
      "1 2026-01-16 22:09:37  \n",
      "2 2026-01-16 15:06:59  \n",
      "\n",
      "üìã Column Details (14 total):\n",
      " 1. ID                  \n",
      "     Non-null: 250/250 (100.0%)\n",
      "     Unique: 250\n",
      "     Sample: 1\n",
      " 2. Name                \n",
      "     Non-null: 250/250 (100.0%)\n",
      "     Unique: 250\n",
      "     Sample: Classic Crew Tee\n",
      " 3. Description         \n",
      "     Non-null: 250/250 (100.0%)\n",
      "     Unique: 250\n",
      "     Sample: Classic Crew Tee. Made from High-quality linen for...\n",
      " 4. Store               \n",
      "     Non-null: 250/250 (100.0%)\n",
      "     Unique: 10\n",
      "     Sample: Fashion Store 6\n",
      " 5. Price               \n",
      "     Non-null: 250/250 (100.0%)\n",
      "     Unique: 229\n",
      "     Sample: 19.99\n",
      " 6. Category            \n",
      "     Non-null: 250/250 (100.0%)\n",
      "     Unique: 19\n",
      "     Sample: T-Shirts\n",
      " 7. Garment Type        \n",
      "     Non-null: 250/250 (100.0%)\n",
      "     Unique: 58\n",
      "     Sample: t_shirt\n",
      " 8. Total Stock         \n",
      "     Non-null: 250/250 (100.0%)\n",
      "     Unique: 50\n",
      "     Sample: 107\n",
      " 9. Color Variants      \n",
      "     Non-null: 250/250 (100.0%)\n",
      "     Unique: 250\n",
      "     Sample: {\"Black\":{\"name\":\"Black\",\"stock\":37},\"White\":{\"nam...\n",
      "10. Sizing Data         \n",
      "     Non-null: 250/250 (100.0%)\n",
      "     Unique: 250\n",
      "     Sample: {\"garment_type\":\"t_shirt\",\"measurements_cm\":{\"XS\":...\n",
      "11. Size Stock          \n",
      "     Non-null: 250/250 (100.0%)\n",
      "     Unique: 250\n",
      "     Sample: {\"XS\":7,\"S\":21,\"M\":28,\"L\":21,\"XL\":20,\"XXL\":10}\n",
      "12. Variants            \n",
      "     Non-null: 250/250 (100.0%)\n",
      "     Unique: 250\n",
      "     Sample: [{\"color\":\"Black\",\"size\":\"XL\",\"stock\":14},{\"color\"...\n",
      "13. Created At          \n",
      "     Non-null: 250/250 (100.0%)\n",
      "     Unique: 184\n",
      "     Sample: 2025-11-04 15:06:59\n",
      "14. Updated At          \n",
      "     Non-null: 250/250 (100.0%)\n",
      "     Unique: 4\n",
      "     Sample: 2026-01-16 15:06:59\n",
      "\n",
      "üíæ Original data saved to 'original_items.pkl' (absolute path: c:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\\original_items.pkl)\n",
      "\n",
      "üìà Data Summary:\n",
      "   - Total items: 250\n",
      "   - Unique Stores: 10\n",
      "   - Unique Categories: 19\n",
      "   - Unique Garment Types: 58\n",
      "   - Price Range: $6.30 - $165.47\n",
      "   - Average Price: $56.58\n",
      "   - Total Stock: 26,609\n",
      "   - Average Stock per Item: 106.4\n",
      "\n",
      "üîç Data Quality Check:\n",
      "   - Name           :   0 missing (  0.0%)\n",
      "   - Price          :   0 missing (  0.0%)\n",
      "   - Store          :   0 missing (  0.0%)\n",
      "   - Category       :   0 missing (  0.0%)\n",
      "\n",
      "============================================================\n",
      "‚úÖ Step 1 completed successfully! Ready for data analysis.\n",
      "============================================================\n",
      "\n",
      "üî¨ Additional Insights:\n",
      "\n",
      "üè™ Store Distribution (Top 5):\n",
      "   Fashion Store 5: 34 items (13.6%)\n",
      "   Fashion Store 9: 30 items (12.0%)\n",
      "   Fashion Store 8: 28 items (11.2%)\n",
      "   Fashion Store 6: 28 items (11.2%)\n",
      "   Fashion Store 10: 25 items (10.0%)\n",
      "\n",
      "üìÅ Category Distribution (Top 5):\n",
      "   Underwear: 18 items (7.2%)\n",
      "   Dresses: 17 items (6.8%)\n",
      "   Jewelry: 16 items (6.4%)\n",
      "   Coats: 14 items (5.6%)\n",
      "   Activewear: 14 items (5.6%)\n",
      "\n",
      "üëï Garment Type Distribution (Top 5):\n",
      "   trench_coat: 14 items (5.6%)\n",
      "   briefs: 13 items (5.2%)\n",
      "   t_shirt: 12 items (4.8%)\n",
      "   regular_pants: 12 items (4.8%)\n",
      "   pullover_hoodie: 10 items (4.0%)\n",
      "\n",
      "üì¶ Stock Analysis:\n",
      "   Items with low stock (<10): 0 (0.0%)\n",
      "   Items out of stock: 0 (0.0%)\n",
      "   Items with high stock (>100): 172 (68.8%)\n",
      "\n",
      "üí∞ Price Distribution:\n",
      "   (6.141, 38.134]: 79 items (31.6%)\n",
      "   (38.134, 69.968]: 98 items (39.2%)\n",
      "   (69.968, 101.802]: 46 items (18.4%)\n",
      "   (101.802, 133.636]: 20 items (8.0%)\n",
      "   (133.636, 165.47]: 7 items (2.8%)\n"
     ]
    }
   ],
   "source": [
    "# @title üìÅ **Step 1: Upload Your Data File or Connect to Database**\n",
    "\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Try to import required database libraries\n",
    "try:\n",
    "    import sqlite3\n",
    "    from sqlalchemy import create_engine\n",
    "    print(\"‚úÖ Database libraries imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Some database libraries not available: {e}\")\n",
    "    print(\"   CSV/Excel upload will still work\")\n",
    "\n",
    "# Configuration - Use local path instead of /content/\n",
    "ORIGINAL_DATA_PICKLE = Path('./original_items.pkl')\n",
    "\n",
    "def _coerce_json(value):\n",
    "    \"\"\"Convert values to JSON strings if needed\"\"\"\n",
    "    if value is None or (isinstance(value, float) and pd.isna(value)):\n",
    "        return \"\"\n",
    "    if isinstance(value, (dict, list)):\n",
    "        return json.dumps(value)\n",
    "    if isinstance(value, str):\n",
    "        return value\n",
    "    return json.dumps(value)\n",
    "\n",
    "def _build_items_query():\n",
    "    \"\"\"Build SQL query for fetching items\"\"\"\n",
    "    return \"\"\"\\\n",
    "SELECT\n",
    "    items.id AS `ID`,\n",
    "    items.name AS `Name`,\n",
    "    items.description AS `Description`,\n",
    "    stores.name AS `Store`,\n",
    "    items.price AS `Price`,\n",
    "    categories.name AS `Category`,\n",
    "    items.garment_type AS `Garment Type`,\n",
    "    items.stock_quantity AS `Total Stock`,\n",
    "    items.color_variants AS `Color Variants`,\n",
    "    items.sizing_data AS `Sizing Data`,\n",
    "    items.size_stock AS `Size Stock`,\n",
    "    items.variants AS `Variants`,\n",
    "    items.created_at AS `Created At`,\n",
    "    items.updated_at AS `Updated At`\n",
    "FROM items\n",
    "LEFT JOIN categories ON items.category_id = categories.id\n",
    "LEFT JOIN stores ON items.store_id = stores.id\n",
    "ORDER BY items.id\n",
    "\"\"\"\n",
    "\n",
    "def _load_from_mysql():\n",
    "    \"\"\"Load data from MySQL database using your Laravel .env settings\"\"\"\n",
    "    print(\"üîç Attempting to connect to MySQL database...\")\n",
    "\n",
    "    # USING YOUR EXACT .ENV VALUES\n",
    "    db_host = \"127.0.0.1\"  # Your DB_HOST\n",
    "    db_port = \"3306\"       # Your DB_PORT\n",
    "    db_name = \"fitfast\"    # Your DB_DATABASE\n",
    "    db_user = \"root\"       # Your DB_USERNAME\n",
    "    db_pass = \"\"           # Your DB_PASSWORD (empty as shown)\n",
    "\n",
    "    print(f\"üìä Using database connection:\")\n",
    "    print(f\"   Host: {db_host}:{db_port}\")\n",
    "    print(f\"   Database: {db_name}\")\n",
    "    print(f\"   Username: {db_user}\")\n",
    "\n",
    "    try:\n",
    "        print(f\"üóÑÔ∏è Connecting to MySQL...\")\n",
    "\n",
    "        # Create connection URL - using pymysql driver\n",
    "        connection_url = f\"mysql+pymysql://{db_user}:{db_pass}@{db_host}:{db_port}/{db_name}\"\n",
    "\n",
    "        # Create engine and connect with timeout\n",
    "        engine = create_engine(\n",
    "            connection_url,\n",
    "            pool_pre_ping=True,\n",
    "            connect_args={'connect_timeout': 10}\n",
    "        )\n",
    "\n",
    "        # Test connection first\n",
    "        with engine.connect() as conn:\n",
    "            print(\"‚úÖ Database connection successful!\")\n",
    "\n",
    "        # Execute query\n",
    "        df_loaded = pd.read_sql_query(_build_items_query(), engine)\n",
    "\n",
    "        print(f\"‚úÖ Retrieved {len(df_loaded)} items from MySQL\")\n",
    "\n",
    "        # Show columns retrieved\n",
    "        print(f\"üìã Columns retrieved: {len(df_loaded.columns)}\")\n",
    "        for i, col in enumerate(df_loaded.columns, 1):\n",
    "            print(f\"   {i:2d}. {col}\")\n",
    "\n",
    "        return df_loaded\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå MySQL connection failed: {e}\")\n",
    "\n",
    "        # Try alternative approach if pymysql fails\n",
    "        try:\n",
    "            print(\"üîÑ Trying alternative connection method...\")\n",
    "            import pymysql\n",
    "\n",
    "            conn = pymysql.connect(\n",
    "                host=db_host,\n",
    "                port=int(db_port),\n",
    "                user=db_user,\n",
    "                password=db_pass,\n",
    "                database=db_name,\n",
    "                charset='utf8mb4'\n",
    "            )\n",
    "\n",
    "            df_loaded = pd.read_sql_query(_build_items_query(), conn)\n",
    "            conn.close()\n",
    "\n",
    "            print(f\"‚úÖ Retrieved {len(df_loaded)} items from MySQL (alternative method)\")\n",
    "            return df_loaded\n",
    "\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Alternative method also failed: {e2}\")\n",
    "            return None\n",
    "\n",
    "# Main data loading logic\n",
    "print(\"üîÑ Connecting to your MySQL database...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try MySQL (based on your .env)\n",
    "df = _load_from_mysql()\n",
    "\n",
    "# If MySQL fails, provide helpful error\n",
    "if df is None or df.empty:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ùå DATABASE CONNECTION FAILED\")\n",
    "    print(\"=\"*60)\n",
    "    raise ConnectionError(\"Failed to connect to MySQL database.\")\n",
    "\n",
    "# Process JSON columns if they exist\n",
    "json_columns = ['Color Variants', 'Sizing Data', 'Size Stock', 'Variants']\n",
    "for col in json_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(_coerce_json)\n",
    "        print(f\"üìù Processed JSON column: {col}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ SUCCESS! Data loaded: {len(df)} rows, {len(df.columns)} columns\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä Sample data (first 3 rows):\")\n",
    "print(df.head(3))\n",
    "\n",
    "# Display column information\n",
    "print(f\"\\nüìã Column Details ({len(df.columns)} total):\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    non_null = df[col].notna().sum()\n",
    "    null_count = df[col].isna().sum()\n",
    "    unique_count = df[col].nunique()\n",
    "\n",
    "    # Get sample value\n",
    "    sample_val = \"N/A\"\n",
    "    if non_null > 0:\n",
    "        first_non_null = df[col].dropna().iloc[0]\n",
    "        sample_val = str(first_non_null)[:50]\n",
    "        if len(str(first_non_null)) > 50:\n",
    "            sample_val += \"...\"\n",
    "\n",
    "    print(f\"{i:2d}. {col:20}\")\n",
    "    print(f\"     Non-null: {non_null}/{len(df)} ({non_null/len(df)*100:.1f}%)\")\n",
    "    print(f\"     Unique: {unique_count}\")\n",
    "    print(f\"     Sample: {sample_val}\")\n",
    "\n",
    "# Save the original data - ensure directory exists\n",
    "ORIGINAL_DATA_PICKLE.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_pickle(ORIGINAL_DATA_PICKLE)\n",
    "print(f\"\\nüíæ Original data saved to '{ORIGINAL_DATA_PICKLE}' (absolute path: {ORIGINAL_DATA_PICKLE.absolute()})\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nüìà Data Summary:\")\n",
    "print(f\"   - Total items: {len(df)}\")\n",
    "print(f\"   - Unique Stores: {df['Store'].nunique() if 'Store' in df.columns else 'N/A'}\")\n",
    "print(f\"   - Unique Categories: {df['Category'].nunique() if 'Category' in df.columns else 'N/A'}\")\n",
    "print(f\"   - Unique Garment Types: {df['Garment Type'].nunique() if 'Garment Type' in df.columns else 'N/A'}\")\n",
    "\n",
    "if 'Price' in df.columns:\n",
    "    print(f\"   - Price Range: ${df['Price'].min():.2f} - ${df['Price'].max():.2f}\")\n",
    "    print(f\"   - Average Price: ${df['Price'].mean():.2f}\")\n",
    "\n",
    "if 'Total Stock' in df.columns:\n",
    "    print(f\"   - Total Stock: {df['Total Stock'].sum():,}\")\n",
    "    print(f\"   - Average Stock per Item: {df['Total Stock'].mean():.1f}\")\n",
    "\n",
    "print(\"\\nüîç Data Quality Check:\")\n",
    "for col in ['Name', 'Price', 'Store', 'Category']:\n",
    "    if col in df.columns:\n",
    "        missing = df[col].isna().sum()\n",
    "        total = len(df)\n",
    "        pct = missing/total*100 if total > 0 else 0\n",
    "        print(f\"   - {col:15}: {missing:3d} missing ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Step 1 completed successfully! Ready for data analysis.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display additional insights about your data\n",
    "print(\"\\nüî¨ Additional Insights:\")\n",
    "\n",
    "# Store distribution\n",
    "if 'Store' in df.columns:\n",
    "    store_counts = df['Store'].value_counts()\n",
    "    print(f\"\\nüè™ Store Distribution (Top 5):\")\n",
    "    for store, count in store_counts.head().items():\n",
    "        pct = count/len(df)*100\n",
    "        print(f\"   {store}: {count} items ({pct:.1f}%)\")\n",
    "\n",
    "# Category distribution\n",
    "if 'Category' in df.columns:\n",
    "    category_counts = df['Category'].value_counts()\n",
    "    print(f\"\\nüìÅ Category Distribution (Top 5):\")\n",
    "    for category, count in category_counts.head().items():\n",
    "        pct = count/len(df)*100\n",
    "        print(f\"   {category}: {count} items ({pct:.1f}%)\")\n",
    "\n",
    "# Garment type distribution\n",
    "if 'Garment Type' in df.columns:\n",
    "    garment_counts = df['Garment Type'].value_counts()\n",
    "    print(f\"\\nüëï Garment Type Distribution (Top 5):\")\n",
    "    for garment, count in garment_counts.head().items():\n",
    "        pct = count/len(df)*100\n",
    "        print(f\"   {garment}: {count} items ({pct:.1f}%)\")\n",
    "\n",
    "# Stock analysis\n",
    "if 'Total Stock' in df.columns:\n",
    "    low_stock = df[df['Total Stock'] < 10].shape[0]\n",
    "    out_of_stock = df[df['Total Stock'] == 0].shape[0]\n",
    "    high_stock = df[df['Total Stock'] > 100].shape[0]\n",
    "\n",
    "    print(f\"\\nüì¶ Stock Analysis:\")\n",
    "    print(f\"   Items with low stock (<10): {low_stock} ({low_stock/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Items out of stock: {out_of_stock} ({out_of_stock/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Items with high stock (>100): {high_stock} ({high_stock/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Price segments\n",
    "if 'Price' in df.columns:\n",
    "    price_bins = pd.cut(df['Price'], bins=5)\n",
    "    price_dist = price_bins.value_counts().sort_index()\n",
    "    print(f\"\\nüí∞ Price Distribution:\")\n",
    "    for price_range, count in price_dist.items():\n",
    "        pct = count/len(df)*100\n",
    "        print(f\"   {price_range}: {count} items ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LuIzOaA8nfOg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ COMPLETE STEP 2: Feature Engineering with SIZE-BASED Measurements\n",
      "============================================================\n",
      "\n",
      "1. üîÑ Loading original data...\n",
      "   Looking for: c:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\\original_items.pkl\n",
      "   ‚úÖ Loaded 250 items\n",
      "   Columns: ['ID', 'Name', 'Description', 'Store', 'Price', 'Category', 'Garment Type', 'Total Stock', 'Color Variants', 'Sizing Data', 'Size Stock', 'Variants', 'Created At', 'Updated At']\n",
      "\n",
      "2. üìã Correct parsing of all data...\n",
      "   Parsing sizing data with nested measurements...\n",
      "   ‚úÖ Parsed 250 items\n",
      "   ‚úÖ Measurements by size extracted for all items\n",
      "\n",
      "3. üìè Extracting measurement features for size-based recommendations...\n",
      "   Extracting measurement statistics...\n",
      "   ‚úÖ Extracted 146 measurement features\n",
      "\n",
      "4. üìê Creating size-related features...\n",
      "   Creating size distribution features...\n",
      "   ‚úÖ Created 8 size-related features\n",
      "\n",
      "5. üè∑Ô∏è Correct categorization with precise rules...\n",
      "\n",
      "6. üîß Applying special fixes for specific items...\n",
      "   Fixing 'Performance Training' items to 'athletic'...\n",
      "   ‚úÖ Fixed 1 'Performance Training' items\n",
      "   Fixing 'Training' items to 'athletic'...\n",
      "   ‚úÖ Fixed 7 'Training' items\n",
      "   Fixing 'Athletic' items to 'athletic'...\n",
      "   ‚úÖ Fixed 3 'Athletic' items\n",
      "   ‚úÖ Categorized all items with special fixes\n",
      "\n",
      "7. üé® Creating color features...\n",
      "   ‚úÖ Created 4 color theme features\n",
      "\n",
      "8. üî† Encoding categorical features...\n",
      "   ‚úÖ Encoded 8 categorical features\n",
      "\n",
      "9. üìè Scaling numerical features...\n",
      "   ‚úÖ Scaled 129 numerical features\n",
      "   Scaled columns: ['brim_width_max', 'length_min', 'height_max', 'shoulder_width_min', 'brim_width_std', 'sleeve_length_avg_step', 'width_min', 'head_circumference_max', 'inseam_length_max', 'leg_opening_max']...\n",
      "\n",
      "10. üéØ Creating final feature matrix for size-based recommendations...\n",
      "   ‚úÖ Feature matrix shape: (250, 186)\n",
      "   ‚úÖ Total features: 186\n",
      "   Feature breakdown:\n",
      "     - Encoded: 8\n",
      "     - Scaled: 129\n",
      "     - Binary: 29\n",
      "     - Measurements: 80\n",
      "\n",
      "11. üíæ Saving processed data for size-based recommendations...\n",
      "   ‚úÖ All data saved for size-based recommendation system\n",
      "\n",
      "============================================================\n",
      "üìä SIZE-BASED FEATURE ENGINEERING SUMMARY\n",
      "============================================================\n",
      "\n",
      "üìà Data Overview:\n",
      "   Total items: 250\n",
      "   Total features: 186\n",
      "   Feature matrix shape: (250, 186)\n",
      "\n",
      "üìè Measurement Coverage:\n",
      "   Items with measurements: 250 (100.0%)\n",
      "   Average sizes per item: 6.0\n",
      "\n",
      "üè∑Ô∏è Garment Categories:\n",
      "   bottom: 64 items (25.6%)\n",
      "   top: 49 items (19.6%)\n",
      "   other: 43 items (17.2%)\n",
      "   outerwear: 28 items (11.2%)\n",
      "   underwear: 18 items (7.2%)\n",
      "   dress: 17 items (6.8%)\n",
      "   socks: 11 items (4.4%)\n",
      "   swimwear: 10 items (4.0%)\n",
      "   footwear: 10 items (4.0%)\n",
      "\n",
      "üìê Key Measurement Statistics:\n",
      "   Found 40 different measurement types\n",
      "   shoulder_width: 77 items (30.8%)\n",
      "   garment_length: 110 items (44.0%)\n",
      "   chest_circumference: 100 items (40.0%)\n",
      "   sleeve_length: 77 items (30.8%)\n",
      "   waist_circumference: 108 items (43.2%)\n",
      "\n",
      "üé≠ Formality Levels:\n",
      "   casual: 158 items (63.2%)\n",
      "   business_casual: 44 items (17.6%)\n",
      "   athletic: 31 items (12.4%)\n",
      "   formal: 17 items (6.8%)\n",
      "\n",
      "üìã SAMPLE ITEMS WITH MEASUREMENTS:\n",
      "--------------------------------------------------------------------------------\n",
      "Classic Crew Tee                         | t_shirt         |  6 sizes | Chest: 90.0-112.0cm\n",
      "V-Neck Tee                               | v_neck_tee      |  6 sizes | Chest: 86.0-115.0cm\n",
      "Graphic Print Tee                        | t_shirt         |  6 sizes | Chest: 91.0-112.0cm\n",
      "Performance Training Tee                 | t_shirt         |  6 sizes | Chest: 87.0-116.0cm\n",
      "Organic Cotton Tee                       | t_shirt         |  6 sizes | Chest: 91.0-115.0cm\n",
      "\n",
      "‚ú® Special Features for Size-Based Recommendations:\n",
      "   - Raw measurements preserved for exact size matching\n",
      "   - Statistical features (min, max, mean, range) for each measurement\n",
      "   - Size availability features (variety, stock distribution)\n",
      "   - Garment-type specific measurement validation\n",
      "   - Ready for user measurement comparison\n",
      "\n",
      "============================================================\n",
      "üéâ STEP 2 COMPLETED SUCCESSFULLY!\n",
      "‚úÖ Size-based measurement extraction implemented\n",
      "‚úÖ Statistical features created for all measurements\n",
      "‚úÖ Raw measurements preserved for exact matching\n",
      "‚úÖ All data saved for recommendation system\n",
      "‚úÖ Ready for Step 3: Create embeddings with size features\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# @title üéØ **STEP 2: Feature Engineering with SIZE-BASED Measurements**\n",
    "print(\"üéØ COMPLETE STEP 2: Feature Engineering with SIZE-BASED Measurements\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from collections import Counter\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "# Define paths\n",
    "ARTIFACTS_DIR = Path('./artifacts')\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Define standard size order for consistent processing\n",
    "STANDARD_SIZES = ['XXS', 'XS', 'S', 'M', 'L', 'XL', 'XXL', 'XXXL']\n",
    "\n",
    "# Define garment type measurement mappings (from your PHP code)\n",
    "GARMENT_TYPE_MEASUREMENTS = {\n",
    "    't_shirt': ['chest_circumference', 'garment_length', 'sleeve_length', 'shoulder_width'],\n",
    "    'fitted_shirt': ['chest_circumference', 'waist_circumference', 'garment_length', 'sleeve_length', 'shoulder_width'],\n",
    "    'dress_shirt': ['chest_circumference', 'waist_circumference', 'garment_length', 'sleeve_length', 'shoulder_width', 'collar_size'],\n",
    "    'slim_pants': ['waist_circumference', 'hips_circumference', 'inseam_length', 'thigh_circumference', 'leg_opening'],\n",
    "    'regular_pants': ['waist_circumference', 'hips_circumference', 'inseam_length', 'thigh_circumference', 'leg_opening'],\n",
    "    'regular_jeans': ['waist_circumference', 'hips_circumference', 'inseam_length', 'thigh_circumference', 'leg_opening', 'rise'],\n",
    "    'slim_jeans': ['waist_circumference', 'hips_circumference', 'inseam_length', 'thigh_circumference', 'leg_opening', 'rise'],\n",
    "    'casual_shorts': ['waist_circumference', 'hips_circumference', 'short_length', 'thigh_circumference', 'leg_opening'],\n",
    "    'a_line_dress': ['chest_circumference', 'waist_circumference', 'hips_circumference', 'dress_length', 'shoulder_to_hem'],\n",
    "    'bodycon_dress': ['chest_circumference', 'waist_circumference', 'hips_circumference', 'dress_length'],\n",
    "    'maxi_dress': ['chest_circumference', 'waist_circumference', 'hips_circumference', 'dress_length', 'shoulder_to_hem'],\n",
    "    'sun_dress': ['chest_circumference', 'waist_circumference', 'hips_circumference', 'dress_length', 'shoulder_to_hem'],\n",
    "    'pencil_skirt': ['waist_circumference', 'hips_circumference', 'skirt_length'],\n",
    "    'a_line_skirt': ['waist_circumference', 'hips_circumference', 'skirt_length'],\n",
    "    'bomber_jacket': ['chest_circumference', 'garment_length', 'sleeve_length', 'shoulder_width', 'bicep_circumference'],\n",
    "    'denim_jacket': ['chest_circumference', 'garment_length', 'sleeve_length', 'shoulder_width'],\n",
    "    'trench_coat': ['chest_circumference', 'garment_length', 'sleeve_length', 'shoulder_width'],\n",
    "    'wool_coat': ['chest_circumference', 'garment_length', 'sleeve_length', 'shoulder_width'],\n",
    "    'crewneck_sweater': ['chest_circumference', 'garment_length', 'sleeve_length', 'shoulder_width'],\n",
    "    'v_neck_sweater': ['chest_circumference', 'garment_length', 'sleeve_length', 'shoulder_width'],\n",
    "    'pullover_hoodie': ['chest_circumference', 'garment_length', 'sleeve_length', 'shoulder_width', 'hood_height'],\n",
    "    'zip_hoodie': ['chest_circumference', 'garment_length', 'sleeve_length', 'shoulder_width', 'hood_height'],\n",
    "    'yoga_pants': ['waist_circumference', 'hips_circumference', 'inseam_length', 'thigh_circumference'],\n",
    "    'training_shorts': ['waist_circumference', 'hips_circumference', 'short_length', 'thigh_circumference'],\n",
    "    'bikini_top': ['chest_circumference', 'underbust_circumference', 'cup_size'],\n",
    "    'swim_trunks': ['waist_circumference', 'hips_circumference', 'short_length', 'thigh_circumference'],\n",
    "    'briefs': ['waist_circumference', 'hips_circumference'],\n",
    "    'boxers': ['waist_circumference', 'hips_circumference', 'short_length'],\n",
    "    'ankle_socks': ['foot_length'],\n",
    "    'crew_socks': ['foot_length'],\n",
    "    'sneakers': ['foot_length', 'foot_width'],\n",
    "    'dress_shoes': ['foot_length', 'foot_width'],\n",
    "    'backpack': [],\n",
    "    'tote_bag': [],\n",
    "    'necklace': ['chain_length'],\n",
    "    'bracelet': ['bracelet_circumference'],\n",
    "    'baseball_cap': ['head_circumference'],\n",
    "    'beanie': ['head_circumference'],\n",
    "}\n",
    "\n",
    "# ========== 1. RELOAD ORIGINAL DATA ==========\n",
    "print(\"\\n1. üîÑ Loading original data...\")\n",
    "\n",
    "# Use the CORRECT path from Step 1\n",
    "ORIGINAL_DATA_PATH = Path('./original_items.pkl')  # Local path from Step 1\n",
    "print(f\"   Looking for: {ORIGINAL_DATA_PATH.absolute()}\")\n",
    "\n",
    "if not ORIGINAL_DATA_PATH.exists():\n",
    "    print(f\"   ‚ùå File not found! Trying fallback paths...\")\n",
    "    # Try common locations\n",
    "    fallback_paths = [\n",
    "        Path('original_items.pkl'),\n",
    "        Path('./original_items.pkl'),\n",
    "        Path('c:/Users/pc/Desktop/FYP/FitFast-FYP/fitfast/frontend/src/ai/original_items.pkl')\n",
    "    ]\n",
    "\n",
    "    for path in fallback_paths:\n",
    "        if path.exists():\n",
    "            ORIGINAL_DATA_PATH = path\n",
    "            print(f\"   ‚úÖ Found at: {path}\")\n",
    "            break\n",
    "\n",
    "    if not ORIGINAL_DATA_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Could not find original_items.pkl. Check Step 1 saved it correctly.\")\n",
    "\n",
    "original_df = pd.read_pickle(ORIGINAL_DATA_PATH)\n",
    "print(f\"   ‚úÖ Loaded {len(original_df)} items\")\n",
    "print(f\"   Columns: {list(original_df.columns)}\")\n",
    "\n",
    "# ========== 2. CORRECT PARSING OF ALL DATA ==========\n",
    "print(\"\\n2. üìã Correct parsing of all data...\")\n",
    "\n",
    "def parse_sizing_data_final(sizing_str):\n",
    "    \"\"\"Properly parse sizing data with nested structure\"\"\"\n",
    "    if pd.isna(sizing_str) or not isinstance(sizing_str, str):\n",
    "        return {\n",
    "            'garment_type': 'unknown',\n",
    "            'measurements_cm': {},\n",
    "            'fit_characteristics': {},\n",
    "            'size_system': 'US'\n",
    "        }\n",
    "\n",
    "    result = {\n",
    "        'garment_type': 'unknown',\n",
    "        'measurements_cm': {},\n",
    "        'fit_characteristics': {},\n",
    "        'size_system': 'US'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Step 1 already processed JSON to strings\n",
    "        if sizing_str.strip().startswith('{'):\n",
    "            try:\n",
    "                data = json.loads(sizing_str)\n",
    "                result['garment_type'] = data.get('garment_type', 'unknown')\n",
    "                result['measurements_cm'] = data.get('measurements_cm', {})\n",
    "                result['fit_characteristics'] = data.get('fit_characteristics', {})\n",
    "                result['size_system'] = data.get('size_system', 'US')\n",
    "            except json.JSONDecodeError:\n",
    "                # Try to extract garment_type from string\n",
    "                if 'garment_type' in sizing_str:\n",
    "                    match = re.search(r'\"garment_type\"\\s*:\\s*\"([^\"]+)\"', sizing_str)\n",
    "                    if match:\n",
    "                        result['garment_type'] = match.group(1)\n",
    "    except Exception as e:\n",
    "        print(f\"   Warning parsing sizing data: {e}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def parse_colors_final(color_str):\n",
    "    \"\"\"Properly parse color variants\"\"\"\n",
    "    colors = {}\n",
    "    if pd.isna(color_str) or not isinstance(color_str, str):\n",
    "        return colors\n",
    "\n",
    "    try:\n",
    "        # Step 1 color variants are JSON strings\n",
    "        if color_str.strip().startswith('{'):\n",
    "            try:\n",
    "                color_dict = json.loads(color_str)\n",
    "                for color_name, color_data in color_dict.items():\n",
    "                    if isinstance(color_data, dict):\n",
    "                        colors[color_name] = color_data.get('stock', 1)\n",
    "                    else:\n",
    "                        colors[color_name] = 1\n",
    "            except json.JSONDecodeError:\n",
    "                # Fallback: try to extract color names\n",
    "                color_names = re.findall(r'\"([A-Za-z\\s]+)\"\\s*:', color_str)\n",
    "                for name in color_names:\n",
    "                    colors[name] = 1\n",
    "    except Exception as e:\n",
    "        print(f\"   Warning parsing colors: {e}\")\n",
    "\n",
    "    return colors\n",
    "\n",
    "def parse_size_stock(size_stock_str):\n",
    "    \"\"\"Parse size stock information\"\"\"\n",
    "    size_stock = {}\n",
    "    if pd.isna(size_stock_str) or not isinstance(size_stock_str, str):\n",
    "        return size_stock\n",
    "\n",
    "    try:\n",
    "        if size_stock_str.strip().startswith('{'):\n",
    "            size_stock = json.loads(size_stock_str)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return size_stock\n",
    "\n",
    "# Parse all data with proper measurement extraction\n",
    "print(\"   Parsing sizing data with nested measurements...\")\n",
    "all_items = []\n",
    "for idx, row in original_df.iterrows():\n",
    "    item = {\n",
    "        'item_id': row.get('ID', idx + 1),\n",
    "        'name': row.get('Name', f'Item {idx+1}'),\n",
    "        'description': row.get('Description', ''),\n",
    "        'price': float(row.get('Price', 0)),\n",
    "        'category': row.get('Category', 'unknown'),\n",
    "        'store': row.get('Store', 'unknown'),\n",
    "        'total_stock': int(row.get('Total Stock', 0)) if pd.notna(row.get('Total Stock')) else 0,\n",
    "        'garment_type_db': row.get('Garment Type', 'unknown')\n",
    "    }\n",
    "\n",
    "    # Parse colors\n",
    "    colors = parse_colors_final(row.get('Color Variants', ''))\n",
    "    item['colors'] = list(colors.keys())\n",
    "    item['color_stocks'] = colors\n",
    "\n",
    "    # Parse sizing data - CORRECTLY for nested structure\n",
    "    sizing = parse_sizing_data_final(row.get('Sizing Data', ''))\n",
    "\n",
    "    # Prioritize: 1. parsed garment_type, 2. database garment_type, 3. unknown\n",
    "    parsed_garment_type = sizing.get('garment_type', 'unknown')\n",
    "    if parsed_garment_type != 'unknown' and parsed_garment_type != '':\n",
    "        item['garment_type'] = parsed_garment_type\n",
    "    else:\n",
    "        item['garment_type'] = item['garment_type_db']\n",
    "\n",
    "    # Store all measurement data (nested by size)\n",
    "    item['measurements_by_size'] = sizing.get('measurements_cm', {})\n",
    "\n",
    "    # Store fit characteristics\n",
    "    fit_chars = sizing.get('fit_characteristics', {})\n",
    "    item['fit_type'] = fit_chars.get('fit_type', 'regular')\n",
    "    item['ease'] = fit_chars.get('ease', 'standard')\n",
    "    item['stretch'] = fit_chars.get('stretch', 'medium')\n",
    "    item['size_system'] = sizing.get('size_system', 'US')\n",
    "\n",
    "    # Parse size stock\n",
    "    size_stock = parse_size_stock(row.get('Size Stock', ''))\n",
    "    item['size_stock'] = size_stock\n",
    "\n",
    "    all_items.append(item)\n",
    "\n",
    "features_df = pd.DataFrame(all_items)\n",
    "print(f\"   ‚úÖ Parsed {len(features_df)} items\")\n",
    "print(f\"   ‚úÖ Measurements by size extracted for all items\")\n",
    "\n",
    "# ========== 3. EXTRACT MEASUREMENT FEATURES ==========\n",
    "print(\"\\n3. üìè Extracting measurement features for size-based recommendations...\")\n",
    "\n",
    "def extract_measurement_stats(measurements_by_size, garment_type):\n",
    "    \"\"\"Extract statistical features from measurements across all sizes\"\"\"\n",
    "    stats = {}\n",
    "\n",
    "    # Initialize all possible measurement fields\n",
    "    all_measurements = set()\n",
    "    for size_data in measurements_by_size.values():\n",
    "        if isinstance(size_data, dict):\n",
    "            all_measurements.update(size_data.keys())\n",
    "\n",
    "    # Convert to list for consistent ordering\n",
    "    all_measurements = list(all_measurements)\n",
    "\n",
    "    # For each measurement type, calculate stats across sizes\n",
    "    for measurement in all_measurements:\n",
    "        values = []\n",
    "        for size, size_data in measurements_by_size.items():\n",
    "            if isinstance(size_data, dict) and measurement in size_data:\n",
    "                try:\n",
    "                    value = float(size_data[measurement])\n",
    "                    values.append(value)\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "\n",
    "        if values:  # Only add if we have values\n",
    "            stats[f'{measurement}_min'] = min(values)\n",
    "            stats[f'{measurement}_max'] = max(values)\n",
    "            stats[f'{measurement}_mean'] = np.mean(values)\n",
    "            stats[f'{measurement}_std'] = np.std(values) if len(values) > 1 else 0\n",
    "            stats[f'{measurement}_range'] = max(values) - min(values)\n",
    "\n",
    "            # Add size progression (difference between consecutive sizes)\n",
    "            if len(values) >= 2:\n",
    "                sorted_values = sorted(values)\n",
    "                diffs = [sorted_values[i+1] - sorted_values[i] for i in range(len(sorted_values)-1)]\n",
    "                stats[f'{measurement}_avg_step'] = np.mean(diffs)\n",
    "            else:\n",
    "                stats[f'{measurement}_avg_step'] = 0\n",
    "\n",
    "    # Add summary stats\n",
    "    stats['num_sizes'] = len(measurements_by_size)\n",
    "    stats['has_measurements'] = 1 if measurements_by_size else 0\n",
    "\n",
    "    # Get garment-specific measurements\n",
    "    if garment_type in GARMENT_TYPE_MEASUREMENTS:\n",
    "        expected_measurements = GARMENT_TYPE_MEASUREMENTS[garment_type]\n",
    "        for measurement in expected_measurements:\n",
    "            stats[f'has_{measurement}'] = 1 if measurement in all_measurements else 0\n",
    "\n",
    "    return stats\n",
    "\n",
    "print(\"   Extracting measurement statistics...\")\n",
    "measurement_stats_list = []\n",
    "for idx, row in features_df.iterrows():\n",
    "    stats = extract_measurement_stats(row['measurements_by_size'], row['garment_type'])\n",
    "    measurement_stats_list.append(stats)\n",
    "\n",
    "# Convert to DataFrame\n",
    "measurement_stats_df = pd.DataFrame(measurement_stats_list)\n",
    "print(f\"   ‚úÖ Extracted {len(measurement_stats_df.columns)} measurement features\")\n",
    "\n",
    "# Merge with main features DataFrame\n",
    "features_df = pd.concat([features_df, measurement_stats_df], axis=1)\n",
    "\n",
    "# ========== 4. CREATE SIZE-RELATED FEATURES ==========\n",
    "print(\"\\n4. üìê Creating size-related features...\")\n",
    "\n",
    "def create_size_features(size_stock_dict, measurements_by_size):\n",
    "    \"\"\"Create features related to size availability and distribution\"\"\"\n",
    "    features = {\n",
    "        'size_variety': 0,\n",
    "        'avg_stock_per_size': 0,\n",
    "        'max_stock_size': '',\n",
    "        'min_stock_size': '',\n",
    "        'total_available_sizes': 0\n",
    "    }\n",
    "\n",
    "    # Size availability from stock\n",
    "    if size_stock_dict:\n",
    "        features['total_available_sizes'] = len(size_stock_dict)\n",
    "        stock_values = list(size_stock_dict.values())\n",
    "        features['size_variety'] = len(stock_values)\n",
    "        features['avg_stock_per_size'] = np.mean(stock_values) if stock_values else 0\n",
    "\n",
    "        if stock_values:\n",
    "            max_stock = max(stock_values)\n",
    "            min_stock = min(stock_values)\n",
    "            for size, stock in size_stock_dict.items():\n",
    "                if stock == max_stock:\n",
    "                    features['max_stock_size'] = size\n",
    "                if stock == min_stock:\n",
    "                    features['min_stock_size'] = size\n",
    "\n",
    "    # Size range from measurements\n",
    "    if measurements_by_size:\n",
    "        # Convert size labels to numeric values for analysis\n",
    "        size_order = {size: i for i, size in enumerate(STANDARD_SIZES)}\n",
    "        available_sizes = []\n",
    "        for size in measurements_by_size.keys():\n",
    "            if size in size_order:\n",
    "                available_sizes.append(size_order[size])\n",
    "            elif size.upper() in size_order:\n",
    "                available_sizes.append(size_order[size.upper()])\n",
    "\n",
    "        if available_sizes:\n",
    "            features['size_range_numeric'] = max(available_sizes) - min(available_sizes)\n",
    "            features['min_size_numeric'] = min(available_sizes)\n",
    "            features['max_size_numeric'] = max(available_sizes)\n",
    "\n",
    "    return features\n",
    "\n",
    "print(\"   Creating size distribution features...\")\n",
    "size_features_list = []\n",
    "for idx, row in features_df.iterrows():\n",
    "    features = create_size_features(row.get('size_stock', {}), row.get('measurements_by_size', {}))\n",
    "    size_features_list.append(features)\n",
    "\n",
    "size_features_df = pd.DataFrame(size_features_list)\n",
    "features_df = pd.concat([features_df, size_features_df], axis=1)\n",
    "print(f\"   ‚úÖ Created {len(size_features_df.columns)} size-related features\")\n",
    "\n",
    "# ========== 5. CORRECT CATEGORIZATION WITH PRECISE RULES ==========\n",
    "print(\"\\n5. üè∑Ô∏è Correct categorization with precise rules...\")\n",
    "\n",
    "# Define precise categorization rules\n",
    "garment_type_to_category = {\n",
    "    # Tops\n",
    "    't_shirt': ('top', 'casual'),\n",
    "    'v_neck_tee': ('top', 'casual'),\n",
    "    'fitted_shirt': ('top', 'business_casual'),\n",
    "    'dress_shirt': ('top', 'formal'),\n",
    "    'polo_shirt': ('top', 'business_casual'),\n",
    "    'henley_shirt': ('top', 'casual'),\n",
    "\n",
    "    # Sweaters & Hoodies\n",
    "    'crewneck_sweater': ('top', 'casual'),\n",
    "    'cardigan': ('top', 'casual'),\n",
    "    'turtleneck': ('top', 'casual'),\n",
    "    'pullover_hoodie': ('top', 'casual'),\n",
    "    'zip_hoodie': ('top', 'casual'),\n",
    "\n",
    "    # Bottoms\n",
    "    'slim_pants': ('bottom', 'business_casual'),\n",
    "    'regular_pants': ('bottom', 'business_casual'),\n",
    "    'cargo_pants': ('bottom', 'casual'),\n",
    "    'regular_jeans': ('bottom', 'casual'),\n",
    "    'slim_jeans': ('bottom', 'casual'),\n",
    "    'casual_shorts': ('bottom', 'casual'),\n",
    "    'cargo_shorts': ('bottom', 'casual'),\n",
    "\n",
    "    # Athletic\n",
    "    'training_shorts': ('bottom', 'athletic'),\n",
    "    'yoga_pants': ('bottom', 'athletic'),\n",
    "    'leggings': ('bottom', 'athletic'),\n",
    "\n",
    "    # Dresses\n",
    "    'a_line_dress': ('dress', 'business_casual'),\n",
    "    'bodycon_dress': ('dress', 'business_casual'),\n",
    "    'maxi_dress': ('dress', 'casual'),\n",
    "    'midi_dress': ('dress', 'business_casual'),\n",
    "    'wrap_dress': ('dress', 'business_casual'),\n",
    "\n",
    "    # Skirts\n",
    "    'a_line_skirt': ('bottom', 'business_casual'),\n",
    "    'pencil_skirt': ('bottom', 'business_casual'),\n",
    "    'tennis_skirt': ('bottom', 'athletic'),\n",
    "\n",
    "    # Outerwear\n",
    "    'bomber_jacket': ('outerwear', 'casual'),\n",
    "    'denim_jacket': ('outerwear', 'casual'),\n",
    "    'windbreaker': ('outerwear', 'casual'),\n",
    "    'puffer_jacket': ('outerwear', 'casual'),\n",
    "    'trench_coat': ('outerwear', 'formal'),\n",
    "\n",
    "    # Swimwear\n",
    "    'bikini_top': ('swimwear', 'athletic'),\n",
    "    'swim_trunks': ('swimwear', 'athletic'),\n",
    "    'board_shorts': ('swimwear', 'athletic'),\n",
    "    'one_piece_swimsuit': ('swimwear', 'athletic'),\n",
    "    'rash_guard': ('swimwear', 'athletic'),\n",
    "\n",
    "    # Footwear\n",
    "    'sneakers': ('footwear', 'casual'),\n",
    "    'dress_shoes': ('footwear', 'formal'),\n",
    "\n",
    "    # Underwear\n",
    "    'briefs': ('underwear', 'casual'),\n",
    "    'boxer_briefs': ('underwear', 'casual'),\n",
    "\n",
    "    # Socks\n",
    "    'crew_socks': ('socks', 'casual'),\n",
    "    'ankle_socks': ('socks', 'casual'),\n",
    "}\n",
    "\n",
    "# Apply categorization\n",
    "features_df['garment_category'] = 'other'\n",
    "features_df['garment_formality'] = 'casual'\n",
    "\n",
    "for idx, row in features_df.iterrows():\n",
    "    garment_type = row['garment_type']\n",
    "    if garment_type in garment_type_to_category:\n",
    "        category, formality = garment_type_to_category[garment_type]\n",
    "        features_df.at[idx, 'garment_category'] = category\n",
    "        features_df.at[idx, 'garment_formality'] = formality\n",
    "    else:\n",
    "        # Fallback based on name\n",
    "        name_lower = str(row['name']).lower()\n",
    "        if any(word in name_lower for word in ['dress', 'gown']):\n",
    "            features_df.at[idx, 'garment_category'] = 'dress'\n",
    "            features_df.at[idx, 'garment_formality'] = 'business_casual'\n",
    "        elif any(word in name_lower for word in ['shirt', 'blouse', 'top', 'tee']):\n",
    "            features_df.at[idx, 'garment_category'] = 'top'\n",
    "            features_df.at[idx, 'garment_formality'] = 'business_casual' if 'shirt' in name_lower else 'casual'\n",
    "        elif any(word in name_lower for word in ['pants', 'jeans', 'shorts', 'skirt']):\n",
    "            features_df.at[idx, 'garment_category'] = 'bottom'\n",
    "            features_df.at[idx, 'garment_formality'] = 'business_casual' if 'pants' in name_lower and 'dress' in name_lower else 'casual'\n",
    "        elif any(word in name_lower for word in ['jacket', 'coat', 'blazer']):\n",
    "            features_df.at[idx, 'garment_category'] = 'outerwear'\n",
    "            features_df.at[idx, 'garment_formality'] = 'formal' if 'coat' in name_lower else 'casual'\n",
    "        elif any(word in name_lower for word in ['shoes', 'sneakers', 'boots']):\n",
    "            features_df.at[idx, 'garment_category'] = 'footwear'\n",
    "            features_df.at[idx, 'garment_formality'] = 'formal' if 'dress' in name_lower else 'casual'\n",
    "\n",
    "# ========== 6. SPECIAL FIXES FOR SPECIFIC ITEMS ==========\n",
    "print(\"\\n6. üîß Applying special fixes for specific items...\")\n",
    "\n",
    "# Fix 1: Performance Training items should be athletic\n",
    "print(\"   Fixing 'Performance Training' items to 'athletic'...\")\n",
    "mask = features_df['name'].str.contains('Performance Training', case=False, na=False)\n",
    "features_df.loc[mask, 'garment_formality'] = 'athletic'\n",
    "print(f\"   ‚úÖ Fixed {mask.sum()} 'Performance Training' items\")\n",
    "\n",
    "# Fix 2: Training items should be athletic\n",
    "print(\"   Fixing 'Training' items to 'athletic'...\")\n",
    "mask = features_df['name'].str.contains('Training', case=False, na=False) & \\\n",
    "       ~features_df['name'].str.contains('Performance Training', case=False, na=False)\n",
    "features_df.loc[mask, 'garment_formality'] = 'athletic'\n",
    "print(f\"   ‚úÖ Fixed {mask.sum()} 'Training' items\")\n",
    "\n",
    "# Fix 3: Athletic items should be athletic\n",
    "print(\"   Fixing 'Athletic' items to 'athletic'...\")\n",
    "mask = features_df['name'].str.contains('Athletic', case=False, na=False)\n",
    "features_df.loc[mask, 'garment_formality'] = 'athletic'\n",
    "print(f\"   ‚úÖ Fixed {mask.sum()} 'Athletic' items\")\n",
    "\n",
    "print(f\"   ‚úÖ Categorized all items with special fixes\")\n",
    "\n",
    "# ========== 7. CREATE COLOR FEATURES ==========\n",
    "print(\"\\n7. üé® Creating color features...\")\n",
    "\n",
    "# Create color features\n",
    "all_colors = []\n",
    "for colors in features_df['colors']:\n",
    "    all_colors.extend(colors)\n",
    "\n",
    "top_colors = [color for color, count in Counter(all_colors).most_common(10)]\n",
    "\n",
    "color_themes = {\n",
    "    'dark_colors': ['Black', 'Navy', 'Charcoal', 'Dark', 'Brown', 'Dark Blue', 'Dark Gray'],\n",
    "    'light_colors': ['White', 'Beige', 'Ivory', 'Cream', 'Light', 'Light Gray'],\n",
    "    'bold_colors': ['Red', 'Blue', 'Green', 'Yellow', 'Pink', 'Orange', 'Purple', 'Royal Blue', 'Burgundy'],\n",
    "    'neutral_colors': ['Gray', 'Beige', 'White', 'Black', 'Navy', 'Brown', 'Charcoal', 'Dark Gray']\n",
    "}\n",
    "\n",
    "for theme_name, colors in color_themes.items():\n",
    "    features_df[f'has_{theme_name}'] = features_df['colors'].apply(\n",
    "        lambda x: 1 if any(color in str(color_item) for color in colors for color_item in x) else 0\n",
    "    )\n",
    "\n",
    "print(f\"   ‚úÖ Created {len(color_themes)} color theme features\")\n",
    "\n",
    "# ========== 8. ENCODE CATEGORICAL FEATURES ==========\n",
    "print(\"\\n8. üî† Encoding categorical features...\")\n",
    "\n",
    "categorical_columns = ['category', 'store', 'garment_type', 'garment_category', 'garment_formality', 'fit_type', 'ease', 'stretch']\n",
    "encoders = {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    if col in features_df.columns:\n",
    "        encoders[col] = LabelEncoder()\n",
    "        # Fill NaN with 'unknown' before encoding\n",
    "        features_df[col] = features_df[col].fillna('unknown')\n",
    "        features_df[f'{col}_encoded'] = encoders[col].fit_transform(features_df[col].astype(str))\n",
    "\n",
    "print(f\"   ‚úÖ Encoded {len(encoders)} categorical features\")\n",
    "\n",
    "# ========== 9. SCALE NUMERICAL FEATURES ==========\n",
    "print(\"\\n9. üìè Scaling numerical features...\")\n",
    "\n",
    "# Identify numerical columns (excluding encoded ones)\n",
    "numerical_columns = []\n",
    "for col in features_df.columns:\n",
    "    if (col not in ['item_id', 'name', 'description', 'colors', 'color_stocks',\n",
    "                   'measurements_by_size', 'size_stock', 'garment_type_db',\n",
    "                   'max_stock_size', 'min_stock_size'] and\n",
    "        not col.endswith('_encoded') and\n",
    "        not col.startswith('has_') and\n",
    "        features_df[col].dtype in [np.float64, np.int64, np.float32, np.int32]):\n",
    "        numerical_columns.append(col)\n",
    "\n",
    "# Filter to most important numerical features\n",
    "important_numerical = ['price', 'total_stock', 'num_sizes', 'size_variety',\n",
    "                      'avg_stock_per_size', 'total_available_sizes',\n",
    "                      'size_range_numeric', 'min_size_numeric', 'max_size_numeric']\n",
    "\n",
    "# Add measurement stats that exist\n",
    "for col in important_numerical:\n",
    "    if col in features_df.columns:\n",
    "        numerical_columns.append(col)\n",
    "\n",
    "# Also add key measurement stats (min values for common measurements)\n",
    "common_measurements = ['chest_circumference', 'waist_circumference', 'hips_circumference',\n",
    "                      'garment_length', 'sleeve_length', 'inseam_length']\n",
    "for measurement in common_measurements:\n",
    "    min_col = f'{measurement}_min'\n",
    "    if min_col in features_df.columns:\n",
    "        numerical_columns.append(min_col)\n",
    "\n",
    "# Remove duplicates\n",
    "numerical_columns = list(set(numerical_columns))\n",
    "\n",
    "# Fill NaN with 0 for scaling\n",
    "for col in numerical_columns:\n",
    "    if col in features_df.columns:\n",
    "        features_df[col] = features_df[col].fillna(0)\n",
    "\n",
    "if numerical_columns:\n",
    "    scaler = StandardScaler()\n",
    "    scaled_values = scaler.fit_transform(features_df[numerical_columns])\n",
    "    scaled_df = pd.DataFrame(scaled_values, columns=[f'scaled_{col}' for col in numerical_columns])\n",
    "    features_df = pd.concat([features_df.reset_index(drop=True), scaled_df.reset_index(drop=True)], axis=1)\n",
    "    print(f\"   ‚úÖ Scaled {len(numerical_columns)} numerical features\")\n",
    "    print(f\"   Scaled columns: {numerical_columns[:10]}...\")  # Show first 10\n",
    "else:\n",
    "    scaler = None\n",
    "    print(\"   ‚ö†Ô∏è No numerical columns to scale\")\n",
    "\n",
    "# ========== 10. CREATE FINAL FEATURE MATRIX ==========\n",
    "print(\"\\n10. üéØ Creating final feature matrix for size-based recommendations...\")\n",
    "\n",
    "# Collect all feature types\n",
    "encoded_cols = [col for col in features_df.columns if col.endswith('_encoded')]\n",
    "scaled_cols = [col for col in features_df.columns if col.startswith('scaled_')]\n",
    "binary_cols = [col for col in features_df.columns if col.startswith('has_')]\n",
    "\n",
    "# Add key measurement features (min values for fitting)\n",
    "measurement_min_cols = [col for col in features_df.columns if col.endswith('_min')]\n",
    "measurement_range_cols = [col for col in features_df.columns if col.endswith('_range')]\n",
    "\n",
    "# Combine all feature columns\n",
    "all_feature_cols = encoded_cols + scaled_cols + binary_cols + measurement_min_cols[:10] + measurement_range_cols[:10]\n",
    "\n",
    "# Remove duplicates\n",
    "all_feature_cols = list(set(all_feature_cols))\n",
    "\n",
    "# Filter to columns that actually exist\n",
    "all_feature_cols = [col for col in all_feature_cols if col in features_df.columns]\n",
    "\n",
    "# Create final feature matrix\n",
    "feature_matrix = features_df[all_feature_cols].copy()\n",
    "\n",
    "# Fill any remaining NaN with 0\n",
    "feature_matrix = feature_matrix.fillna(0)\n",
    "\n",
    "print(f\"   ‚úÖ Feature matrix shape: {feature_matrix.shape}\")\n",
    "print(f\"   ‚úÖ Total features: {len(all_feature_cols)}\")\n",
    "print(f\"   Feature breakdown:\")\n",
    "print(f\"     - Encoded: {len(encoded_cols)}\")\n",
    "print(f\"     - Scaled: {len(scaled_cols)}\")\n",
    "print(f\"     - Binary: {len(binary_cols)}\")\n",
    "print(f\"     - Measurements: {len(measurement_min_cols) + len(measurement_range_cols)}\")\n",
    "\n",
    "# ========== 11. SAVE PROCESSED DATA ==========\n",
    "print(\"\\n11. üíæ Saving processed data for size-based recommendations...\")\n",
    "\n",
    "# Save raw measurements for later use in size matching\n",
    "raw_measurements_data = {\n",
    "    'measurements_by_size': features_df['measurements_by_size'].tolist(),\n",
    "    'item_ids': features_df['item_id'].tolist(),\n",
    "    'garment_types': features_df['garment_type'].tolist()\n",
    "}\n",
    "\n",
    "with open(ARTIFACTS_DIR / 'raw_measurements.pkl', 'wb') as f:\n",
    "    pickle.dump(raw_measurements_data, f)\n",
    "\n",
    "# Save artifacts\n",
    "with open(ARTIFACTS_DIR / 'feature_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(encoders, f)\n",
    "\n",
    "if scaler:\n",
    "    with open(ARTIFACTS_DIR / 'scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "# Save features DataFrame with all information\n",
    "features_df.to_pickle(ARTIFACTS_DIR / 'features_df.pkl')\n",
    "\n",
    "# Save feature matrix for ML models\n",
    "feature_matrix.to_pickle(ARTIFACTS_DIR / 'feature_matrix.pkl')\n",
    "\n",
    "# Save measurement mapping for size-based recommendations\n",
    "measurement_mapping = {\n",
    "    'garment_type_measurements': GARMENT_TYPE_MEASUREMENTS,\n",
    "    'standard_sizes': STANDARD_SIZES,\n",
    "    'feature_columns': all_feature_cols\n",
    "}\n",
    "\n",
    "with open(ARTIFACTS_DIR / 'measurement_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(measurement_mapping, f)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'total_items': len(features_df),\n",
    "    'total_features': len(all_feature_cols),\n",
    "    'feature_types': {\n",
    "        'encoded': len(encoded_cols),\n",
    "        'scaled': len(scaled_cols),\n",
    "        'binary': len(binary_cols),\n",
    "        'measurement_min': len([c for c in all_feature_cols if c.endswith('_min')]),\n",
    "        'measurement_range': len([c for c in all_feature_cols if c.endswith('_range')])\n",
    "    },\n",
    "    'garment_categories': features_df['garment_category'].nunique(),\n",
    "    'formality_levels': features_df['garment_formality'].nunique(),\n",
    "    'garment_types': features_df['garment_type'].nunique(),\n",
    "    'has_size_measurements': features_df['has_measurements'].sum() if 'has_measurements' in features_df.columns else 0,\n",
    "    'size_based_features': True\n",
    "}\n",
    "\n",
    "with open(ARTIFACTS_DIR / 'feature_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(\"   ‚úÖ All data saved for size-based recommendation system\")\n",
    "\n",
    "# ========== 12. DISPLAY SIZE-BASED SUMMARY ==========\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä SIZE-BASED FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìà Data Overview:\")\n",
    "print(f\"   Total items: {len(features_df)}\")\n",
    "print(f\"   Total features: {len(all_feature_cols)}\")\n",
    "print(f\"   Feature matrix shape: {feature_matrix.shape}\")\n",
    "\n",
    "print(f\"\\nüìè Measurement Coverage:\")\n",
    "if 'has_measurements' in features_df.columns:\n",
    "    measurement_count = features_df['has_measurements'].sum()\n",
    "    print(f\"   Items with measurements: {measurement_count} ({measurement_count/len(features_df):.1%})\")\n",
    "\n",
    "if 'num_sizes' in features_df.columns:\n",
    "    avg_sizes = features_df['num_sizes'].mean()\n",
    "    print(f\"   Average sizes per item: {avg_sizes:.1f}\")\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è Garment Categories:\")\n",
    "category_dist = features_df['garment_category'].value_counts()\n",
    "for cat, count in category_dist.items():\n",
    "    print(f\"   {cat}: {count} items ({count/len(features_df):.1%})\")\n",
    "\n",
    "print(f\"\\nüìê Key Measurement Statistics:\")\n",
    "# Show most common measurements\n",
    "measurement_cols = [col for col in features_df.columns if col.endswith('_min')]\n",
    "if measurement_cols:\n",
    "    print(f\"   Found {len(measurement_cols)} different measurement types\")\n",
    "    # Show top 5 most common measurements\n",
    "    for col in measurement_cols[:5]:\n",
    "        non_zero = (features_df[col] > 0).sum()\n",
    "        if non_zero > 0:\n",
    "            measurement_name = col.replace('_min', '')\n",
    "            print(f\"   {measurement_name}: {non_zero} items ({non_zero/len(features_df):.1%})\")\n",
    "\n",
    "print(f\"\\nüé≠ Formality Levels:\")\n",
    "formality_dist = features_df['garment_formality'].value_counts()\n",
    "for level, count in formality_dist.items():\n",
    "    print(f\"   {level}: {count} items ({count/len(features_df):.1%})\")\n",
    "\n",
    "# Show sample of items with measurements\n",
    "print(f\"\\nüìã SAMPLE ITEMS WITH MEASUREMENTS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "sample_with_measurements = features_df[features_df['num_sizes'] > 0].head(5) if 'num_sizes' in features_df.columns else features_df.head(5)\n",
    "for idx, row in sample_with_measurements.iterrows():\n",
    "    name = row['name'][:35] + \"...\" if len(row['name']) > 35 else row['name']\n",
    "    sizes = len(row['measurements_by_size']) if isinstance(row['measurements_by_size'], dict) else 0\n",
    "    garment_type = row['garment_type']\n",
    "\n",
    "    # Get some measurement stats\n",
    "    if 'chest_circumference_min' in features_df.columns:\n",
    "        chest_min = row.get('chest_circumference_min', 'N/A')\n",
    "        chest_max = row.get('chest_circumference_max', 'N/A')\n",
    "        print(f\"{name:<40} | {garment_type:<15} | {sizes:>2} sizes | Chest: {chest_min}-{chest_max}cm\")\n",
    "    else:\n",
    "        print(f\"{name:<40} | {garment_type:<15} | {sizes:>2} sizes\")\n",
    "\n",
    "print(f\"\\n‚ú® Special Features for Size-Based Recommendations:\")\n",
    "print(f\"   - Raw measurements preserved for exact size matching\")\n",
    "print(f\"   - Statistical features (min, max, mean, range) for each measurement\")\n",
    "print(f\"   - Size availability features (variety, stock distribution)\")\n",
    "print(f\"   - Garment-type specific measurement validation\")\n",
    "print(f\"   - Ready for user measurement comparison\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ STEP 2 COMPLETED SUCCESSFULLY!\")\n",
    "print(\"‚úÖ Size-based measurement extraction implemented\")\n",
    "print(\"‚úÖ Statistical features created for all measurements\")\n",
    "print(\"‚úÖ Raw measurements preserved for exact matching\")\n",
    "print(\"‚úÖ All data saved for recommendation system\")\n",
    "print(\"‚úÖ Ready for Step 3: Create embeddings with size features\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "S6sUt2QpnfzA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ STEP 3: Create Item Embeddings (CORRECTED - LOCAL VERSION)\n",
      "============================================================\n",
      "‚úÖ All libraries imported\n",
      "üìÅ Working directory: c:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\n",
      "üìÅ Artifacts directory: c:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\\artifacts\n",
      "\n",
      "1. üîÑ Loading data from Step 2...\n",
      "   Looking for Step 2 artifacts...\n",
      "   Available files:\n",
      "   - complete_outfit_system.pkl\n",
      "   - complete_size_system.pkl\n",
      "   - embeddings\n",
      "   - features_df.pkl\n",
      "   - feature_encoders.pkl\n",
      "   - feature_matrix.pkl\n",
      "   - feature_metadata.pkl\n",
      "   - hybrid_recommender.pkl\n",
      "   - intelligent_outfit_builder.pkl\n",
      "   - intelligent_outfit_builder_fixed_names.pkl\n",
      "   - measurement_mapping.pkl\n",
      "   - raw_measurements.pkl\n",
      "   - scaler.pkl\n",
      "   - size_recommender_v2.pkl\n",
      "   - size_system_summary.pkl\n",
      "   - user_108.json\n",
      "   - user_114.json\n",
      "   - user_116.json\n",
      "   - user_212.json\n",
      "   - user_213.json\n",
      "   - user_214.json\n",
      "   - user_215.json\n",
      "   - user_216.json\n",
      "   - user_217.json\n",
      "   - user_317.json\n",
      "   - original_items.pkl\n",
      "   ‚úÖ Loaded features_df: (250, 314)\n",
      "   ‚úÖ Loaded feature_matrix: (250, 186)\n",
      "   Columns in features_df: 314\n",
      "   Sample columns: ['item_id', 'name', 'description', 'price', 'category', 'store', 'total_stock', 'garment_type_db', 'colors', 'color_stocks']\n",
      "\n",
      "2. üîç Verifying data from Step 2...\n",
      "   ‚úÖ All required columns found\n",
      "   Garment categories found: 9\n",
      "   Categories: ['top', 'bottom', 'dress', 'outerwear', 'swimwear', 'underwear', 'socks', 'footwear', 'other']\n",
      "   Category distribution:\n",
      "     bottom: 64 items\n",
      "     top: 49 items\n",
      "     other: 43 items\n",
      "     outerwear: 28 items\n",
      "     underwear: 18 items\n",
      "     dress: 17 items\n",
      "     socks: 11 items\n",
      "     swimwear: 10 items\n",
      "     footwear: 10 items\n",
      "   Found 8 encoded columns from Step 2\n",
      "   Found 129 scaled columns from Step 2\n",
      "   Found 168 measurement columns\n",
      "\n",
      "3. üõ†Ô∏è Preparing for embedding creation...\n",
      "   Converted garment_category to string\n",
      "   Converted garment_formality to string\n",
      "   Creating category-aware features...\n",
      "   Creating clothing vs non-clothing feature...\n",
      "   Creating one-hot category features...\n",
      "   Added 9 category features\n",
      "   Creating formality features...\n",
      "   Added 4 formality features\n",
      "   Creating interaction features...\n",
      "   ‚úÖ Scaled price feature\n",
      "\n",
      "4. üéØ Selecting features for embeddings...\n",
      "   ‚úÖ category_features: 9 features (weight: 5.0x)\n",
      "   ‚úÖ category_strength: 2 features (weight: 5.0x)\n",
      "   ‚úÖ formality_features: 4 features (weight: 3.0x)\n",
      "   ‚úÖ formality_strength: 1 features (weight: 5.0x)\n",
      "   ‚úÖ interaction_features: 2 features (weight: 3.0x)\n",
      "   ‚úÖ encoded_features: 8 features (weight: 2.0x)\n",
      "   ‚úÖ scaled_features: 129 features (weight: 1.0x)\n",
      "   ‚úÖ color_features: 4 features (weight: 1.0x)\n",
      "   ‚úÖ price_feature: 1 features (weight: 1.0x)\n",
      "\n",
      "   Total selected features: 160\n",
      "\n",
      "   Creating weighted feature matrix...\n",
      "   ‚úÖ Weighted feature matrix: (250, 160)\n",
      "\n",
      "5. ü§ñ Creating embeddings with PCA...\n",
      "   Creating 32 PCA components\n",
      "   ‚úÖ PCA embeddings created: (250, 32)\n",
      "   Explained variance ratio: 99.81%\n",
      "\n",
      "   Top 5 components explain: 66.7% of variance\n",
      "\n",
      "6. üè∑Ô∏è Validating embeddings with clustering...\n",
      "   Creating 9 clusters\n",
      "\n",
      "   Cluster-Category Alignment:\n",
      "--------------------------------------------------\n",
      "   Cluster 0: dress           purity = 51.5%\n",
      "   Cluster 1: top             purity = 63.6%\n",
      "   Cluster 2: other           purity = 100.0%\n",
      "   Cluster 3: bottom          purity = 100.0%\n",
      "   Cluster 4: underwear       purity = 43.9%\n",
      "   Cluster 5: other           purity = 100.0%\n",
      "   Cluster 6: socks           purity = 100.0%\n",
      "   Cluster 7: footwear        purity = 83.3%\n",
      "   Cluster 8: other           purity = 100.0%\n",
      "\n",
      "   Average cluster purity: 82.5%\n",
      "\n",
      "7. üîç Creating similarity function...\n",
      "\n",
      "   Testing similarity function...\n",
      "--------------------------------------------------\n",
      "   Classic Crew Tee     ‚Üí Graphic Print Tee    (96.0%)\n",
      "   V-Neck Tee           ‚Üí V-Neck Tee Variant 1 (99.3%)\n",
      "   Graphic Print Tee    ‚Üí Classic Crew Tee     (96.0%)\n",
      "\n",
      "8. üíæ Creating final embeddings dataframe...\n",
      "   ‚úÖ Embeddings dataframe: (250, 41)\n",
      "   Embedding dimensions: 32\n",
      "   Items with embeddings: 250\n",
      "\n",
      "9. üíæ Saving all artifacts...\n",
      "   ‚úÖ Saved item_embeddings.pkl\n",
      "   ‚úÖ Saved embeddings_array.npy\n",
      "   ‚úÖ Saved pca_model.pkl\n",
      "   ‚úÖ Saved kmeans_model.pkl\n",
      "   ‚úÖ Saved scaler_pca.pkl\n",
      "   ‚úÖ Saved robust_features.pkl\n",
      "   ‚úÖ Saved similarity test results\n",
      "   ‚úÖ Saved embeddings summary\n",
      "\n",
      "10. üìä FINAL RESULTS\n",
      "============================================================\n",
      "\n",
      "üéØ METHOD: PCA with weighted feature engineering\n",
      "üìà Embedding Dimensions: 32\n",
      "üìä Explained Variance: 99.8%\n",
      "üè∑Ô∏è  Cluster Purity: 82.5%\n",
      "üî¢ Total Items: 250\n",
      "\n",
      "üìã Feature Engineering:\n",
      "   - Total features used: 160\n",
      "   - Category features: 9\n",
      "   - Formality features: 4\n",
      "   - Encoded features: 8\n",
      "   - Scaled features: 129\n",
      "\n",
      "üìÅ Output Files (saved to ./artifacts/embeddings/):\n",
      "   ‚Ä¢ embeddings_summary.pkl\n",
      "   ‚Ä¢ item_embeddings.pkl\n",
      "   ‚Ä¢ kmeans_model.pkl\n",
      "   ‚Ä¢ pca_model.pkl\n",
      "   ‚Ä¢ robust_features.pkl\n",
      "   ‚Ä¢ scaler_pca.pkl\n",
      "   ‚Ä¢ similarity_test_results.pkl\n",
      "   ‚Ä¢ embeddings_array.npy\n",
      "   ‚Ä¢ embeddings_summary.pkl\n",
      "   ‚Ä¢ item_embeddings.pkl\n",
      "   ‚Ä¢ kmeans_model.pkl\n",
      "   ‚Ä¢ pca_model.pkl\n",
      "   ‚Ä¢ robust_features.pkl\n",
      "   ‚Ä¢ scaler_pca.pkl\n",
      "   ‚Ä¢ similarity_test_results.pkl\n",
      "   ‚Ä¢ embeddings_array.npy\n",
      "\n",
      "üé≠ Category Distribution:\n",
      "   bottom         :  64 items\n",
      "   top            :  49 items\n",
      "   other          :  43 items\n",
      "   outerwear      :  28 items\n",
      "   underwear      :  18 items\n",
      "   dress          :  17 items\n",
      "   socks          :  11 items\n",
      "   swimwear       :  10 items\n",
      "   footwear       :  10 items\n",
      "\n",
      "============================================================\n",
      "‚úÖ EXCELLENT EMBEDDINGS - READY FOR PRODUCTION!\n",
      "\n",
      "üéØ Proceed to Step 4: Size Recommendation Engine\n",
      "============================================================\n",
      "\n",
      "üìã SAMPLE EMBEDDINGS (first 3 items):\n",
      "--------------------------------------------------\n",
      "\n",
      "Classic Crew Tee               (top/casual)\n",
      "  Cluster: 1\n",
      "  Embedding norm: 8.87\n",
      "  First 3 dims: [7.162, -2.221, -0.4]\n",
      "\n",
      "V-Neck Tee                     (top/casual)\n",
      "  Cluster: 1\n",
      "  Embedding norm: 8.92\n",
      "  First 3 dims: [7.291, -2.317, -0.506]\n",
      "\n",
      "Graphic Print Tee              (top/casual)\n",
      "  Cluster: 1\n",
      "  Embedding norm: 8.90\n",
      "  First 3 dims: [7.108, -2.234, -0.532]\n",
      "\n",
      "============================================================\n",
      "üéâ STEP 3 COMPLETE WITH PCA EMBEDDINGS!\n",
      "‚úÖ Ready for size recommendation engine\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# @title üéØ **STEP 3: Create Item Embeddings (CORRECTED - LOCAL VERSION)**\n",
    "print(\"üéØ STEP 3: Create Item Embeddings (CORRECTED - LOCAL VERSION)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported\")\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "# Use same directory structure as Steps 1 and 2\n",
    "ARTIFACTS_DIR = Path('./artifacts')\n",
    "ORIGINAL_DATA_DIR = Path('./')\n",
    "\n",
    "print(f\"üìÅ Working directory: {Path.cwd()}\")\n",
    "print(f\"üìÅ Artifacts directory: {ARTIFACTS_DIR.absolute()}\")\n",
    "\n",
    "# ========== 1. LOAD & PREPARE DATA ==========\n",
    "print(\"\\n1. üîÑ Loading data from Step 2...\")\n",
    "\n",
    "# Check what files exist from Step 2\n",
    "print(\"   Looking for Step 2 artifacts...\")\n",
    "available_files = list(ARTIFACTS_DIR.glob('*')) + list(Path('.').glob('*.pkl'))\n",
    "print(\"   Available files:\")\n",
    "for file in available_files:\n",
    "    print(f\"   - {file.name}\")\n",
    "\n",
    "# Load the processed data from Step 2 (should be in artifacts folder)\n",
    "try:\n",
    "    # Load features DataFrame from Step 2\n",
    "    features_df_path = ARTIFACTS_DIR / 'features_df.pkl'\n",
    "    if features_df_path.exists():\n",
    "        features_df = pd.read_pickle(features_df_path)\n",
    "        print(f\"   ‚úÖ Loaded features_df: {features_df.shape}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå features_df.pkl not found in {ARTIFACTS_DIR}\")\n",
    "        raise FileNotFoundError(f\"features_df.pkl not found\")\n",
    "\n",
    "    # Load feature matrix from Step 2\n",
    "    feature_matrix_path = ARTIFACTS_DIR / 'feature_matrix.pkl'\n",
    "    if feature_matrix_path.exists():\n",
    "        feature_matrix = pd.read_pickle(feature_matrix_path)\n",
    "        print(f\"   ‚úÖ Loaded feature_matrix: {feature_matrix.shape}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è feature_matrix.pkl not found, using features_df\")\n",
    "        feature_matrix = None\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error loading Step 2 data: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"   Columns in features_df: {len(features_df.columns)}\")\n",
    "print(f\"   Sample columns: {list(features_df.columns)[:10]}\")\n",
    "\n",
    "# ========== 2. VERIFY DATA FROM STEP 2 ==========\n",
    "print(\"\\n2. üîç Verifying data from Step 2...\")\n",
    "\n",
    "# Check critical columns exist\n",
    "required_columns = ['item_id', 'name', 'garment_category', 'garment_formality', 'price']\n",
    "missing_columns = [col for col in required_columns if col not in features_df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"   ‚ö†Ô∏è Missing columns: {missing_columns}\")\n",
    "    print(\"   Checking for alternative column names...\")\n",
    "\n",
    "    # Try to find alternative column names\n",
    "    for col in missing_columns:\n",
    "        possible_matches = [c for c in features_df.columns if col in str(c).lower()]\n",
    "        if possible_matches:\n",
    "            print(f\"   Found possible match for '{col}': {possible_matches[0]}\")\n",
    "else:\n",
    "    print(\"   ‚úÖ All required columns found\")\n",
    "\n",
    "# Check garment categories\n",
    "if 'garment_category' in features_df.columns:\n",
    "    categories = features_df['garment_category'].unique()\n",
    "    print(f\"   Garment categories found: {len(categories)}\")\n",
    "    print(f\"   Categories: {list(categories)}\")\n",
    "\n",
    "    # Show distribution\n",
    "    category_dist = features_df['garment_category'].value_counts()\n",
    "    print(f\"   Category distribution:\")\n",
    "    for cat, count in category_dist.items():\n",
    "        print(f\"     {cat}: {count} items\")\n",
    "\n",
    "# Check if we have encoded columns from Step 2\n",
    "encoded_cols = [col for col in features_df.columns if col.endswith('_encoded')]\n",
    "print(f\"   Found {len(encoded_cols)} encoded columns from Step 2\")\n",
    "\n",
    "# Check if we have scaled columns from Step 2\n",
    "scaled_cols = [col for col in features_df.columns if col.startswith('scaled_')]\n",
    "print(f\"   Found {len(scaled_cols)} scaled columns from Step 2\")\n",
    "\n",
    "# Check measurement features\n",
    "measurement_cols = [col for col in features_df.columns if any(m in col for m in ['min', 'max', 'mean', 'range'])]\n",
    "print(f\"   Found {len(measurement_cols)} measurement columns\")\n",
    "\n",
    "# ========== 3. PREPARE FOR EMBEDDING CREATION ==========\n",
    "print(\"\\n3. üõ†Ô∏è Preparing for embedding creation...\")\n",
    "\n",
    "# Start with robust features\n",
    "robust_features = features_df.copy()\n",
    "\n",
    "# Ensure critical categorical columns are string type\n",
    "categorical_cols = ['garment_category', 'garment_formality']\n",
    "for col in categorical_cols:\n",
    "    if col in robust_features.columns:\n",
    "        robust_features[col] = robust_features[col].astype(str)\n",
    "        print(f\"   Converted {col} to string\")\n",
    "\n",
    "# === Create category-aware features ===\n",
    "print(\"   Creating category-aware features...\")\n",
    "\n",
    "# Create category strength mapping (same as Step 2 categories)\n",
    "category_strength_map = {\n",
    "    'top': 1.0, 'bottom': 2.0, 'dress': 3.0,\n",
    "    'outerwear': 4.0, 'swimwear': 5.0,\n",
    "    'footwear': 6.0, 'socks': 7.0, 'underwear': 8.0, 'accessory': 9.0\n",
    "}\n",
    "\n",
    "# Add default for 'other' category\n",
    "robust_features['category_strength'] = robust_features['garment_category'].apply(\n",
    "    lambda x: category_strength_map.get(str(x), 0.0)\n",
    ")\n",
    "\n",
    "# Create clothing vs non-clothing feature\n",
    "print(\"   Creating clothing vs non-clothing feature...\")\n",
    "robust_features['is_clothing'] = robust_features['garment_category'].apply(\n",
    "    lambda x: 0.0 if str(x) in ['accessory', 'footwear', 'socks', 'underwear'] else 1.0\n",
    ")\n",
    "\n",
    "# Create one-hot category features\n",
    "print(\"   Creating one-hot category features...\")\n",
    "for category in robust_features['garment_category'].unique():\n",
    "    col_name = f'cat_{category}'\n",
    "    robust_features[col_name] = (robust_features['garment_category'] == category).astype(float) * 5.0\n",
    "\n",
    "print(f\"   Added {len(robust_features['garment_category'].unique())} category features\")\n",
    "\n",
    "# === Create formality features ===\n",
    "print(\"   Creating formality features...\")\n",
    "formality_strength_map = {\n",
    "    'athletic': 1.0, 'casual': 2.0, 'business_casual': 3.0, 'formal': 4.0\n",
    "}\n",
    "\n",
    "robust_features['formality_strength'] = robust_features['garment_formality'].apply(\n",
    "    lambda x: formality_strength_map.get(str(x), 2.0)\n",
    ")\n",
    "\n",
    "# Create one-hot formality features\n",
    "for formality in robust_features['garment_formality'].unique():\n",
    "    col_name = f'form_{formality}'\n",
    "    robust_features[col_name] = (robust_features['garment_formality'] == formality).astype(float) * 3.0\n",
    "\n",
    "print(f\"   Added {len(robust_features['garment_formality'].unique())} formality features\")\n",
    "\n",
    "# === Create interaction features ===\n",
    "print(\"   Creating interaction features...\")\n",
    "robust_features['clothing_formality'] = robust_features['is_clothing'] * robust_features['formality_strength']\n",
    "robust_features['category_formality'] = robust_features['category_strength'] * robust_features['formality_strength']\n",
    "\n",
    "# === Ensure we have price feature ===\n",
    "if 'price' not in robust_features.columns and 'Price' in robust_features.columns:\n",
    "    robust_features['price'] = robust_features['Price']\n",
    "\n",
    "if 'price' in robust_features.columns:\n",
    "    # Scale price\n",
    "    price_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    robust_features['price_scaled'] = price_scaler.fit_transform(robust_features[['price']].fillna(0))\n",
    "    print(\"   ‚úÖ Scaled price feature\")\n",
    "\n",
    "# === Ensure we have total_stock feature ===\n",
    "if 'total_stock' not in robust_features.columns:\n",
    "    if 'Total Stock' in robust_features.columns:\n",
    "        robust_features['total_stock'] = robust_features['Total Stock']\n",
    "    elif 'total_stock_scaled' in scaled_cols:\n",
    "        print(\"   Using existing total_stock_scaled\")\n",
    "\n",
    "# ========== 4. SELECT FEATURES FOR EMBEDDINGS ==========\n",
    "print(\"\\n4. üéØ Selecting features for embeddings...\")\n",
    "\n",
    "# Define feature groups\n",
    "feature_groups = {\n",
    "    # High importance (5x weight)\n",
    "    'category_features': [col for col in robust_features.columns if col.startswith('cat_')],\n",
    "    'category_strength': ['category_strength', 'is_clothing'],\n",
    "\n",
    "    # Medium importance (3x weight)\n",
    "    'formality_features': [col for col in robust_features.columns if col.startswith('form_')],\n",
    "    'formality_strength': ['formality_strength'],\n",
    "    'interaction_features': ['clothing_formality', 'category_formality'],\n",
    "\n",
    "    # Use encoded features from Step 2 (2x weight)\n",
    "    'encoded_features': [col for col in encoded_cols if col in robust_features.columns],\n",
    "\n",
    "    # Use scaled features from Step 2 (1x weight)\n",
    "    'scaled_features': [col for col in scaled_cols if col in robust_features.columns],\n",
    "\n",
    "    # Color features from Step 2\n",
    "    'color_features': [col for col in robust_features.columns if col.startswith('has_') and 'color' in col],\n",
    "\n",
    "    # Price feature\n",
    "    'price_feature': ['price_scaled'] if 'price_scaled' in robust_features.columns else [],\n",
    "}\n",
    "\n",
    "# Collect all features with weights\n",
    "all_features = []\n",
    "weights = []\n",
    "\n",
    "for group_name, feature_list in feature_groups.items():\n",
    "    available_features = [f for f in feature_list if f in robust_features.columns]\n",
    "    if available_features:\n",
    "        # Assign weight based on group\n",
    "        if 'category' in group_name or 'strength' in group_name:\n",
    "            weight = 5.0\n",
    "        elif 'formality' in group_name or 'interaction' in group_name:\n",
    "            weight = 3.0\n",
    "        elif 'encoded' in group_name:\n",
    "            weight = 2.0\n",
    "        else:\n",
    "            weight = 1.0\n",
    "\n",
    "        all_features.extend(available_features)\n",
    "        weights.extend([weight] * len(available_features))\n",
    "\n",
    "        print(f\"   ‚úÖ {group_name}: {len(available_features)} features (weight: {weight}x)\")\n",
    "\n",
    "print(f\"\\n   Total selected features: {len(all_features)}\")\n",
    "\n",
    "# Create weighted feature matrix\n",
    "print(\"\\n   Creating weighted feature matrix...\")\n",
    "X_weighted = np.zeros((len(robust_features), len(all_features)))\n",
    "\n",
    "for i, (feature, weight) in enumerate(zip(all_features, weights)):\n",
    "    X_weighted[:, i] = robust_features[feature].fillna(0).values * weight\n",
    "\n",
    "print(f\"   ‚úÖ Weighted feature matrix: {X_weighted.shape}\")\n",
    "\n",
    "# ========== 5. CREATE EMBEDDINGS WITH PCA ==========\n",
    "print(\"\\n5. ü§ñ Creating embeddings with PCA...\")\n",
    "\n",
    "# Scale the weighted features\n",
    "scaler_pca = StandardScaler()\n",
    "X_weighted_scaled = scaler_pca.fit_transform(X_weighted)\n",
    "\n",
    "# Determine optimal number of components\n",
    "n_samples = X_weighted_scaled.shape[0]\n",
    "n_components = min(32, n_samples - 1)  # Don't exceed n_samples - 1\n",
    "print(f\"   Creating {n_components} PCA components\")\n",
    "\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "embeddings = pca.fit_transform(X_weighted_scaled)\n",
    "\n",
    "print(f\"   ‚úÖ PCA embeddings created: {embeddings.shape}\")\n",
    "print(f\"   Explained variance ratio: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Show top components\n",
    "print(f\"\\n   Top 5 components explain: {pca.explained_variance_ratio_[:5].sum():.1%} of variance\")\n",
    "\n",
    "# ========== 6. CLUSTER VALIDATION ==========\n",
    "print(\"\\n6. üè∑Ô∏è Validating embeddings with clustering...\")\n",
    "\n",
    "# Create clusters based on garment categories\n",
    "n_clusters = min(robust_features['garment_category'].nunique(), 10)\n",
    "print(f\"   Creating {n_clusters} clusters\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=20)\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "robust_features['embedding_cluster'] = cluster_labels\n",
    "\n",
    "# Calculate cluster purity\n",
    "print(\"\\n   Cluster-Category Alignment:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "purity_scores = []\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_items = robust_features[robust_features['embedding_cluster'] == cluster_id]\n",
    "    if len(cluster_items) > 0:\n",
    "        dominant_category = cluster_items['garment_category'].mode()[0]\n",
    "        purity = (cluster_items['garment_category'] == dominant_category).mean()\n",
    "        purity_scores.append(purity)\n",
    "\n",
    "        print(f\"   Cluster {cluster_id}: {dominant_category:<15} purity = {purity:.1%}\")\n",
    "\n",
    "avg_purity = np.mean(purity_scores) if purity_scores else 0\n",
    "print(f\"\\n   Average cluster purity: {avg_purity:.1%}\")\n",
    "\n",
    "# ========== 7. SIMILARITY FUNCTION ==========\n",
    "print(\"\\n7. üîç Creating similarity function...\")\n",
    "\n",
    "def find_similar_items_cosine(item_id, top_k=5, same_category_only=False):\n",
    "    \"\"\"Find similar items using cosine similarity\"\"\"\n",
    "    # Find item index\n",
    "    item_idx = robust_features[robust_features['item_id'] == item_id].index\n",
    "    if len(item_idx) == 0:\n",
    "        return []\n",
    "\n",
    "    item_idx = item_idx[0]\n",
    "    item_embedding = embeddings[item_idx].reshape(1, -1)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(item_embedding, embeddings)[0]\n",
    "\n",
    "    # Get indices sorted by similarity\n",
    "    sorted_indices = np.argsort(similarities)[::-1]\n",
    "\n",
    "    # Collect results\n",
    "    results = []\n",
    "    for idx in sorted_indices:\n",
    "        if idx == item_idx:\n",
    "            continue\n",
    "\n",
    "        if same_category_only:\n",
    "            item_category = robust_features.iloc[item_idx]['garment_category']\n",
    "            other_category = robust_features.iloc[idx]['garment_category']\n",
    "            if item_category != other_category:\n",
    "                continue\n",
    "\n",
    "        results.append({\n",
    "            'item_id': int(robust_features.iloc[idx]['item_id']),\n",
    "            'name': robust_features.iloc[idx]['name'],\n",
    "            'category': robust_features.iloc[idx]['garment_category'],\n",
    "            'formality': robust_features.iloc[idx]['garment_formality'],\n",
    "            'similarity': float(similarities[idx]),\n",
    "            'similarity_percent': float(similarities[idx] * 100)\n",
    "        })\n",
    "\n",
    "        if len(results) >= top_k:\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test similarity function\n",
    "print(\"\\n   Testing similarity function...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Get a few sample items for testing\n",
    "sample_items = robust_features.head(3)\n",
    "for _, item in sample_items.iterrows():\n",
    "    similar = find_similar_items_cosine(item['item_id'], top_k=2, same_category_only=True)\n",
    "    if similar:\n",
    "        print(f\"   {item['name'][:20]:<20} ‚Üí {similar[0]['name'][:20]:<20} ({similar[0]['similarity_percent']:.1f}%)\")\n",
    "\n",
    "# ========== 8. CREATE FINAL EMBEDDINGS DATAFRAME ==========\n",
    "print(\"\\n8. üíæ Creating final embeddings dataframe...\")\n",
    "\n",
    "# Create embeddings dataframe\n",
    "embeddings_df = pd.DataFrame(embeddings)\n",
    "embeddings_df.columns = [f'embedding_{i}' for i in range(embeddings.shape[1])]\n",
    "\n",
    "# Add metadata\n",
    "metadata_cols = ['item_id', 'name', 'garment_category', 'garment_formality',\n",
    "                 'price', 'total_stock', 'embedding_cluster']\n",
    "\n",
    "for col in metadata_cols:\n",
    "    if col in robust_features.columns:\n",
    "        embeddings_df[col] = robust_features[col].values\n",
    "\n",
    "# Add embedding quality metrics\n",
    "embeddings_df['embedding_norm'] = np.linalg.norm(embeddings, axis=1)\n",
    "embeddings_df['embedding_magnitude'] = embeddings_df['embedding_norm']\n",
    "\n",
    "print(f\"   ‚úÖ Embeddings dataframe: {embeddings_df.shape}\")\n",
    "print(f\"   Embedding dimensions: {embeddings.shape[1]}\")\n",
    "print(f\"   Items with embeddings: {len(embeddings_df)}\")\n",
    "\n",
    "# ========== 9. SAVE ALL ARTIFACTS ==========\n",
    "print(\"\\n9. üíæ Saving all artifacts...\")\n",
    "\n",
    "# Create embeddings directory\n",
    "EMBEDDINGS_DIR = ARTIFACTS_DIR / 'embeddings'\n",
    "EMBEDDINGS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Save embeddings dataframe\n",
    "embeddings_df.to_pickle(EMBEDDINGS_DIR / 'item_embeddings.pkl')\n",
    "print(\"   ‚úÖ Saved item_embeddings.pkl\")\n",
    "\n",
    "# Save embeddings array\n",
    "np.save(EMBEDDINGS_DIR / 'embeddings_array.npy', embeddings)\n",
    "print(\"   ‚úÖ Saved embeddings_array.npy\")\n",
    "\n",
    "# Save PCA model\n",
    "with open(EMBEDDINGS_DIR / 'pca_model.pkl', 'wb') as f:\n",
    "    pickle.dump(pca, f)\n",
    "print(\"   ‚úÖ Saved pca_model.pkl\")\n",
    "\n",
    "# Save KMeans model\n",
    "with open(EMBEDDINGS_DIR / 'kmeans_model.pkl', 'wb') as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "print(\"   ‚úÖ Saved kmeans_model.pkl\")\n",
    "\n",
    "# Save scaler\n",
    "with open(EMBEDDINGS_DIR / 'scaler_pca.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_pca, f)\n",
    "print(\"   ‚úÖ Saved scaler_pca.pkl\")\n",
    "\n",
    "# Save robust features\n",
    "robust_features.to_pickle(EMBEDDINGS_DIR / 'robust_features.pkl')\n",
    "print(\"   ‚úÖ Saved robust_features.pkl\")\n",
    "\n",
    "# Save similarity function test\n",
    "test_results = {}\n",
    "for _, item in robust_features.head(5).iterrows():\n",
    "    similar = find_similar_items_cosine(item['item_id'], top_k=3, same_category_only=True)\n",
    "    test_results[item['item_id']] = {\n",
    "        'name': item['name'],\n",
    "        'similar_items': similar\n",
    "    }\n",
    "\n",
    "with open(EMBEDDINGS_DIR / 'similarity_test_results.pkl', 'wb') as f:\n",
    "    pickle.dump(test_results, f)\n",
    "print(\"   ‚úÖ Saved similarity test results\")\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'step': 3,\n",
    "    'method': 'PCA with weighted feature engineering',\n",
    "    'embeddings_info': {\n",
    "        'total_items': len(embeddings_df),\n",
    "        'embedding_dimensions': embeddings.shape[1],\n",
    "        'original_features': len(all_features),\n",
    "        'explained_variance_ratio': float(pca.explained_variance_ratio_.sum()),\n",
    "        'average_cluster_purity': float(avg_purity)\n",
    "    },\n",
    "    'features_used': {\n",
    "        'total_features': len(all_features),\n",
    "        'category_features': len([f for f in all_features if 'cat_' in f]),\n",
    "        'formality_features': len([f for f in all_features if 'form_' in f]),\n",
    "        'encoded_features': len(encoded_cols),\n",
    "        'scaled_features': len(scaled_cols)\n",
    "    },\n",
    "    'quality_metrics': {\n",
    "        'cluster_purity': float(avg_purity),\n",
    "        'embedding_quality': 'good' if avg_purity > 0.6 else 'acceptable',\n",
    "        'category_separation': len(embeddings_df['garment_category'].unique()),\n",
    "        'formality_levels': len(embeddings_df['garment_formality'].unique())\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(EMBEDDINGS_DIR / 'embeddings_summary.pkl', 'wb') as f:\n",
    "    pickle.dump(summary, f)\n",
    "\n",
    "print(\"   ‚úÖ Saved embeddings summary\")\n",
    "\n",
    "# ========== 10. FINAL RESULTS ==========\n",
    "print(\"\\n10. üìä FINAL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüéØ METHOD: PCA with weighted feature engineering\")\n",
    "print(f\"üìà Embedding Dimensions: {embeddings.shape[1]}\")\n",
    "print(f\"üìä Explained Variance: {pca.explained_variance_ratio_.sum():.1%}\")\n",
    "print(f\"üè∑Ô∏è  Cluster Purity: {avg_purity:.1%}\")\n",
    "print(f\"üî¢ Total Items: {len(embeddings_df)}\")\n",
    "\n",
    "print(f\"\\nüìã Feature Engineering:\")\n",
    "print(f\"   - Total features used: {len(all_features)}\")\n",
    "print(f\"   - Category features: {len([f for f in all_features if 'cat_' in f])}\")\n",
    "print(f\"   - Formality features: {len([f for f in all_features if 'form_' in f])}\")\n",
    "print(f\"   - Encoded features: {len(encoded_cols)}\")\n",
    "print(f\"   - Scaled features: {len(scaled_cols)}\")\n",
    "\n",
    "print(f\"\\nüìÅ Output Files (saved to ./artifacts/embeddings/):\")\n",
    "for file in EMBEDDINGS_DIR.glob('*.pkl'):\n",
    "    print(f\"   ‚Ä¢ {file.name}\")\n",
    "print(f\"   ‚Ä¢ embeddings_array.npy\")\n",
    "\n",
    "for file in EMBEDDINGS_DIR.glob('*.pkl'):\n",
    "    print(f\"   ‚Ä¢ {file.name}\")\n",
    "print(f\"   ‚Ä¢ embeddings_array.npy\")\n",
    "\n",
    "print(f\"\\nüé≠ Category Distribution:\")\n",
    "category_dist = embeddings_df['garment_category'].value_counts()\n",
    "for cat, count in category_dist.items():\n",
    "    print(f\"   {cat:<15}: {count:>3} items\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Quality assessment\n",
    "if avg_purity > 0.7:\n",
    "    print(\"‚úÖ EXCELLENT EMBEDDINGS - READY FOR PRODUCTION!\")\n",
    "elif avg_purity > 0.6:\n",
    "    print(\"‚úÖ VERY GOOD EMBEDDINGS - READY FOR NEXT STEP!\")\n",
    "elif avg_purity > 0.5:\n",
    "    print(\"‚úÖ GOOD EMBEDDINGS - READY FOR USE\")\n",
    "elif avg_purity > 0.4:\n",
    "    print(\"‚ö†Ô∏è  ACCEPTABLE EMBEDDINGS - PROCEED WITH CAUTION\")\n",
    "else:\n",
    "    print(\"‚ùå POOR EMBEDDINGS - NEEDS REVISION\")\n",
    "\n",
    "print(f\"\\nüéØ Proceed to Step 4: Size Recommendation Engine\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show sample embeddings\n",
    "print(\"\\nüìã SAMPLE EMBEDDINGS (first 3 items):\")\n",
    "print(\"-\" * 50)\n",
    "sample = embeddings_df.head(3)\n",
    "for _, row in sample.iterrows():\n",
    "    print(f\"\\n{row['name'][:30]:<30} ({row['garment_category']}/{row['garment_formality']})\")\n",
    "    print(f\"  Cluster: {row['embedding_cluster']}\")\n",
    "    print(f\"  Embedding norm: {row['embedding_norm']:.2f}\")\n",
    "    emb_values = [row[f'embedding_{i}'] for i in range(3)]  # First 3 dimensions\n",
    "    print(f\"  First 3 dims: {[round(v, 3) for v in emb_values]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ STEP 3 COMPLETE WITH PCA EMBEDDINGS!\")\n",
    "print(\"‚úÖ Ready for size recommendation engine\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mOTudfFcnipx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè STEP 4: Size Recommendation Engine with Real Measurements (LOCAL FIXED VERSION)\n",
      "============================================================\n",
      "‚úÖ Libraries imported\n",
      "üìÅ Working directory: c:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\n",
      "üìÅ Artifacts directory: c:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\\artifacts\n",
      "üìÅ Original data: c:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\\original_items.pkl\n",
      "\n",
      "1. üîÑ Loading data from previous steps...\n",
      "   ‚úÖ Loaded original data: (250, 14)\n",
      "   ‚úÖ Loaded features from Step 2: (250, 314)\n",
      "   ‚úÖ Loaded embeddings from Step 3: (250, 41)\n",
      "\n",
      "2. üîß Parsing your actual sizing data format...\n",
      "   Testing parsing with actual data...\n",
      "   Sample data: {\"garment_type\":\"t_shirt\",\"measurements_cm\":{\"XS\":{\"chest_circumference\":\"90\",\"garment_length\":\"71\",...\n",
      "   Parsed: garment_type='t_shirt', has_measurements=True, sizes=['XS', 'S', 'M']...\n",
      "\n",
      "3. üìä Extracting all measurements...\n",
      "   ‚úÖ Created measurement database: (1500, 28)\n",
      "   Items with measurements: 250/250\n",
      "   Total size records: 1500\n",
      "\n",
      "   üìã Measurement Database Summary:\n",
      "   - Unique garment types: 58\n",
      "   - Unique sizes: 6\n",
      "   - Available measurements: ['chest_circumference', 'garment_length', 'sleeve_length', 'shoulder_width', 'waist_circumference', 'hips_circumference', 'inseam_length', 'thigh_circumference', 'leg_opening', 'hood_height', 'foot_length', 'calf_circumference', 'foot_width', 'height', 'width', 'depth', 'length', 'circumference', 'head_circumference', 'brim_width']\n",
      "\n",
      "   üìã Sample records:\n",
      "   Item 1 (t_shirt - Size XS): ['chest_circumference', 'garment_length', 'sleeve_length']...\n",
      "   Item 1 (t_shirt - Size S): ['chest_circumference', 'garment_length', 'sleeve_length']...\n",
      "   Item 1 (t_shirt - Size M): ['chest_circumference', 'garment_length', 'sleeve_length']...\n",
      "\n",
      "4. ü§ñ Building Size Recommender...\n",
      "\n",
      "5. üöÄ Building and training Size Recommender...\n",
      "\n",
      "   ‚úÖ Recommender built successfully!\n",
      "   Available garment types: 58\n",
      "   üíæ Saved to: artifacts\\size_recommender_v2.pkl\n",
      "\n",
      "6. üîó Creating Hybrid Recommendation System...\n",
      "\n",
      "   Creating hybrid system...\n",
      "   ‚úÖ Loaded 250 embeddings\n",
      "   Item IDs range: 1 to 250\n",
      "   üíæ Saved hybrid recommender to: artifacts\\hybrid_recommender.pkl\n",
      "\n",
      "7. üß™ Testing the system...\n",
      "============================================================\n",
      "\n",
      "üë§ TEST USER:\n",
      "  chest_circumference: 95cm\n",
      "  waist_circumference: 82cm\n",
      "  garment_length: 75cm\n",
      "  sleeve_length: 62cm\n",
      "  shoulder_width: 45cm\n",
      "  inseam_length: 78cm\n",
      "  hips_circumference: 96cm\n",
      "\n",
      "üß™ TESTING SIZE RECOMMENDATIONS:\n",
      "--------------------------------------------------\n",
      "\n",
      "T_SHIRT:\n",
      "  Expectation: Expected: Size M for 95cm chest\n",
      "  ‚úÖ RECOMMENDED: Size S\n",
      "     Item: Graphic Print Tee\n",
      "     Fit Score: 1.00 (Excellent Fit)\n",
      "     Price: $24.99\n",
      "     Key Measurements:\n",
      "       ‚Ä¢ chest_circumference: Perfect (User is 2.0cm smaller)\n",
      "       ‚Ä¢ garment_length: Perfect (User is 2.0cm larger)\n",
      "       ‚Ä¢ sleeve_length: Perfect (User is 2.0cm larger)\n",
      "\n",
      "REGULAR_JEANS:\n",
      "  Expectation: Expected: Size M for 82cm waist\n",
      "  ‚úÖ RECOMMENDED: Size M\n",
      "     Item: Classic Straight Jeans\n",
      "     Fit Score: 0.93 (Excellent Fit)\n",
      "     Price: $54.00\n",
      "     Key Measurements:\n",
      "       ‚Ä¢ waist_circumference: Good (User is 2.0cm smaller)\n",
      "       ‚Ä¢ hips_circumference: Perfect (User is 1.0cm smaller)\n",
      "       ‚Ä¢ inseam_length: Perfect (User is 2.0cm smaller)\n",
      "\n",
      "SLIM_PANTS:\n",
      "  Expectation: Expected: Size M or L\n",
      "  ‚úÖ RECOMMENDED: Size M\n",
      "     Item: Slim Chino Pants\n",
      "     Fit Score: 1.00 (Excellent Fit)\n",
      "     Price: $44.99\n",
      "     Key Measurements:\n",
      "       ‚Ä¢ waist_circumference: Perfect (User is 1.0cm larger)\n",
      "       ‚Ä¢ hips_circumference: Perfect (User is 0.0cm larger)\n",
      "       ‚Ä¢ inseam_length: Perfect (User is 2.0cm smaller)\n",
      "\n",
      "üß™ TESTING HYBRID RECOMMENDATIONS:\n",
      "--------------------------------------------------\n",
      "\n",
      "Style Preference: Classic Crew Tee\n",
      "\n",
      "   üîç Hybrid recommendation for t_shirt...\n",
      "\n",
      "Hybrid Recommendations:\n",
      "1. Graphic Print Tee\n",
      "   Size: S, Fit: 1.00, Style: 0.99, Combined: 1.00\n",
      "2. Organic Cotton Tee Variant 16\n",
      "   Size: S, Fit: 1.00, Style: 0.98, Combined: 0.99\n",
      "\n",
      "8. üíæ Saving complete system...\n",
      "============================================================\n",
      "   ‚úÖ Complete system saved to: artifacts\\complete_size_system.pkl\n",
      "   ‚úÖ System summary saved to: artifacts\\size_system_summary.pkl\n",
      "\n",
      "üìä SYSTEM SUMMARY:\n",
      "------------------------------\n",
      "‚Ä¢ Total items: 250\n",
      "‚Ä¢ Items with measurements: 250\n",
      "‚Ä¢ Measurement records: 1500\n",
      "‚Ä¢ Unique garment types: 58\n",
      "‚Ä¢ Available sizes: 6\n",
      "‚Ä¢ Has embeddings: True\n",
      "‚Ä¢ Has hybrid system: True\n",
      "\n",
      "üéØ FEATURES:\n",
      "------------------------------\n",
      "1. ‚úÖ Item-first size recommendations\n",
      "2. ‚úÖ Detailed fit scoring per measurement\n",
      "3. ‚úÖ Garment-type specific measurements\n",
      "4. ‚úÖ Human-readable fit assessments\n",
      "5. ‚úÖ Hybrid style + size recommendations\n",
      "6. ‚úÖ Integration with Step 3 embeddings\n",
      "7. ‚úÖ All files saved locally\n",
      "‚úÖ Imported ai_module from: c:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\\ai_module.py\n",
      "‚úÖ Available classes: ['SizeRecommenderV2', 'IntelligentOutfitBuilder']\n",
      "\n",
      "============================================================\n",
      "‚úÖ STEP 4 COMPLETE - SIZE RECOMMENDATION SYSTEM READY!\n",
      "============================================================\n",
      "\n",
      "üìñ QUICK USAGE GUIDE:\n",
      "============================================================\n",
      "\n",
      "HOW TO USE THE SYSTEM:\n",
      "\n",
      "1. Get size-only recommendations:\n",
      "   recommendations = size_recommender.find_best_fitting_items(\n",
      "       user_measurements,\n",
      "       't_shirt',  # garment type\n",
      "       top_k=5\n",
      "   )\n",
      "\n",
      "2. Get hybrid recommendations (style + size):\n",
      "   hybrid_recs = hybrid_recommender.hybrid_recommend(\n",
      "       user_measurements,\n",
      "       't_shirt',\n",
      "       style_preference=item_id,  # or embedding vector\n",
      "       top_k=5,\n",
      "       size_weight=0.7,  # balance between fit and style\n",
      "       style_weight=0.3\n",
      "   )\n",
      "\n",
      "3. Get garment statistics:\n",
      "   stats = size_recommender.get_garment_stats('t_shirt')\n",
      "\n",
      "4. Load saved system:\n",
      "   with open('artifacts/complete_size_system.pkl', 'rb') as f:\n",
      "       system = pickle.load(f)\n",
      "   size_recommender = system['size_recommender']\n",
      "\n",
      "EXAMPLE:\n",
      "--------\n",
      "user = {\n",
      "    'chest_circumference': 95,\n",
      "    'waist_circumference': 82,\n",
      "    'sleeve_length': 62\n",
      "}\n",
      "\n",
      "# Size-only\n",
      "recs = size_recommender.find_best_fitting_items(user, 't_shirt', top_k=3)\n",
      "\n",
      "# Hybrid (prefer items similar to item_id 123)\n",
      "hybrid = hybrid_recommender.hybrid_recommend(\n",
      "    user, 't_shirt', \n",
      "    style_preference=123,\n",
      "    top_k=3\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title üìè **STEP 4: Size Recommendation Engine with Real Measurements (LOCAL FIXED VERSION)**\n",
    "print(\"üìè STEP 4: Size Recommendation Engine with Real Measurements (LOCAL FIXED VERSION)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()  # Get current working directory\n",
    "sys.path.insert(0, current_dir)\n",
    "\n",
    "import ai_module\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")\n",
    "\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "# Use same directory structure as Steps 1-3\n",
    "ARTIFACTS_DIR = Path('./artifacts')\n",
    "ORIGINAL_DATA_PATH = Path('./original_items.pkl')  # From Step 1\n",
    "\n",
    "print(f\"üìÅ Working directory: {Path.cwd()}\")\n",
    "print(f\"üìÅ Artifacts directory: {ARTIFACTS_DIR.absolute()}\")\n",
    "print(f\"üìÅ Original data: {ORIGINAL_DATA_PATH.absolute()}\")\n",
    "\n",
    "# ========== 1. LOAD DATA FROM PREVIOUS STEPS ==========\n",
    "print(\"\\n1. üîÑ Loading data from previous steps...\")\n",
    "\n",
    "# Load original data from Step 1\n",
    "if not ORIGINAL_DATA_PATH.exists():\n",
    "    print(f\"   ‚ùå Original data not found at {ORIGINAL_DATA_PATH}\")\n",
    "    # Try alternative paths\n",
    "    fallback_paths = [\n",
    "        Path('original_items.pkl'),\n",
    "        Path('./original_items.pkl'),\n",
    "        Path('./artifacts/original_items.pkl')\n",
    "    ]\n",
    "    for path in fallback_paths:\n",
    "        if path.exists():\n",
    "            ORIGINAL_DATA_PATH = path\n",
    "            print(f\"   ‚úÖ Found at: {path}\")\n",
    "            break\n",
    "    \n",
    "    if not ORIGINAL_DATA_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Could not find original_items.pkl\")\n",
    "\n",
    "original_df = pd.read_pickle(ORIGINAL_DATA_PATH)\n",
    "print(f\"   ‚úÖ Loaded original data: {original_df.shape}\")\n",
    "\n",
    "# Load features from Step 2\n",
    "features_df_path = ARTIFACTS_DIR / 'features_df.pkl'\n",
    "if features_df_path.exists():\n",
    "    features_df = pd.read_pickle(features_df_path)\n",
    "    print(f\"   ‚úÖ Loaded features from Step 2: {features_df.shape}\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è features_df.pkl not found\")\n",
    "    features_df = None\n",
    "\n",
    "# Load embeddings from Step 3\n",
    "embeddings_path = ARTIFACTS_DIR / 'embeddings' / 'item_embeddings.pkl'\n",
    "if embeddings_path.exists():\n",
    "    embeddings_df = pd.read_pickle(embeddings_path)\n",
    "    print(f\"   ‚úÖ Loaded embeddings from Step 3: {embeddings_df.shape}\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è embeddings not found\")\n",
    "    embeddings_df = None\n",
    "\n",
    "# ========== 2. PARSE YOUR REAL DATA FORMAT ==========\n",
    "print(\"\\n2. üîß Parsing your actual sizing data format...\")\n",
    "\n",
    "def parse_actual_sizing_data(sizing_str):\n",
    "    \"\"\"\n",
    "    Parse the actual format from your database\n",
    "    Example: {\"garment_type\":\"t_shirt\",\"measurements_cm\":{\"XS\":{\"chest_circumference\":86,...}}}\n",
    "    \"\"\"\n",
    "    if pd.isna(sizing_str) or not isinstance(sizing_str, str):\n",
    "        return {'garment_type': 'unknown', 'measurements': {}, 'has_measurements': False}\n",
    "    \n",
    "    try:\n",
    "        # The data is already JSON from Step 1 processing\n",
    "        if sizing_str.strip().startswith('{'):\n",
    "            data = json.loads(sizing_str)\n",
    "            \n",
    "            result = {\n",
    "                'garment_type': data.get('garment_type', 'unknown'),\n",
    "                'measurements': data.get('measurements_cm', {}),\n",
    "                'fit_characteristics': data.get('fit_characteristics', {}),\n",
    "                'size_system': data.get('size_system', 'US'),\n",
    "                'has_measurements': bool(data.get('measurements_cm', {}))\n",
    "            }\n",
    "            return result\n",
    "        else:\n",
    "            return {'garment_type': 'unknown', 'measurements': {}, 'has_measurements': False}\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"   Warning: JSON decode error: {e}\")\n",
    "        return {'garment_type': 'unknown', 'measurements': {}, 'has_measurements': False}\n",
    "    except Exception as e:\n",
    "        print(f\"   Warning: Other error: {e}\")\n",
    "        return {'garment_type': 'unknown', 'measurements': {}, 'has_measurements': False}\n",
    "\n",
    "# Test parsing with actual data\n",
    "print(\"   Testing parsing with actual data...\")\n",
    "sample_data = original_df.iloc[0]['Sizing Data'] if 'Sizing Data' in original_df.columns else ''\n",
    "print(f\"   Sample data: {str(sample_data)[:100]}...\")\n",
    "parsed_sample = parse_actual_sizing_data(sample_data)\n",
    "print(f\"   Parsed: garment_type='{parsed_sample['garment_type']}', \"\n",
    "      f\"has_measurements={parsed_sample['has_measurements']}, \"\n",
    "      f\"sizes={list(parsed_sample.get('measurements', {}).keys())[:3]}...\")\n",
    "\n",
    "# ========== 3. EXTRACT ALL MEASUREMENTS ==========\n",
    "print(\"\\n3. üìä Extracting all measurements...\")\n",
    "\n",
    "all_measurements = []\n",
    "items_with_measurements = 0\n",
    "\n",
    "for idx, row in original_df.iterrows():\n",
    "    item_id = row.get('ID', idx + 1)\n",
    "    item_name = row.get('Name', f'Item {item_id}')\n",
    "    \n",
    "    # Get sizing data\n",
    "    sizing_str = row.get('Sizing Data', '')\n",
    "    parsed_data = parse_actual_sizing_data(sizing_str)\n",
    "    \n",
    "    if parsed_data.get('has_measurements', False):\n",
    "        measurements = parsed_data.get('measurements', {})\n",
    "        \n",
    "        for size, size_measurements in measurements.items():\n",
    "            if isinstance(size_measurements, dict):\n",
    "                record = {\n",
    "                    'item_id': item_id,\n",
    "                    'item_name': item_name,\n",
    "                    'garment_type': parsed_data['garment_type'],\n",
    "                    'size': size,\n",
    "                    'fit_type': parsed_data.get('fit_characteristics', {}).get('fit_type', 'regular'),\n",
    "                    'ease': parsed_data.get('fit_characteristics', {}).get('ease', 'standard'),\n",
    "                    'stretch': parsed_data.get('fit_characteristics', {}).get('stretch', 'medium'),\n",
    "                    'size_system': parsed_data.get('size_system', 'US')\n",
    "                }\n",
    "                \n",
    "                # Add all measurements as separate columns\n",
    "                for key, value in size_measurements.items():\n",
    "                    if isinstance(value, (int, float)):\n",
    "                        record[key] = float(value)\n",
    "                    elif isinstance(value, str):\n",
    "                        # Try to convert string numbers\n",
    "                        try:\n",
    "                            record[key] = float(value)\n",
    "                        except:\n",
    "                            record[key] = value\n",
    "                    else:\n",
    "                        record[key] = value\n",
    "                \n",
    "                all_measurements.append(record)\n",
    "        \n",
    "        items_with_measurements += 1\n",
    "\n",
    "# Create measurement database\n",
    "if all_measurements:\n",
    "    measurement_db = pd.DataFrame(all_measurements)\n",
    "    print(f\"   ‚úÖ Created measurement database: {measurement_db.shape}\")\n",
    "    print(f\"   Items with measurements: {items_with_measurements}/{len(original_df)}\")\n",
    "    print(f\"   Total size records: {len(measurement_db)}\")\n",
    "    \n",
    "    # Show what we found\n",
    "    print(f\"\\n   üìã Measurement Database Summary:\")\n",
    "    print(f\"   - Unique garment types: {measurement_db['garment_type'].nunique()}\")\n",
    "    print(f\"   - Unique sizes: {measurement_db['size'].nunique()}\")\n",
    "    print(f\"   - Available measurements: {[col for col in measurement_db.columns if col not in ['item_id', 'item_name', 'garment_type', 'size', 'fit_type', 'ease', 'stretch', 'size_system']]}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\n   üìã Sample records:\")\n",
    "    sample = measurement_db.head(3)\n",
    "    for _, row in sample.iterrows():\n",
    "        measurements = {k: v for k, v in row.items() if k not in ['item_id', 'item_name', 'garment_type', 'size', 'fit_type', 'ease', 'stretch', 'size_system'] and isinstance(v, (int, float))}\n",
    "        print(f\"   Item {row['item_id']} ({row['garment_type']} - Size {row['size']}): {list(measurements.keys())[:3]}...\")\n",
    "else:\n",
    "    measurement_db = pd.DataFrame()\n",
    "    print(\"   ‚ùå No measurements extracted\")\n",
    "\n",
    "# ========== 4. BUILD SIZE RECOMMENDER ==========\n",
    "print(\"\\n4. ü§ñ Building Size Recommender...\")\n",
    "SizeRecommenderV2 = ai_module.SizeRecommenderV2\n",
    "\n",
    "\n",
    "\n",
    "# ========== 5. BUILD AND TRAIN THE RECOMMENDER ==========\n",
    "print(\"\\n5. üöÄ Building and training Size Recommender...\")\n",
    "\n",
    "if not measurement_db.empty:\n",
    "    # Initialize recommender\n",
    "    size_recommender = SizeRecommenderV2()\n",
    "    size_recommender.load_data(measurement_db, original_df)\n",
    "    \n",
    "    print(f\"\\n   ‚úÖ Recommender built successfully!\")\n",
    "    print(f\"   Available garment types: {len(size_recommender.get_garment_types())}\")\n",
    "    \n",
    "    # Save the recommender\n",
    "    recommender_path = ARTIFACTS_DIR / 'size_recommender_v2.pkl'\n",
    "    with open(recommender_path, 'wb') as f:\n",
    "        pickle.dump(size_recommender, f)\n",
    "    print(f\"   üíæ Saved to: {recommender_path}\")\n",
    "else:\n",
    "    print(\"   ‚ùå No measurement data available\")\n",
    "    size_recommender = None\n",
    "\n",
    "# ========== 6. CREATE HYBRID SYSTEM WITH EMBEDDINGS ==========\n",
    "print(\"\\n6. üîó Creating Hybrid Recommendation System...\")\n",
    "\n",
    "class HybridRecommender:\n",
    "    \"\"\"Combine size recommendations with style embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, size_recommender, embeddings_df=None):\n",
    "        self.size_recommender = size_recommender\n",
    "        self.embeddings_df = embeddings_df\n",
    "        self.item_embeddings = None\n",
    "        self.item_ids = None\n",
    "        \n",
    "        # Load embeddings if available\n",
    "        if embeddings_df is not None:\n",
    "            self._load_embeddings()\n",
    "    \n",
    "    def _load_embeddings(self):\n",
    "        \"\"\"Load and prepare embeddings\"\"\"\n",
    "        # Extract embedding columns\n",
    "        embed_cols = [col for col in self.embeddings_df.columns if col.startswith('embedding_')]\n",
    "        if embed_cols:\n",
    "            self.item_embeddings = self.embeddings_df[embed_cols].values\n",
    "            \n",
    "            # Get item IDs - ensure they're integers\n",
    "            if 'item_id' in self.embeddings_df.columns:\n",
    "                self.item_ids = self.embeddings_df['item_id'].values.astype(int)\n",
    "            else:\n",
    "                # Create sequential IDs if not present\n",
    "                self.item_ids = np.arange(len(self.item_embeddings))\n",
    "                \n",
    "            print(f\"   ‚úÖ Loaded {len(self.item_embeddings)} embeddings\")\n",
    "            print(f\"   Item IDs range: {self.item_ids.min()} to {self.item_ids.max()}\")\n",
    "    \n",
    "    def hybrid_recommend(self, user_measurements, garment_type, \n",
    "                        style_preference=None, top_k=5, size_weight=0.7, style_weight=0.3):\n",
    "        \"\"\"\n",
    "        Hybrid recommendation combining size fit and style similarity\n",
    "        \n",
    "        Args:\n",
    "            user_measurements: User body measurements\n",
    "            garment_type: Type of garment\n",
    "            style_preference: Optional preferred item ID (integer)\n",
    "            top_k: Number of recommendations\n",
    "            size_weight: Weight for size fit (0-1)\n",
    "            style_weight: Weight for style similarity (0-1)\n",
    "        \"\"\"\n",
    "        print(f\"\\n   üîç Hybrid recommendation for {garment_type}...\")\n",
    "        \n",
    "        # Get size-based recommendations\n",
    "        size_recommendations = self.size_recommender.find_best_fitting_items(\n",
    "            user_measurements, garment_type, top_k=top_k * 2\n",
    "        )\n",
    "        \n",
    "        if not size_recommendations:\n",
    "            print(\"   ‚ö†Ô∏è No size-based recommendations found\")\n",
    "            return []\n",
    "        \n",
    "        # If no style preference or embeddings not available, just return size recommendations\n",
    "        if style_preference is None or self.item_embeddings is None:\n",
    "            print(\"   ‚ÑπÔ∏è  No style preference or embeddings available, returning size-only recommendations\")\n",
    "            return size_recommendations[:top_k]\n",
    "        \n",
    "        # Calculate style similarity\n",
    "        style_scores = self._calculate_style_similarity(style_preference)\n",
    "        \n",
    "        # If no style scores, return size-only\n",
    "        if not style_scores:\n",
    "            print(\"   ‚ÑπÔ∏è  Could not calculate style scores, returning size-only recommendations\")\n",
    "            return size_recommendations[:top_k]\n",
    "        \n",
    "        # Combine scores\n",
    "        final_recommendations = []\n",
    "        for rec in size_recommendations:\n",
    "            item_id = rec['item_id']\n",
    "            \n",
    "            # Get style score if available\n",
    "            style_score = style_scores.get(item_id, 0.5)  # Default to neutral\n",
    "            \n",
    "            # Combine scores\n",
    "            combined_score = (rec['overall_fit_score'] * size_weight + \n",
    "                            style_score * style_weight)\n",
    "            \n",
    "            final_rec = rec.copy()\n",
    "            final_rec['style_similarity'] = style_score\n",
    "            final_rec['combined_score'] = combined_score\n",
    "            final_recommendations.append(final_rec)\n",
    "        \n",
    "        # Sort by combined score\n",
    "        final_recommendations.sort(key=lambda x: x['combined_score'], reverse=True)\n",
    "        \n",
    "        return final_recommendations[:top_k]\n",
    "    \n",
    "    def _calculate_style_similarity(self, style_preference):\n",
    "        \"\"\"Calculate style similarity scores for all items\"\"\"\n",
    "        if self.item_embeddings is None:\n",
    "            return {}\n",
    "        \n",
    "        # Ensure style_preference is an integer (item ID)\n",
    "        try:\n",
    "            style_item_id = int(style_preference)\n",
    "        except (ValueError, TypeError):\n",
    "            print(f\"   ‚ö†Ô∏è Style preference must be an item ID (integer), got: {style_preference}\")\n",
    "            return {}\n",
    "        \n",
    "        # Find the embedding for this item ID\n",
    "        idx_mask = self.item_ids == style_item_id\n",
    "        if not np.any(idx_mask):\n",
    "            print(f\"   ‚ö†Ô∏è Item ID {style_item_id} not found in embeddings\")\n",
    "            return {}\n",
    "        \n",
    "        idx = np.where(idx_mask)[0][0]\n",
    "        target_embedding = self.item_embeddings[idx]\n",
    "        \n",
    "        # Reshape to 2D for cosine_similarity\n",
    "        if len(target_embedding.shape) == 1:\n",
    "            target_embedding = target_embedding.reshape(1, -1)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        \n",
    "        similarities = cosine_similarity(target_embedding, self.item_embeddings)[0]\n",
    "        \n",
    "        # Create dictionary of item_id -> similarity\n",
    "        style_scores = {}\n",
    "        for item_id, similarity in zip(self.item_ids, similarities):\n",
    "            style_scores[int(item_id)] = float(similarity)\n",
    "        \n",
    "        return style_scores\n",
    "\n",
    "# Create hybrid recommender\n",
    "print(\"\\n   Creating hybrid system...\")\n",
    "hybrid_recommender = HybridRecommender(size_recommender, embeddings_df)\n",
    "\n",
    "# Save hybrid system\n",
    "hybrid_path = ARTIFACTS_DIR / 'hybrid_recommender.pkl'\n",
    "with open(hybrid_path, 'wb') as f:\n",
    "    pickle.dump(hybrid_recommender, f)\n",
    "print(f\"   üíæ Saved hybrid recommender to: {hybrid_path}\")\n",
    "\n",
    "# ========== 7. TEST THE SYSTEM ==========\n",
    "print(\"\\n7. üß™ Testing the system...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test user measurements\n",
    "test_user = {\n",
    "    'chest_circumference': 95,      # Medium chest\n",
    "    'waist_circumference': 82,      # Medium waist\n",
    "    'garment_length': 75,          # Average length\n",
    "    'sleeve_length': 62,           # Average sleeve\n",
    "    'shoulder_width': 45,          # Average shoulder\n",
    "    'inseam_length': 78,           # Average inseam\n",
    "    'hips_circumference': 96       # Medium hips\n",
    "}\n",
    "\n",
    "print(f\"\\nüë§ TEST USER:\")\n",
    "for key, value in test_user.items():\n",
    "    print(f\"  {key}: {value}cm\")\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    ('t_shirt', 'Expected: Size M for 95cm chest'),\n",
    "    ('regular_jeans', 'Expected: Size M for 82cm waist'),\n",
    "    ('slim_pants', 'Expected: Size M or L'),\n",
    "]\n",
    "\n",
    "print(f\"\\nüß™ TESTING SIZE RECOMMENDATIONS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if size_recommender:\n",
    "    for garment_type, expectation in test_cases:\n",
    "        print(f\"\\n{garment_type.upper()}:\")\n",
    "        print(f\"  Expectation: {expectation}\")\n",
    "        \n",
    "        recommendations = size_recommender.find_best_fitting_items(\n",
    "            test_user, garment_type, top_k=2\n",
    "        )\n",
    "        \n",
    "        if recommendations:\n",
    "            best = recommendations[0]\n",
    "            print(f\"  ‚úÖ RECOMMENDED: Size {best['recommended_size']}\")\n",
    "            print(f\"     Item: {best['item_name']}\")\n",
    "            print(f\"     Fit Score: {best['overall_fit_score']:.2f} ({best['fit_assessment']})\")\n",
    "            print(f\"     Price: ${best['price']:.2f}\")\n",
    "            \n",
    "            # Show key measurements\n",
    "            if best.get('key_measurements'):\n",
    "                print(f\"     Key Measurements:\")\n",
    "                for meas, info in list(best['key_measurements'].items())[:3]:\n",
    "                    diff_word = \"smaller\" if info['difference'] < 0 else \"larger\"\n",
    "                    print(f\"       ‚Ä¢ {meas}: {info['assessment']} \"\n",
    "                          f\"(User is {abs(info['difference']):.1f}cm {diff_word})\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  No recommendations found\")\n",
    "\n",
    "# Test hybrid recommendations if embeddings available\n",
    "if hybrid_recommender and embeddings_df is not None:\n",
    "    print(f\"\\nüß™ TESTING HYBRID RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Use first item as style preference\n",
    "    style_item_id = embeddings_df.iloc[0]['item_id']\n",
    "    style_item_name = embeddings_df.iloc[0]['name']\n",
    "    \n",
    "    print(f\"\\nStyle Preference: {style_item_name}\")\n",
    "    \n",
    "    hybrid_recs = hybrid_recommender.hybrid_recommend(\n",
    "        test_user, 't_shirt', \n",
    "        style_preference=style_item_id,\n",
    "        top_k=2\n",
    "    )\n",
    "    \n",
    "    if hybrid_recs:\n",
    "        print(f\"\\nHybrid Recommendations:\")\n",
    "        for i, rec in enumerate(hybrid_recs, 1):\n",
    "            print(f\"{i}. {rec['item_name']}\")\n",
    "            print(f\"   Size: {rec['recommended_size']}, \"\n",
    "                  f\"Fit: {rec['overall_fit_score']:.2f}, \"\n",
    "                  f\"Style: {rec['style_similarity']:.2f}, \"\n",
    "                  f\"Combined: {rec['combined_score']:.2f}\")\n",
    "\n",
    "# ========== 8. SAVE COMPLETE SYSTEM ==========\n",
    "print(\"\\n8. üíæ Saving complete system...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create system package\n",
    "system_package = {\n",
    "    'size_recommender': size_recommender,\n",
    "    'hybrid_recommender': hybrid_recommender,\n",
    "    'measurement_database': measurement_db,\n",
    "    'test_user': test_user,\n",
    "    'test_results': {\n",
    "        't_shirt': size_recommender.find_best_fitting_items(test_user, 't_shirt', top_k=3) if size_recommender else []\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save complete system\n",
    "complete_path = ARTIFACTS_DIR / 'complete_size_system.pkl'\n",
    "with open(complete_path, 'wb') as f:\n",
    "    pickle.dump(system_package, f)\n",
    "\n",
    "print(f\"   ‚úÖ Complete system saved to: {complete_path}\")\n",
    "\n",
    "# Create summary\n",
    "summary = {\n",
    "    'system_info': {\n",
    "        'total_items': len(original_df),\n",
    "        'items_with_measurements': items_with_measurements,\n",
    "        'measurement_records': len(measurement_db),\n",
    "        'garment_types': measurement_db['garment_type'].nunique() if not measurement_db.empty else 0,\n",
    "        'available_sizes': measurement_db['size'].nunique() if not measurement_db.empty else 0,\n",
    "        'has_embeddings': embeddings_df is not None,\n",
    "        'has_hybrid': hybrid_recommender is not None\n",
    "    },\n",
    "    'file_locations': {\n",
    "        'size_recommender': str(ARTIFACTS_DIR / 'size_recommender_v2.pkl'),\n",
    "        'hybrid_recommender': str(ARTIFACTS_DIR / 'hybrid_recommender.pkl'),\n",
    "        'complete_system': str(complete_path),\n",
    "        'original_data': str(ORIGINAL_DATA_PATH)\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = ARTIFACTS_DIR / 'size_system_summary.pkl'\n",
    "with open(summary_path, 'wb') as f:\n",
    "    pickle.dump(summary, f)\n",
    "\n",
    "print(f\"   ‚úÖ System summary saved to: {summary_path}\")\n",
    "\n",
    "print(\"\\nüìä SYSTEM SUMMARY:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"‚Ä¢ Total items: {summary['system_info']['total_items']}\")\n",
    "print(f\"‚Ä¢ Items with measurements: {summary['system_info']['items_with_measurements']}\")\n",
    "print(f\"‚Ä¢ Measurement records: {summary['system_info']['measurement_records']}\")\n",
    "print(f\"‚Ä¢ Unique garment types: {summary['system_info']['garment_types']}\")\n",
    "print(f\"‚Ä¢ Available sizes: {summary['system_info']['available_sizes']}\")\n",
    "print(f\"‚Ä¢ Has embeddings: {summary['system_info']['has_embeddings']}\")\n",
    "print(f\"‚Ä¢ Has hybrid system: {summary['system_info']['has_hybrid']}\")\n",
    "\n",
    "print(\"\\nüéØ FEATURES:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"1. ‚úÖ Item-first size recommendations\")\n",
    "print(\"2. ‚úÖ Detailed fit scoring per measurement\")\n",
    "print(\"3. ‚úÖ Garment-type specific measurements\")\n",
    "print(\"4. ‚úÖ Human-readable fit assessments\")\n",
    "print(\"5. ‚úÖ Hybrid style + size recommendations\")\n",
    "print(\"6. ‚úÖ Integration with Step 3 embeddings\")\n",
    "print(\"7. ‚úÖ All files saved locally\")\n",
    "# Test that ai_module is imported correctly\n",
    "print(f\"‚úÖ Imported ai_module from: {ai_module.__file__}\")\n",
    "print(f\"‚úÖ Available classes: {ai_module.__all__}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ STEP 4 COMPLETE - SIZE RECOMMENDATION SYSTEM READY!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========== 9. USAGE EXAMPLE ==========\n",
    "print(\"\\nüìñ QUICK USAGE GUIDE:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "HOW TO USE THE SYSTEM:\n",
    "\n",
    "1. Get size-only recommendations:\n",
    "   recommendations = size_recommender.find_best_fitting_items(\n",
    "       user_measurements,\n",
    "       't_shirt',  # garment type\n",
    "       top_k=5\n",
    "   )\n",
    "\n",
    "2. Get hybrid recommendations (style + size):\n",
    "   hybrid_recs = hybrid_recommender.hybrid_recommend(\n",
    "       user_measurements,\n",
    "       't_shirt',\n",
    "       style_preference=item_id,  # or embedding vector\n",
    "       top_k=5,\n",
    "       size_weight=0.7,  # balance between fit and style\n",
    "       style_weight=0.3\n",
    "   )\n",
    "\n",
    "3. Get garment statistics:\n",
    "   stats = size_recommender.get_garment_stats('t_shirt')\n",
    "\n",
    "4. Load saved system:\n",
    "   with open('artifacts/complete_size_system.pkl', 'rb') as f:\n",
    "       system = pickle.load(f)\n",
    "   size_recommender = system['size_recommender']\n",
    "\n",
    "EXAMPLE:\n",
    "--------\n",
    "user = {\n",
    "    'chest_circumference': 95,\n",
    "    'waist_circumference': 82,\n",
    "    'sleeve_length': 62\n",
    "}\n",
    "\n",
    "# Size-only\n",
    "recs = size_recommender.find_best_fitting_items(user, 't_shirt', top_k=3)\n",
    "\n",
    "# Hybrid (prefer items similar to item_id 123)\n",
    "hybrid = hybrid_recommender.hybrid_recommend(\n",
    "    user, 't_shirt', \n",
    "    style_preference=123,\n",
    "    top_k=3\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YEi36bggnnig"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëó STEP 5: Intelligent Outfit Builder (LOCAL COMPATIBLE VERSION)\n",
      "============================================================\n",
      "‚úÖ Libraries imported\n",
      "\n",
      "1. üîÑ Loading data from Steps 3 & 4 (LOCAL)...\n",
      "üìÅ Working directory: c:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\n",
      "üìÅ Artifacts directory: c:\\Users\\Rana\\OneDrive\\Desktop\\FitFast FYP\\fitfast\\frontend\\src\\ai\\artifacts\n",
      "   Loading item embeddings from Step 3...\n",
      "   ‚úÖ Loaded item embeddings: (250, 41)\n",
      "   Columns: ['embedding_0', 'embedding_1', 'embedding_2', 'embedding_3', 'embedding_4']...\n",
      "\n",
      "   Loading robust features from Step 3...\n",
      "   ‚úÖ Loaded robust features: (250, 334)\n",
      "\n",
      "   Loading original items...\n",
      "   ‚úÖ Loaded original items: (250, 14)\n",
      "\n",
      "   Loading size recommender from Step 4...\n",
      "   ‚úÖ Loaded size recommender\n",
      "\n",
      "2. üõ†Ô∏è Creating unified item database...\n",
      "   Starting with embeddings: (250, 41)\n",
      "   ‚úÖ Added metadata from original data\n",
      "   ‚úÖ Added garment info from robust features\n",
      "\n",
      "   Ensuring essential columns...\n",
      "   ‚úÖ Unified database: (250, 51)\n",
      "   Columns: ['embedding_0', 'embedding_1', 'embedding_2', 'embedding_3', 'embedding_4', 'embedding_5', 'embedding_6', 'embedding_7', 'embedding_8', 'embedding_9']...\n",
      "\n",
      "3. ü§ñ Creating embeddings for similarity search...\n",
      "   Found 35 embedding columns\n",
      "   Created embeddings for 250 items\n",
      "\n",
      "4. üé® Creating Intelligent Outfit Builder...\n",
      "\n",
      "5. üöÄ Building and testing outfit builder...\n",
      "\n",
      "   ‚úÖ Outfit builder created!\n",
      "   ‚Ä¢ Items loaded: 250\n",
      "   ‚Ä¢ Categories: 9\n",
      "   ‚Ä¢ Style themes: 5\n",
      "\n",
      "6. üß™ DEMONSTRATION\n",
      "============================================================\n",
      "\n",
      "üë§ TEST USER MEASUREMENTS:\n",
      "  chest_circumference: 95cm\n",
      "  waist_circumference: 82cm\n",
      "  garment_length: 75cm\n",
      "  sleeve_length: 62cm\n",
      "  hips_circumference: 96cm\n",
      "  inseam_length: 78cm\n",
      "\n",
      "üîç Finding t-shirt items...\n",
      "Found 16 t-shirt items\n",
      "\n",
      "üß™ TEST 1: Find similar items to 'Item 1'\n",
      "  1. Item 3 (similarity: 0.99)\n",
      "  2. Item 92 (similarity: 0.99)\n",
      "  3. Item 188 (similarity: 0.98)\n",
      "\n",
      "üß™ TEST 2: Build 'casual_everyday' outfit\n",
      "\n",
      "üéØ OUTFIT: CASUAL_EVERYDAY\n",
      "   Description: Comfortable everyday wear\n",
      "   Compatibility: 40/100\n",
      "   Style Coherence: 100/100\n",
      "   Total Price: $144.97\n",
      "\n",
      "üëï ITEMS (3):\n",
      "1. Item 1\n",
      "   Type: t_shirt (top)\n",
      "   Formality: casual\n",
      "   Price: $19.99\n",
      "   Recommended Size: S\n",
      "2. Item 11\n",
      "   Type: slim_pants (bottom)\n",
      "   Formality: casual\n",
      "   Price: $44.99\n",
      "   Recommended Size: M\n",
      "3. Item 72\n",
      "   Type: sneakers (footwear)\n",
      "   Formality: casual\n",
      "   Price: $79.99\n",
      "\n",
      "üß™ TEST 3: Build 'smart_casual' outfit\n",
      "\n",
      "üéØ OUTFIT: SMART_CASUAL\n",
      "   Description: Polished yet comfortable\n",
      "   Compatibility: 40/100\n",
      "   Style Coherence: 80/100\n",
      "   Total Price: $234.96\n",
      "\n",
      "üëï ITEMS (4):\n",
      "1. Item 1\n",
      "   Type: t_shirt (top)\n",
      "   Formality: casual\n",
      "   Price: $19.99\n",
      "   Recommended Size: S\n",
      "2. Item 11\n",
      "   Type: slim_pants (bottom)\n",
      "   Formality: casual\n",
      "   Price: $44.99\n",
      "   Recommended Size: M\n",
      "3. Item 36\n",
      "   Type: bomber_jacket (outerwear)\n",
      "   Formality: casual\n",
      "   Price: $89.99\n",
      "   Recommended Size: S\n",
      "4. Item 72\n",
      "   Type: sneakers (footwear)\n",
      "   Formality: casual\n",
      "   Price: $79.99\n",
      "\n",
      "üß™ TEST 4: Generate multiple outfit options\n",
      "\n",
      "Generated 2 outfit options:\n",
      "\n",
      "  Option 1: casual_everyday\n",
      "     Description: Comfortable everyday wear\n",
      "     Score: 40/100\n",
      "     Price: $144.97\n",
      "     Items: 3\n",
      "\n",
      "  Option 2: smart_casual\n",
      "     Description: Polished yet comfortable\n",
      "     Score: 40/100\n",
      "     Price: $234.96\n",
      "     Items: 4\n",
      "\n",
      "7. üíæ SAVING COMPLETE SYSTEM\n",
      "============================================================\n",
      "   ‚úÖ Outfit builder saved to: artifacts\\intelligent_outfit_builder.pkl\n",
      "   ‚úÖ Complete system saved to: artifacts\\complete_outfit_system.pkl\n",
      "\n",
      "üìä SYSTEM SUMMARY:\n",
      "------------------------------\n",
      "‚Ä¢ Total items: 250\n",
      "‚Ä¢ Categories: 9\n",
      "‚Ä¢ Style themes: 5\n",
      "‚Ä¢ Has size recommender: True\n",
      "\n",
      "üéØ KEY FEATURES:\n",
      "------------------------------\n",
      "1. ‚úÖ Category-based compatibility rules\n",
      "2. ‚úÖ Formality level matching\n",
      "3. ‚úÖ Style theme generation (casual, smart, athletic, etc.)\n",
      "4. ‚úÖ Integration with Step 4 size recommendations\n",
      "5. ‚úÖ Multiple outfit generation\n",
      "6. ‚úÖ Price-aware outfit building\n",
      "7. ‚úÖ Compatibility scoring system\n",
      "\n",
      "============================================================\n",
      "‚úÖ STEP 5 COMPLETE - INTELLIGENT OUTFIT BUILDER READY!\n",
      "============================================================\n",
      "\n",
      "üìñ QUICK USAGE:\n",
      "1. Build an outfit:\n",
      "   outfit = outfit_builder.build_outfit(\n",
      "       starting_item_id='1',\n",
      "       user_measurements=user_data,\n",
      "       style_theme='casual_everyday',\n",
      "       max_items=4\n",
      "   )\n",
      "\n",
      "2. Find similar items:\n",
      "   similar = outfit_builder.find_similar_items(\n",
      "       item_id='1',\n",
      "       n=5,\n",
      "       same_category=True\n",
      "   )\n",
      "\n",
      "3. Generate multiple outfits:\n",
      "   outfits = outfit_builder.generate_multiple_outfits(\n",
      "       starting_item_id='1',\n",
      "       user_measurements=user_data,\n",
      "       n_outfits=3\n",
      "   )\n",
      "\n",
      "4. Save/load model:\n",
      "   outfit_builder.save_model('outfit_builder.pkl')\n",
      "   loaded = IntelligentOutfitBuilder.load_model('outfit_builder.pkl')\n",
      "\n",
      "\n",
      "üéØ READY FOR PRODUCTION INTEGRATION!\n",
      "The system can now:\n",
      "‚Ä¢ Build complete outfits based on style preferences\n",
      "‚Ä¢ Recommend sizes for each outfit item\n",
      "‚Ä¢ Generate multiple outfit options\n",
      "‚Ä¢ Score outfit compatibility and style coherence\n"
     ]
    }
   ],
   "source": [
    "# @title üëó **STEP 5: Intelligent Outfit Builder (LOCAL COMPATIBLE VERSION)**\n",
    "print(\"üëó STEP 5: Intelligent Outfit Builder (LOCAL COMPATIBLE VERSION)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()  # Get current working directory\n",
    "sys.path.insert(0, current_dir)\n",
    "\n",
    "import ai_module\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")\n",
    "\n",
    "# ========== 1. LOAD DATA FROM STEPS 3 & 4 (LOCAL VERSION) ==========\n",
    "print(\"\\n1. üîÑ Loading data from Steps 3 & 4 (LOCAL)...\")\n",
    "\n",
    "# Define local paths\n",
    "ARTIFACTS_DIR = Path('./artifacts')\n",
    "EMBEDDINGS_DIR = ARTIFACTS_DIR / 'embeddings'\n",
    "ORIGINAL_DATA_PATH = Path('./original_items.pkl')\n",
    "\n",
    "print(f\"üìÅ Working directory: {Path.cwd()}\")\n",
    "print(f\"üìÅ Artifacts directory: {ARTIFACTS_DIR.absolute()}\")\n",
    "\n",
    "# Load item embeddings from Step 3 (local version)\n",
    "print(\"   Loading item embeddings from Step 3...\")\n",
    "try:\n",
    "    embeddings_path = EMBEDDINGS_DIR / 'item_embeddings.pkl'\n",
    "    if embeddings_path.exists():\n",
    "        item_embeddings_df = pd.read_pickle(embeddings_path)\n",
    "        print(f\"   ‚úÖ Loaded item embeddings: {item_embeddings_df.shape}\")\n",
    "        print(f\"   Columns: {list(item_embeddings_df.columns)[:5]}...\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Item embeddings not found at {embeddings_path}\")\n",
    "        item_embeddings_df = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error loading item embeddings: {e}\")\n",
    "    item_embeddings_df = pd.DataFrame()\n",
    "\n",
    "# Load robust features from Step 3\n",
    "print(\"\\n   Loading robust features from Step 3...\")\n",
    "try:\n",
    "    robust_features_path = EMBEDDINGS_DIR / 'robust_features.pkl'\n",
    "    if robust_features_path.exists():\n",
    "        robust_features = pd.read_pickle(robust_features_path)\n",
    "        print(f\"   ‚úÖ Loaded robust features: {robust_features.shape}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è Robust features not found\")\n",
    "        robust_features = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error loading robust features: {e}\")\n",
    "    robust_features = pd.DataFrame()\n",
    "\n",
    "# Load original items (for metadata)\n",
    "print(\"\\n   Loading original items...\")\n",
    "if ORIGINAL_DATA_PATH.exists():\n",
    "    original_df = pd.read_pickle(ORIGINAL_DATA_PATH)\n",
    "    print(f\"   ‚úÖ Loaded original items: {original_df.shape}\")\n",
    "    \n",
    "    # Rename ID column for consistency\n",
    "    if 'ID' in original_df.columns:\n",
    "        original_df.rename(columns={'ID': 'item_id'}, inplace=True)\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Original items not found at {ORIGINAL_DATA_PATH}\")\n",
    "    original_df = pd.DataFrame()\n",
    "\n",
    "# Load size recommender from Step 4\n",
    "print(\"\\n   Loading size recommender from Step 4...\")\n",
    "try:\n",
    "    size_recommender_path = ARTIFACTS_DIR / 'size_recommender_v2.pkl'\n",
    "    if size_recommender_path.exists():\n",
    "        with open(size_recommender_path, 'rb') as f:\n",
    "            size_recommender = pickle.load(f)\n",
    "        print(\"   ‚úÖ Loaded size recommender\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è Size recommender not found at {size_recommender_path}\")\n",
    "        size_recommender = None\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Could not load size recommender: {e}\")\n",
    "    size_recommender = None\n",
    "\n",
    "# ========== 2. CREATE UNIFIED ITEM DATABASE ==========\n",
    "print(\"\\n2. üõ†Ô∏è Creating unified item database...\")\n",
    "\n",
    "# Start with embeddings as base (they contain essential metadata)\n",
    "if not item_embeddings_df.empty:\n",
    "    unified_df = item_embeddings_df.copy()\n",
    "    print(f\"   Starting with embeddings: {unified_df.shape}\")\n",
    "    \n",
    "    # Ensure we have critical columns\n",
    "    if 'item_id' not in unified_df.columns:\n",
    "        if 'item_id' in robust_features.columns:\n",
    "            unified_df['item_id'] = robust_features['item_id']\n",
    "        else:\n",
    "            unified_df['item_id'] = unified_df.index + 1\n",
    "    \n",
    "    # Add missing metadata from original data\n",
    "    if not original_df.empty:\n",
    "        # Keep only essential columns from original\n",
    "        original_metadata = original_df[['item_id', 'Name', 'Description', 'Store', 'Category']].copy()\n",
    "        original_metadata.rename(columns={\n",
    "            'Name': 'name',\n",
    "            'Description': 'description',\n",
    "            'Store': 'store',\n",
    "            'Category': 'category'\n",
    "        }, inplace=True)\n",
    "        \n",
    "        # Merge with unified_df\n",
    "        unified_df = pd.merge(\n",
    "            unified_df,\n",
    "            original_metadata,\n",
    "            on='item_id',\n",
    "            how='left'\n",
    "        )\n",
    "        print(f\"   ‚úÖ Added metadata from original data\")\n",
    "    \n",
    "    # Add garment_type from robust_features\n",
    "    if not robust_features.empty and 'garment_type' in robust_features.columns:\n",
    "        garment_info = robust_features[['item_id', 'garment_type', 'garment_category', 'garment_formality']].copy()\n",
    "        unified_df = pd.merge(\n",
    "            unified_df,\n",
    "            garment_info,\n",
    "            on='item_id',\n",
    "            how='left'\n",
    "        )\n",
    "        print(f\"   ‚úÖ Added garment info from robust features\")\n",
    "    \n",
    "else:\n",
    "    # Fallback: create from original data\n",
    "    print(\"   ‚ö†Ô∏è No embeddings found, creating from original data\")\n",
    "    unified_df = original_df.copy()\n",
    "    \n",
    "    # Rename columns for consistency\n",
    "    rename_map = {\n",
    "        'Name': 'name',\n",
    "        'Description': 'description',\n",
    "        'Store': 'store',\n",
    "        'Category': 'category',\n",
    "        'Garment Type': 'garment_type',\n",
    "        'Price': 'price',\n",
    "        'Total Stock': 'total_stock'\n",
    "    }\n",
    "    \n",
    "    for old_col, new_col in rename_map.items():\n",
    "        if old_col in unified_df.columns:\n",
    "            unified_df[new_col] = unified_df[old_col]\n",
    "    \n",
    "    # Create item_id if missing\n",
    "    if 'item_id' not in unified_df.columns:\n",
    "        unified_df['item_id'] = unified_df.index + 1\n",
    "\n",
    "# Ensure essential columns exist\n",
    "print(\"\\n   Ensuring essential columns...\")\n",
    "required_columns = {\n",
    "    'name': 'Item {item_id}',\n",
    "    'garment_type': 'unknown',\n",
    "    'garment_category': 'other',\n",
    "    'garment_formality': 'casual',\n",
    "    'price': 0.0,\n",
    "    'description': '',\n",
    "    'store': 'unknown',\n",
    "    'category': 'unknown'\n",
    "}\n",
    "\n",
    "for col, default in required_columns.items():\n",
    "    if col not in unified_df.columns:\n",
    "        if col == 'name':\n",
    "            unified_df[col] = unified_df.apply(lambda x: default.format(item_id=x['item_id']), axis=1)\n",
    "        elif col == 'price':\n",
    "            # Try to find price from any source\n",
    "            if 'Price' in unified_df.columns:\n",
    "                unified_df[col] = pd.to_numeric(unified_df['Price'], errors='coerce').fillna(0.0)\n",
    "            else:\n",
    "                unified_df[col] = default\n",
    "        else:\n",
    "            unified_df[col] = default\n",
    "\n",
    "print(f\"   ‚úÖ Unified database: {unified_df.shape}\")\n",
    "print(f\"   Columns: {list(unified_df.columns)[:10]}...\")\n",
    "\n",
    "# ========== 3. CREATE EMBEDDINGS FOR SIMILARITY ==========\n",
    "print(\"\\n3. ü§ñ Creating embeddings for similarity search...\")\n",
    "\n",
    "# Extract embedding columns\n",
    "embedding_cols = [col for col in unified_df.columns if col.startswith('embedding_')]\n",
    "print(f\"   Found {len(embedding_cols)} embedding columns\")\n",
    "\n",
    "if embedding_cols:\n",
    "    # Create embeddings dictionary\n",
    "    item_embeddings_dict = {}\n",
    "    for idx, row in unified_df.iterrows():\n",
    "        item_id = str(int(row['item_id']))  # Ensure string ID\n",
    "        embeddings = row[embedding_cols].values.astype(np.float32)\n",
    "        item_embeddings_dict[item_id] = embeddings\n",
    "\n",
    "    print(f\"   Created embeddings for {len(item_embeddings_dict)} items\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è No embedding columns found, creating simple embeddings...\")\n",
    "    \n",
    "    # Create simple embeddings based on categories and price\n",
    "    item_embeddings_dict = {}\n",
    "    for idx, row in unified_df.iterrows():\n",
    "        item_id = str(int(row['item_id']))\n",
    "        \n",
    "        # Create 10-dimensional embedding\n",
    "        embedding = np.zeros(10, dtype=np.float32)\n",
    "        \n",
    "        # Encode garment category (simple one-hot)\n",
    "        categories = ['top', 'bottom', 'dress', 'outerwear', 'footwear', \n",
    "                     'swimwear', 'underwear', 'socks', 'other']\n",
    "        category = row.get('garment_category', 'other')\n",
    "        if category in categories:\n",
    "            embedding[categories.index(category)] = 1.0\n",
    "        \n",
    "        # Encode price (scaled)\n",
    "        price = float(row.get('price', 0))\n",
    "        embedding[9] = np.log1p(price) / 10.0  # Log scale for price\n",
    "        \n",
    "        item_embeddings_dict[item_id] = embedding\n",
    "    \n",
    "    print(f\"   Created generated embeddings for {len(item_embeddings_dict)} items\")\n",
    "\n",
    "# ========== 4. INTELLIGENT OUTFIT BUILDER ==========\n",
    "print(\"\\n4. üé® Creating Intelligent Outfit Builder...\")\n",
    "IntelligentOutfitBuilder = ai_module.IntelligentOutfitBuilder\n",
    "\n",
    "\n",
    "# ========== 5. BUILD AND TEST THE SYSTEM ==========\n",
    "print(\"\\n5. üöÄ Building and testing outfit builder...\")\n",
    "\n",
    "# Create outfit builder\n",
    "outfit_builder = IntelligentOutfitBuilder(\n",
    "    items_df=unified_df,\n",
    "    item_embeddings_dict=item_embeddings_dict,\n",
    "    size_recommender=size_recommender\n",
    ")\n",
    "\n",
    "print(f\"\\n   ‚úÖ Outfit builder created!\")\n",
    "print(f\"   ‚Ä¢ Items loaded: {len(outfit_builder.item_metadata)}\")\n",
    "print(f\"   ‚Ä¢ Categories: {len(set(item['garment_category'] for item in outfit_builder.item_metadata.values()))}\")\n",
    "print(f\"   ‚Ä¢ Style themes: {len(outfit_builder.style_themes)}\")\n",
    "\n",
    "# Continue from where it left off (replace the broken section from line 763 onward):\n",
    "\n",
    "# ========== 6. DEMONSTRATION ==========\n",
    "print(\"\\n6. üß™ DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test user measurements (same as Step 4)\n",
    "test_user = {\n",
    "    'chest_circumference': 95,\n",
    "    'waist_circumference': 82,\n",
    "    'garment_length': 75,\n",
    "    'sleeve_length': 62,\n",
    "    'hips_circumference': 96,\n",
    "    'inseam_length': 78\n",
    "}\n",
    "\n",
    "print(f\"\\nüë§ TEST USER MEASUREMENTS:\")\n",
    "for key, value in test_user.items():\n",
    "    print(f\"  {key}: {value}cm\")\n",
    "\n",
    "# Find a starting item (t-shirt)\n",
    "print(f\"\\nüîç Finding t-shirt items...\")\n",
    "tshirt_items = []\n",
    "for item_id, metadata in outfit_builder.item_metadata.items():\n",
    "    if 'tee' in metadata['garment_type'] or 't_shirt' in metadata['garment_type']:\n",
    "        tshirt_items.append((item_id, metadata))\n",
    "\n",
    "if tshirt_items:\n",
    "    print(f\"Found {len(tshirt_items)} t-shirt items\")\n",
    "    \n",
    "    # Test with first t-shirt\n",
    "    starting_item_id, starting_item = tshirt_items[0]\n",
    "    \n",
    "    # Test 1: Similar items\n",
    "    print(f\"\\nüß™ TEST 1: Find similar items to '{starting_item['name']}'\")\n",
    "    similar_items = outfit_builder.find_similar_items(starting_item_id, n=3, same_category=True)\n",
    "    if similar_items:\n",
    "        for i, item in enumerate(similar_items, 1):\n",
    "            print(f\"  {i}. {item['name']} (similarity: {item['similarity']:.2f})\")\n",
    "    \n",
    "    # Test 2: Build casual outfit\n",
    "    print(f\"\\nüß™ TEST 2: Build 'casual_everyday' outfit\")\n",
    "    casual_outfit = outfit_builder.build_outfit(\n",
    "        starting_item_id=starting_item_id,\n",
    "        user_measurements=test_user,\n",
    "        style_theme='casual_everyday',\n",
    "        max_items=4\n",
    "    )\n",
    "    \n",
    "    if casual_outfit:\n",
    "        print(f\"\\nüéØ OUTFIT: {casual_outfit['style_theme'].upper()}\")\n",
    "        print(f\"   Description: {casual_outfit['description']}\")\n",
    "        print(f\"   Compatibility: {casual_outfit['compatibility_score']:.0f}/100\")\n",
    "        print(f\"   Style Coherence: {casual_outfit['style_coherence']:.0f}/100\")\n",
    "        print(f\"   Total Price: ${casual_outfit['total_price']:.2f}\")\n",
    "        \n",
    "        print(f\"\\nüëï ITEMS ({casual_outfit['item_count']}):\")\n",
    "        for i, item in enumerate(casual_outfit['outfit_items'], 1):\n",
    "            size_rec = casual_outfit['size_recommendations'].get(item['id'], 'Size info N/A')\n",
    "            print(f\"{i}. {item['name']}\")\n",
    "            print(f\"   Type: {item['garment_type']} ({item['garment_category']})\")\n",
    "            print(f\"   Formality: {item['formality']}\")\n",
    "            print(f\"   Price: ${item['price']:.2f}\")\n",
    "            if size_rec != 'Size info N/A':\n",
    "                print(f\"   Recommended Size: {size_rec}\")\n",
    "    \n",
    "    # Test 3: Build smart casual outfit\n",
    "    print(f\"\\nüß™ TEST 3: Build 'smart_casual' outfit\")\n",
    "    smart_outfit = outfit_builder.build_outfit(\n",
    "        starting_item_id=starting_item_id,\n",
    "        user_measurements=test_user,\n",
    "        style_theme='smart_casual',\n",
    "        max_items=4\n",
    "    )\n",
    "    \n",
    "    if smart_outfit:\n",
    "        print(f\"\\nüéØ OUTFIT: {smart_outfit['style_theme'].upper()}\")\n",
    "        print(f\"   Description: {smart_outfit['description']}\")\n",
    "        print(f\"   Compatibility: {smart_outfit['compatibility_score']:.0f}/100\")\n",
    "        print(f\"   Style Coherence: {smart_outfit['style_coherence']:.0f}/100\")\n",
    "        print(f\"   Total Price: ${smart_outfit['total_price']:.2f}\")\n",
    "        \n",
    "        print(f\"\\nüëï ITEMS ({smart_outfit['item_count']}):\")\n",
    "        for i, item in enumerate(smart_outfit['outfit_items'], 1):\n",
    "            size_rec = smart_outfit['size_recommendations'].get(item['id'], 'Size info N/A')\n",
    "            print(f\"{i}. {item['name']}\")\n",
    "            print(f\"   Type: {item['garment_type']} ({item['garment_category']})\")\n",
    "            print(f\"   Formality: {item['formality']}\")\n",
    "            print(f\"   Price: ${item['price']:.2f}\")\n",
    "            if size_rec != 'Size info N/A':\n",
    "                print(f\"   Recommended Size: {size_rec}\")\n",
    "    \n",
    "    # Test 4: Generate multiple outfit options\n",
    "    print(f\"\\nüß™ TEST 4: Generate multiple outfit options\")\n",
    "    all_outfits = outfit_builder.generate_multiple_outfits(\n",
    "        starting_item_id=starting_item_id,\n",
    "        user_measurements=test_user,\n",
    "        n_outfits=2\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nGenerated {len(all_outfits)} outfit options:\")\n",
    "    for i, outfit in enumerate(all_outfits, 1):\n",
    "        print(f\"\\n  Option {i}: {outfit['style_theme']}\")\n",
    "        print(f\"     Description: {outfit['description']}\")\n",
    "        print(f\"     Score: {outfit['compatibility_score']:.0f}/100\")\n",
    "        print(f\"     Price: ${outfit['total_price']:.2f}\")\n",
    "        print(f\"     Items: {len(outfit['outfit_items'])}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No t-shirt items found for testing\")\n",
    "\n",
    "# ========== 7. SAVE THE COMPLETE SYSTEM ==========\n",
    "print(\"\\n7. üíæ SAVING COMPLETE SYSTEM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save outfit builder\n",
    "outfit_builder_path = ARTIFACTS_DIR / 'intelligent_outfit_builder.pkl'\n",
    "outfit_builder.save_model(outfit_builder_path)\n",
    "print(f\"   ‚úÖ Outfit builder saved to: {outfit_builder_path}\")\n",
    "\n",
    "# Create complete system package\n",
    "system_package = {\n",
    "    'outfit_builder': outfit_builder,\n",
    "    'items_data': unified_df,\n",
    "    'embeddings': item_embeddings_dict,\n",
    "    'test_user': test_user,\n",
    "    'metadata': {\n",
    "        'total_items': len(outfit_builder.item_metadata),\n",
    "        'categories': len(set(item['garment_category'] for item in outfit_builder.item_metadata.values())),\n",
    "        'style_themes': len(outfit_builder.style_themes),\n",
    "        'has_size_recommender': size_recommender is not None\n",
    "    }\n",
    "}\n",
    "\n",
    "complete_system_path = ARTIFACTS_DIR / 'complete_outfit_system.pkl'\n",
    "with open(complete_system_path, 'wb') as f:\n",
    "    pickle.dump(system_package, f)\n",
    "\n",
    "print(f\"   ‚úÖ Complete system saved to: {complete_system_path}\")\n",
    "\n",
    "print(\"\\nüìä SYSTEM SUMMARY:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"‚Ä¢ Total items: {system_package['metadata']['total_items']}\")\n",
    "print(f\"‚Ä¢ Categories: {system_package['metadata']['categories']}\")\n",
    "print(f\"‚Ä¢ Style themes: {system_package['metadata']['style_themes']}\")\n",
    "print(f\"‚Ä¢ Has size recommender: {system_package['metadata']['has_size_recommender']}\")\n",
    "\n",
    "print(\"\\nüéØ KEY FEATURES:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"1. ‚úÖ Category-based compatibility rules\")\n",
    "print(\"2. ‚úÖ Formality level matching\")\n",
    "print(\"3. ‚úÖ Style theme generation (casual, smart, athletic, etc.)\")\n",
    "print(\"4. ‚úÖ Integration with Step 4 size recommendations\")\n",
    "print(\"5. ‚úÖ Multiple outfit generation\")\n",
    "print(\"6. ‚úÖ Price-aware outfit building\")\n",
    "print(\"7. ‚úÖ Compatibility scoring system\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ STEP 5 COMPLETE - INTELLIGENT OUTFIT BUILDER READY!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìñ QUICK USAGE:\")\n",
    "print(\"\"\"1. Build an outfit:\n",
    "   outfit = outfit_builder.build_outfit(\n",
    "       starting_item_id='1',\n",
    "       user_measurements=user_data,\n",
    "       style_theme='casual_everyday',\n",
    "       max_items=4\n",
    "   )\n",
    "\n",
    "2. Find similar items:\n",
    "   similar = outfit_builder.find_similar_items(\n",
    "       item_id='1',\n",
    "       n=5,\n",
    "       same_category=True\n",
    "   )\n",
    "\n",
    "3. Generate multiple outfits:\n",
    "   outfits = outfit_builder.generate_multiple_outfits(\n",
    "       starting_item_id='1',\n",
    "       user_measurements=user_data,\n",
    "       n_outfits=3\n",
    "   )\n",
    "\n",
    "4. Save/load model:\n",
    "   outfit_builder.save_model('outfit_builder.pkl')\n",
    "   loaded = IntelligentOutfitBuilder.load_model('outfit_builder.pkl')\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüéØ READY FOR PRODUCTION INTEGRATION!\")\n",
    "print(\"The system can now:\")\n",
    "print(\"‚Ä¢ Build complete outfits based on style preferences\")\n",
    "print(\"‚Ä¢ Recommend sizes for each outfit item\")\n",
    "print(\"‚Ä¢ Generate multiple outfit options\")\n",
    "print(\"‚Ä¢ Score outfit compatibility and style coherence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success! Model from: ai_module\n",
      "   Class name: SizeRecommenderV2\n",
      "   Is SizeRecommenderV2? True\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import ai_module\n",
    "\n",
    "# Load and test\n",
    "with open('artifacts/size_recommender_v2.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    \n",
    "print(f\"‚úÖ Success! Model from: {model.__class__.__module__}\")\n",
    "print(f\"   Class name: {model.__class__.__name__}\")\n",
    "print(f\"   Is SizeRecommenderV2? {isinstance(model, ai_module.SizeRecommenderV2)}\") "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
